{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-15T14:33:35.019686500Z",
     "start_time": "2023-09-15T14:33:25.510669800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lab\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import efficientnet as efn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from scipy.spatial.distance import squareform\n",
    "%matplotlib inline\n",
    "from toolz import interleave\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n",
      "N_REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', TPU.master())\n",
    "except ValueError:\n",
    "    print('Running on GPU')\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.TPUStrategy(TPU)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "# Number of computing cores, is 8 for a TPU V3-8\n",
    "print(f'N_REPLICAS: {N_REPLICAS}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T14:33:35.079565600Z",
     "start_time": "2023-09-15T14:33:35.024215500Z"
    }
   },
   "id": "58e3080e72ae46d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ploidy support](./assets/ploidy.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ec2e580a177172"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    \"\"\"\n",
    "    If the reference is unphased, cannot handle phased target data, so the valid (ref, target) combinations are:\n",
    "    (phased, phased), (phased, unphased), (unphased, unphased) \n",
    "    If the reference is haps, the target cannot be unphased (can we merge every two haps to form unphased diploids?)\n",
    "    Important note: for each case, the model should be trained separately\n",
    "    \"\"\"\n",
    "    def __init__(self, ):\n",
    "        self.target_is_gonna_be_phased = None\n",
    "        self.target_set = None\n",
    "        self.target_sample_value_index = 2\n",
    "        self.ref_sample_value_index = 2\n",
    "        self.target_file_extension = None\n",
    "        self.allele_count = 2\n",
    "        self.genotype_vals = None\n",
    "        self.ref_is_phased = None\n",
    "        self.reference_panel = None\n",
    "        self.VARIANT_COUNT = 0\n",
    "        self.is_phased = False\n",
    "        self.MISSING_VALUE = None\n",
    "        self.ref_is_hap = False\n",
    "        self.target_is_hap = False\n",
    "        self.ref_n_header_lines = []\n",
    "        self.ref_n_data_header = \"\"\n",
    "        self.target_n_header_lines = []\n",
    "        self.target_n_data_header = \"\"\n",
    "        self.ref_separator = None\n",
    "        self.map_values_1_vec = np.vectorize(self.map_hap_2_ind_parent_1)\n",
    "        self.map_values_2_vec = np.vectorize(self.map_hap_2_ind_parent_2)\n",
    "        self.map_haps_to_vec = np.vectorize(self.map_haps_2_ind)\n",
    "        self.delimiter_dictionary = {\"vcf\":\"\\t\", \"csv\":\",\", \"tsv\":\"\\t\", \"infer\":\"\\t\"}\n",
    "        self.ref_file_extension = \"vcf\"\n",
    "        self.test_file_extension = \"vcf\"\n",
    "        self.target_is_phased = True\n",
    "        ## Idea: keep track of possible alleles in each variant, and filter the predictions based on that\n",
    "        \n",
    "    def read_csv(self, file_path, is_vcf=False, is_reference=False, separator=\"\\t\", first_column_is_index=True, comments=\"##\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        In this form the data should not have more than a column for ids. The first column can be either sample ids or variant ids. In case of latter, make sure to pass :param variants_as_columns=True. Example of sample input file:\n",
    "        ## Comment line 0\n",
    "        ## Comment line 1\n",
    "        Sample_id 17392_chrI_17400_T_G ....\n",
    "        HG1023               1\n",
    "        HG1024               0\n",
    "        \"\"\"\n",
    "        print(\"Reading the file...\")\n",
    "        data_header = None\n",
    "        path_sep = \"/\" if \"/\" in file_path else os.path.sep\n",
    "        root, ext = os.path.splitext(file_path)\n",
    "        with gzip.open(file_path, 'rt') if ext == '.gz' else open(file_path, 'rt') as f_in:\n",
    "            # skip info\n",
    "            while True:\n",
    "                line = f_in.readline()\n",
    "                if line.startswith(comments):\n",
    "                    if is_reference:\n",
    "                        self.ref_n_header_lines.append(line)\n",
    "                    else:\n",
    "                        self.target_n_header_lines.append(line)\n",
    "                else:\n",
    "                    data_header = line\n",
    "                    break\n",
    "        if data_header is None:\n",
    "            raise IOError(\"The file only contains comments!\")\n",
    "        df = pd.read_csv(file_path,\n",
    "                           sep=separator,\n",
    "                           comment=comments[0],\n",
    "                           index_col=0 if first_column_is_index else None,\n",
    "                           dtype='category',\n",
    "                           names=data_header.strip().split(separator) if is_vcf else None)\n",
    "        # df = df.astype('category')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def find_file_extension(self, file_path, file_format, delimiter):\n",
    "        # Default assumption\n",
    "        separator = \"\\t\"\n",
    "        found_file_format = \"vcf\"\n",
    "        \n",
    "        if file_format not in {\"vcf\", \"csv\", \"tsv\", \"infer\"}:\n",
    "            raise ValueError(\"File extension must be one of {'vcf', 'csv', 'tsv', 'infer'}.\")\n",
    "        if file_format == 'infer':\n",
    "            file_name_tokenized = file_path.split(\".\")\n",
    "            for possible_extension in file_name_tokenized[::-1]:\n",
    "                if possible_extension in {\"vcf\", \"csv\", \"tsv\"}:\n",
    "                    found_file_format = possible_extension\n",
    "                    separator = self.delimiter_dictionary[possible_extension] if delimiter is None else delimiter\n",
    "                    break\n",
    "        else:\n",
    "            found_file_format = file_format\n",
    "            separator = self.delimiter_dictionary[file_format] if delimiter is None else delimiter\n",
    "            \n",
    "        return found_file_format, separator\n",
    "\n",
    "    \n",
    "    def assign_training_set(self, file_path:str,\n",
    "                            target_is_gonna_be_phased_or_haps:bool,\n",
    "                            variants_as_columns:bool=False,\n",
    "                            delimiter=None,\n",
    "                            file_format=\"infer\",\n",
    "                            first_column_is_index=True,\n",
    "                            comments=\"##\") -> None:\n",
    "        \"\"\"\n",
    "        :param file_path: reference panel or the training file path. Currently, VCF, CSV, and TSV are supported\n",
    "        :param target_is_gonna_be_phased: Indicates whether the targets for the imputation will be phased or unphased.\n",
    "        :param variants_as_columns: Whether the columns are variants and rows are samples or vice versa.\n",
    "        :param delimiter: the seperator used for the file\n",
    "        :param file_format: one of {\"vcf\", \"csv\", \"tsv\", \"infer\"}. If \"infer\" then the class will try to find the extension using the file name.\n",
    "        :param first_column_is_index: used for csv and tsv files to indicate if the first column should be used as identifier for samples/variants.\n",
    "        :param comments: The token to be used to filter out the lines indicating comments.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.target_is_gonna_be_phased = target_is_gonna_be_phased_or_haps\n",
    "        self.ref_file_extension, self.ref_separator = self.find_file_extension(file_path, file_format, delimiter)\n",
    "        if file_format == \"infer\":\n",
    "            print(f\"Ref file format is {self.ref_file_extension} and Ref file sep is {self.ref_separator}.\")\n",
    "\n",
    "        self.reference_panel = self.read_csv(file_path, is_reference=True, is_vcf=False, separator=self.ref_separator, first_column_is_index=first_column_is_index, comments=comments) if self.ref_file_extension != 'vcf' else self.read_csv(file_path, is_reference=True, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "        \n",
    "        if self.ref_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                self.reference_panel = self.reference_panel.transpose()\n",
    "            self.reference_panel.reset_index(drop=False, inplace=True)\n",
    "            self.reference_panel.rename(columns={self.reference_panel.columns[0]: \"ID\"}, inplace=True)\n",
    "        else: # VCF\n",
    "            self.ref_sample_value_index += 8\n",
    "        \n",
    "        self.ref_is_hap = not(\"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index] or \"/\"  in self.reference_panel.iloc[0, self.ref_sample_value_index])\n",
    "        self.ref_is_phased = \"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index]\n",
    "        ## For now I won't support merging haploids into unphased data\n",
    "        if self.ref_is_hap and not target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\"The reference contains haploids while the target will be unphased diploids. The model cannot predict the target at this rate.\")\n",
    "\n",
    "        if not (self.ref_is_phased or self.ref_is_hap) and target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\"The reference contains unphased diploids while the target will be phased or haploid data. The model cannot predict the target at this rate.\")\n",
    "\n",
    "        self.VARIANT_COUNT = self.reference_panel.shape[0]\n",
    "        print(f\"{self.VARIANT_COUNT} {'haplotype' if self.ref_is_hap else 'diplotype'} variants found!\")\n",
    "\n",
    "        self.is_phased = target_is_gonna_be_phased_or_haps and (self.ref_is_phased or self.ref_is_hap)\n",
    "        \n",
    "        original_allele_sep = \"|\" if self.ref_is_phased or self.ref_is_hap else \"/\"\n",
    "        final_allele_sep = \"|\" if self.is_phased else \"/\"\n",
    "        def get_num_allels(g):\n",
    "            v1, v2 = g.split(final_allele_sep)\n",
    "            return max(int(v1), int(v2)) + 1\n",
    "\n",
    "        genotype_vals = np.unique(self.reference_panel.iloc[:, self.ref_sample_value_index-1:].values)\n",
    "        if self.ref_is_phased and not target_is_gonna_be_phased_or_haps: # In this case ref is not haps due to the above checks\n",
    "            # Convert phased values in the reference to unphased values\n",
    "            phased_to_unphased_dict = {}\n",
    "            for i in range(genotype_vals.shape[0]):\n",
    "                key = genotype_vals[i]\n",
    "                v1, v2 = [int(s) for s in genotype_vals[i].split(original_allele_sep)]\n",
    "                genotype_vals[i] = f\"{min(v1, v2)}/{max(v1, v2)}\"\n",
    "                phased_to_unphased_dict[key] = genotype_vals[i]\n",
    "            self.reference_panel.iloc[:, self.ref_sample_value_index-1:].replace(phased_to_unphased_dict, inplace=True)\n",
    "\n",
    "        self.genotype_vals = np.unique(genotype_vals)\n",
    "\n",
    "        self.allele_count = max(map(get_num_allels, self.genotype_vals)) if not self.ref_is_hap else len(self.genotype_vals)\n",
    "        self.MISSING_VALUE = self.allele_count if self.is_phased else len(self.genotype_vals)\n",
    "\n",
    "        def key_gen(v1, v2):\n",
    "            return f\"{v1}{final_allele_sep}{v2}\"\n",
    "\n",
    "        if self.is_phased:\n",
    "            self.hap_map = {str(i): i for i in range(self.allele_count)}\n",
    "            self.hap_map.update({\".\": self.allele_count})\n",
    "            self.r_hap_map = {i:k for k, i in self.hap_map.items()}\n",
    "            self.map_preds_2_allele = np.vectorize(lambda x: self.r_hap_map[x])\n",
    "        else:\n",
    "            unphased_missing_genotype = \"./.\"\n",
    "            self.replacement_dict = {g:i for i,g in enumerate(self.genotype_vals)}\n",
    "            self.replacement_dict[unphased_missing_genotype] = len(self.genotype_vals)\n",
    "            self.reverse_replacement_dict = {v:k for k,v in enumerate(self.replacement_dict)}\n",
    "\n",
    "        self.SEQ_DEPTH = self.allele_count + 1\n",
    "        print(\"Done!\")\n",
    "\n",
    "\n",
    "    def assign_test_set(self, file_path,\n",
    "                        variants_as_columns=False,\n",
    "                        delimiter=None,\n",
    "                        file_format=\"infer\",\n",
    "                        first_column_is_index=True,\n",
    "                        comments=\"##\") -> None:\n",
    "        \"\"\"\n",
    "        :param file_path: reference panel or the training file path. Currently, VCF, CSV, and TSV are supported\n",
    "        :param variants_as_columns: Whether the columns are variants and rows are samples or vice versa.\n",
    "        :param delimiter: the seperator used for the file\n",
    "        :param file_format: one of {\"vcf\", \"csv\", \"tsv\", \"infer\"}. If \"infer\" then the class will try to find the extension using the file name.\n",
    "        :param first_column_is_index: used for csv and tsv files to indicate if the first column should be used as identifier for samples/variants.\n",
    "        :param comments: The token to be used to filter out the lines indicating comments.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.reference_panel is None:\n",
    "            raise RuntimeError(\"First you need to use 'DataReader.assign_training_set(...) to assign a training set.' \")\n",
    "\n",
    "        self.target_file_extension, separator = self.find_file_extension(file_path, file_format, delimiter)\n",
    "\n",
    "        test_df = self.read_csv(file_path, is_reference=False, is_vcf=False, separator=separator, first_column_is_index=first_column_is_index, comments=comments) if self.ref_file_extension != 'vcf' else self.read_csv(file_path, is_reference=False, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "\n",
    "        if self.target_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                test_df = test_df.transpose()\n",
    "            test_df.reset_index(drop=False, inplace=True)\n",
    "            test_df.rename(columns={test_df.columns[0]: \"ID\"}, inplace=True)\n",
    "        else: # VCF\n",
    "            self.target_sample_value_index += 8\n",
    "        \n",
    "        \n",
    "        self.target_is_hap = not(\"|\" in test_df.iloc[0, self.target_sample_value_index] or \"/\"  in test_df.iloc[0, self.target_sample_value_index])\n",
    "        is_phased = \"|\" in test_df.iloc[0, self.target_sample_value_index]\n",
    "        test_var_count = test_df.shape[0]\n",
    "        print(f\"{test_var_count} {'haplotype' if self.target_is_hap else 'diplotype'} variants found!\")\n",
    "        if (self.target_is_hap or is_phased) and not (self.ref_is_phased or self.ref_is_hap):\n",
    "            raise RuntimeError(\"The training set contains unphased data. The target must be unphased as well.\")\n",
    "        if self.ref_is_hap and not (self.target_is_hap or is_phased):\n",
    "            raise RuntimeError(\"The training set contains haploids. The current software version supports phased or haploids as the target set.\")\n",
    "\n",
    "        self.target_set = test_df.merge(right=self.reference_panel[\"ID\"], on='ID', how='right')\n",
    "        if self.target_file_extension == \"vcf\" == self.ref_file_extension:\n",
    "            self.target_set[self.reference_panel.columns[:9]] = self.reference_panel[self.reference_panel.columns[:9]]\n",
    "        self.target_set = self.target_set.astype('str')\n",
    "        self.target_set.fillna(\".\" if self.target_is_hap else \".|.\" if self.is_phased else \"./.\", inplace=True)\n",
    "        self.target_set = self.target_set.astype('category')\n",
    "        print(\"Done!\")\n",
    "        \n",
    "    def map_hap_2_ind_parent_1(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[0]]\n",
    "\n",
    "    def map_hap_2_ind_parent_2(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[1]]\n",
    "\n",
    "    def map_haps_2_ind(self, x) -> int:\n",
    "        return self.hap_map[x]\n",
    "\n",
    "    def __diploids_to_hap_vecs(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        _x = np.empty((data.shape[1] * 2, data.shape[0]), dtype=np.int32)\n",
    "        _x[0::2] = self.map_values_1_vec(data.values.T)\n",
    "        _x[1::2] = self.map_values_2_vec(data.values.T)\n",
    "        return _x\n",
    "\n",
    "    def __get_forward_data(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        if self.is_phased:\n",
    "            is_haps = \"|\" not in data.iloc[0, 0]\n",
    "            print(f\"__get_forward_data > data.iloc[0, 0]={data.iloc[0, 0]}, is_haps={is_haps}\")\n",
    "            if not is_haps:\n",
    "                return self.__diploids_to_hap_vecs(data)\n",
    "            else:\n",
    "                return self.map_haps_to_vec(data.values.T)\n",
    "        else:\n",
    "            return data.replace(self.replacement_dict).values.T.astype(np.int32)\n",
    "\n",
    "    def get_ref_set(self, starting_var_index=0, ending_var_index=0)-> np.ndarray:\n",
    "        if 0 <= starting_var_index < ending_var_index:\n",
    "            return self.__get_forward_data(data=self.reference_panel.iloc[starting_var_index:ending_var_index, self.ref_sample_value_index-1:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(data=self.reference_panel.iloc[:, self.ref_sample_value_index-1:])\n",
    "\n",
    "    def get_target_set(self, starting_var_index=0, ending_var_index=0)-> np.ndarray:\n",
    "        if 0 <= starting_var_index < ending_var_index:\n",
    "            return self.__get_forward_data(data=self.target_set.iloc[starting_var_index:ending_var_index, self.target_sample_value_index-1:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(data=self.target_set.iloc[:, self.target_sample_value_index-1:])\n",
    "\n",
    "    def convert_genotypes_to_vcf(self, genotypes, pred_format=\"GT:DS:GP\"):\n",
    "        n_samples, n_variants = genotypes.shape\n",
    "        new_vcf = self.target_set.copy()\n",
    "        new_vcf.iloc[:n_variants, 9:] = genotypes.T\n",
    "        new_vcf[\"FORMAT\"] = pred_format\n",
    "        new_vcf[\"QUAL\"] = \".\"\n",
    "        new_vcf[\"FILTER\"] = \".\"\n",
    "        new_vcf[\"INFO\"] = \"IMPUTED\"\n",
    "        return new_vcf\n",
    "\n",
    "    def convert_hap_probs_to_diploid_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        n_haploids, n_variants, n_alleles = allele_probs.shape\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "\n",
    "        if n_haploids % 2 != 0:\n",
    "            raise ValueError(\"Number of haploids should be even.\")\n",
    "\n",
    "        n_samples = n_haploids // 2\n",
    "        genotypes = np.zeros((n_samples, n_variants), dtype=object)\n",
    "\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            # haploid_1 = allele_probs_normalized[2 * i]\n",
    "            # haploid_2 = allele_probs_normalized[2 * i + 1]\n",
    "\n",
    "            for j in range(n_variants):\n",
    "                # phased_probs = np.multiply.outer(haploid_1[j], haploid_2[j]).flatten()\n",
    "                # unphased_probs = np.array([phased_probs[0], sum(phased_probs[1:3]), phased_probs[-1]])\n",
    "                # unphased_probs_str = \",\".join([f\"{v:.6f}\" for v in unphased_probs])\n",
    "                # alt_dosage = np.dot(unphased_probs, [0, 1, 2])\n",
    "                variant_genotypes = [str(v) for v in np.argmax(allele_probs_normalized[i*2:(i+1)*2, j], axis=-1)]\n",
    "                genotypes[i, j] = '|'.join(variant_genotypes) #+ f\":{alt_dosage:.3f}:{unphased_probs_str}\"\n",
    "\n",
    "        return genotypes\n",
    "\n",
    "\n",
    "    def convert_hap_probs_to_hap_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "        return np.argmax(allele_probs_normalized, axis=1).astype(str)\n",
    "\n",
    "    def convert_unphased_probs_to_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        n_samples, n_variants, n_alleles = allele_probs.shape\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "        genotypes = np.zeros((n_samples, n_variants), dtype=object)\n",
    "\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            for j in range(n_variants):\n",
    "                unphased_probs = allele_probs_normalized[i, j]\n",
    "                variant_genotypes = np.vectorize(self.reverse_replacement_dict.get)(np.argmax(unphased_probs, axis=-1)).flatten()\n",
    "                genotypes[i, j] = variant_genotypes\n",
    "\n",
    "        return genotypes\n",
    "\n",
    "    def __get_headers_for_output(self, contain_probs):\n",
    "        headers = [\"##fileformat=VCFv4.2\",\n",
    "           '''##source=STI v1.1.0''',\n",
    "           '''##INFO=<ID=IMPUTED,Number=0,Type=Flag,Description=\"Marker was imputed\">''',\n",
    "           '''##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">''',\n",
    "           ]\n",
    "        probs_headers = ['''##FORMAT=<ID=DS,Number=A,Type=Float,Description=\"Estimated Alternate Allele Dosage : [P(0/1)+2*P(1/1)]\">''',\n",
    "           '''##FORMAT=<ID=GP,Number=G,Type=Float,Description=\"Estimated Posterior Probabilities for Genotypes 0/0, 0/1 and 1/1\">''']\n",
    "        return headers.extend(probs_headers) if contain_probs else headers\n",
    "\n",
    "    def preds_to_genotypes(self, preds:np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param preds: numpy array of (n_samples, n_variants, n_alleles)\n",
    "        :return: numpy array of the same shape, with genotype calls, e.g., \"0/1\"\n",
    "        \"\"\"\n",
    "        target_df = self.target_set.copy()\n",
    "        if not self.is_phased:\n",
    "            target_df.values = self.convert_unphased_probs_to_genotypes(preds).T\n",
    "        elif self.target_is_hap:\n",
    "            target_df.values = self.convert_hap_probs_to_hap_genotypes(preds).T\n",
    "        else:\n",
    "            target_df.values = self.convert_hap_probs_to_diploid_genotypes(preds).T\n",
    "        return target_df\n",
    "\n",
    "    def write_ligated_results_to_file(self, df:pd.DataFrame, file_name:str) -> None:\n",
    "        with gzip.open(file_name, 'wt') if file_name.endswith(\".gz\") else open(file_name, 'wt') as f_out:\n",
    "            # write info\n",
    "            if self.ref_file_extension == \"vcf\":\n",
    "                f_out.write(\"\\n\".join(self.__get_headers_for_output(contain_probs=False))+\"\\n\")\n",
    "            else: # Not the best idea?\n",
    "                f_out.write(\"\\n\".join(self.ref_n_header_lines))\n",
    "        df.to_csv(file_name, sep=self.ref_separator, mode='a', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:40:39.672400100Z",
     "start_time": "2023-09-16T16:40:39.661966Z"
    }
   },
   "id": "addd78d2ffdfdbf0"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "dr = DataReader()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:40:40.391585900Z",
     "start_time": "2023-09-16T16:40:40.377539Z"
    }
   },
   "id": "cf4640f614580e45"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref file format is vcf and Ref file sep is \t.\n",
      "Reading the file...\n",
      "573 diplotype variants found!\n",
      "Done!\n",
      "Reading the file...\n",
      "573 diplotype variants found!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dr.assign_training_set(\"./data/test_purpose_datasets/Chr22_Dels_train_fold_1.vcf\", target_is_gonna_be_phased_or_haps=True)\n",
    "dr.assign_test_set(\"./data/test_purpose_datasets/Chr22_Dels_test_mr_0.2_fold_1.vcf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:40:43.015206800Z",
     "start_time": "2023-09-16T16:40:40.808554800Z"
    }
   },
   "id": "980c8e2000b6097d"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No variant indices provided or indices not valid, using the whole sequence...\n",
      "__get_forward_data > data.iloc[0, 0]=0|0, is_haps=False\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.get_ref_set()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:40:43.466950400Z",
     "start_time": "2023-09-16T16:40:43.016211600Z"
    }
   },
   "id": "1ccaf2d3388c2852"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57dec5e079f17a76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
