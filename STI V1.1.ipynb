{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-15T14:33:35.019686500Z",
     "start_time": "2023-09-15T14:33:25.510669800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lab\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import efficientnet as efn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from scipy.spatial.distance import squareform\n",
    "%matplotlib inline\n",
    "from toolz import interleave\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n",
      "N_REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', TPU.master())\n",
    "except ValueError:\n",
    "    print('Running on GPU')\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.TPUStrategy(TPU)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "# Number of computing cores, is 8 for a TPU V3-8\n",
    "print(f'N_REPLICAS: {N_REPLICAS}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T14:33:35.079565600Z",
     "start_time": "2023-09-15T14:33:35.024215500Z"
    }
   },
   "id": "58e3080e72ae46d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ploidy support](./assets/ploidy.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ec2e580a177172"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    \"\"\"\n",
    "    If the reference is unphased, cannot handle phased target data, so the valid (ref, target) combinations are:\n",
    "    (phased, phased), (phased, unphased), (unphased, unphased) \n",
    "    If the reference is haps, the target cannot be unphased (can we merge every two haps to form unphased diploids?)\n",
    "    Important note: for each case, the model should be trained separately\n",
    "    \"\"\"\n",
    "    def __init__(self, ):\n",
    "        self.target_is_gonna_be_phased = None\n",
    "        self.target_set = None\n",
    "        self.target_sample_value_index = 2\n",
    "        self.ref_sample_value_index = 2\n",
    "        self.target_file_extension = None\n",
    "        self.allele_count = 2\n",
    "        self.genotype_vals = None\n",
    "        self.ref_is_phased = None\n",
    "        self.reference_panel = None\n",
    "        self.VARIANT_COUNT = 0\n",
    "        self.is_phased = False\n",
    "        self.MISSING_VALUE = None\n",
    "        self.ref_is_hap = False\n",
    "        self.target_is_hap = False\n",
    "        self.ref_n_header_lines = []\n",
    "        self.ref_n_data_header = \"\"\n",
    "        self.target_n_header_lines = []\n",
    "        self.target_n_data_header = \"\"\n",
    "        self.ref_separator = None\n",
    "        self.map_values_1_vec = np.vectorize(self.map_hap_2_ind_parent_1)\n",
    "        self.map_values_2_vec = np.vectorize(self.map_hap_2_ind_parent_2)\n",
    "        self.map_haps_to_vec = np.vectorize(self.map_haps_2_ind)\n",
    "        self.delimiter_dictionary = {\"vcf\":\"\\t\", \"csv\":\",\", \"tsv\":\"\\t\", \"infer\":\"\\t\"}\n",
    "        self.ref_file_extension = \"vcf\"\n",
    "        self.test_file_extension = \"vcf\"\n",
    "        self.target_is_phased = True\n",
    "        ## Idea: keep track of possible alleles in each variant, and filter the predictions based on that\n",
    "        \n",
    "    def read_csv(self, file_path, is_vcf=False, is_reference=False, separator=\"\\t\", first_column_is_index=True, comments=\"##\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        In this form the data should not have more than a column for ids. The first column can be either sample ids or variant ids. In case of latter, make sure to pass :param variants_as_columns=True. Example of sample input file:\n",
    "        ## Comment line 0\n",
    "        ## Comment line 1\n",
    "        Sample_id 17392_chrI_17400_T_G ....\n",
    "        HG1023               1\n",
    "        HG1024               0\n",
    "        \"\"\"\n",
    "        print(\"Reading the file...\")\n",
    "        data_header = None\n",
    "        path_sep = \"/\" if \"/\" in file_path else os.path.sep\n",
    "        root, ext = os.path.splitext(file_path)\n",
    "        with gzip.open(file_path, 'rt') if ext == '.gz' else open(file_path, 'rt') as f_in:\n",
    "            # skip info\n",
    "            while True:\n",
    "                line = f_in.readline()\n",
    "                if line.startswith(comments):\n",
    "                    if is_reference:\n",
    "                        self.ref_n_header_lines.append(line)\n",
    "                    else:\n",
    "                        self.target_n_header_lines.append(line)\n",
    "                else:\n",
    "                    data_header = line\n",
    "                    break\n",
    "        if data_header is None:\n",
    "            raise IOError(\"The file only contains comments!\")\n",
    "        df = pd.read_csv(file_path,\n",
    "                           sep=separator,\n",
    "                           comment=comments[0],\n",
    "                           index_col=0 if first_column_is_index else None,\n",
    "                           dtype='category',\n",
    "                           names=data_header.strip().split(separator) if is_vcf else None)\n",
    "        # df = df.astype('category')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def find_file_extension(self, file_path, file_format, delimiter):\n",
    "        # Default assumption\n",
    "        separator = \"\\t\"\n",
    "        found_file_format = \"vcf\"\n",
    "        \n",
    "        if file_format not in {\"vcf\", \"csv\", \"tsv\", \"infer\"}:\n",
    "            raise ValueError(\"File extension must be one of {'vcf', 'csv', 'tsv', 'infer'}.\")\n",
    "        if file_format == 'infer':\n",
    "            file_name_tokenized = file_path.split(\".\")\n",
    "            for possible_extension in file_name_tokenized[::-1]:\n",
    "                if possible_extension in {\"vcf\", \"csv\", \"tsv\"}:\n",
    "                    found_file_format = possible_extension\n",
    "                    separator = self.delimiter_dictionary[possible_extension] if delimiter is not None else delimiter\n",
    "                    break\n",
    "        else:\n",
    "            found_file_format = file_format\n",
    "            \n",
    "        return found_file_format, separator\n",
    "\n",
    "    \n",
    "    def assign_training_set(self, file_path:str,\n",
    "                            target_is_gonna_be_phased_or_haps:bool,\n",
    "                            variants_as_columns:bool=False,\n",
    "                            delimiter=None,\n",
    "                            file_format=\"infer\",\n",
    "                            first_column_is_index=True,\n",
    "                            comments=\"##\") -> None:\n",
    "        \"\"\"\n",
    "        :param file_path: reference panel or the training file path. Currently, VCF, CSV, and TSV are supported\n",
    "        :param target_is_gonna_be_phased: Indicates whether the targets for the imputation will be phased or unphased.\n",
    "        :param variants_as_columns: Whether the columns are variants and rows are samples or vice versa.\n",
    "        :param delimiter: the seperator used for the file\n",
    "        :param file_format: one of {\"vcf\", \"csv\", \"tsv\", \"infer\"}. If \"infer\" then the class will try to find the extension using the file name.\n",
    "        :param first_column_is_index: used for csv and tsv files to indicate if the first column should be used as identifier for samples/variants.\n",
    "        :param comments: The token to be used to filter out the lines indicating comments.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.target_is_gonna_be_phased = target_is_gonna_be_phased_or_haps\n",
    "        self.ref_file_extension, self.ref_separator = self.find_file_extension(file_path, file_format, delimiter)\n",
    "\n",
    "        self.reference_panel = self.read_csv(file_path, is_reference=True, is_vcf=False, separator=self.ref_separator, first_column_is_index=first_column_is_index, comments=comments) if self.ref_file_extension != 'vcf' else self.read_csv(file_path, is_reference=True, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "        \n",
    "        if self.ref_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                self.reference_panel = self.reference_panel.transpose()\n",
    "            self.reference_panel.reset_index(drop=False, inplace=True)\n",
    "            self.reference_panel.rename(columns={self.reference_panel.columns[0]: \"ID\"}, inplace=True)\n",
    "        else: # VCF\n",
    "            self.ref_sample_value_index += 8\n",
    "        \n",
    "        self.ref_is_hap = not(\"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index] or \"/\"  in self.reference_panel.iloc[0, self.ref_sample_value_index])\n",
    "        self.ref_is_phased = \"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index]\n",
    "        ## For now I won't support merging haploids into unphased data\n",
    "        if self.ref_is_hap and not target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\"The reference contains haploids while the target will be unphased diploids. The model cannot predict the target at this rate.\")\n",
    "\n",
    "        if not (self.ref_is_phased or self.ref_is_hap) and target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\"The reference contains unphased diploids while the target will be phased or haploid data. The model cannot predict the target at this rate.\")\n",
    "\n",
    "        self.VARIANT_COUNT = self.reference_panel.shape[0]\n",
    "        print(f\"{self.VARIANT_COUNT} {'haplotype' if self.ref_is_hap else 'diplotype'} variants found!\")\n",
    "\n",
    "        self.is_phased = target_is_gonna_be_phased_or_haps and (self.ref_is_phased or self.ref_is_hap)\n",
    "        \n",
    "        original_allele_sep = \"|\" if self.ref_is_phased or self.ref_is_hap else \"/\"\n",
    "        final_allele_sep = \"|\" if self.is_phased else \"/\"\n",
    "        def get_num_allels(g):\n",
    "            v1, v2 = g.split(final_allele_sep)\n",
    "            return max(int(v1), int(v2)) + 1\n",
    "\n",
    "        genotype_vals = np.unique(self.reference_panel.iloc[:, self.ref_sample_value_index-1:].values)\n",
    "        if self.ref_is_phased and not target_is_gonna_be_phased_or_haps: # In this case ref is not haps due to the above checks\n",
    "            # Convert phased values in the reference to unphased values\n",
    "            phased_to_unphased_dict = {}\n",
    "            for i in range(genotype_vals.shape[0]):\n",
    "                key = genotype_vals[i]\n",
    "                v1, v2 = [int(s) for s in genotype_vals[i].split(original_allele_sep)]\n",
    "                genotype_vals[i] = f\"{min(v1, v2)}/{max(v1, v2)}\"\n",
    "                phased_to_unphased_dict[key] = genotype_vals[i]\n",
    "            self.reference_panel.iloc[:, self.ref_sample_value_index-1:].replace(phased_to_unphased_dict, inplace=True)\n",
    "\n",
    "        self.genotype_vals = np.unique(genotype_vals)\n",
    "\n",
    "        self.allele_count = max(map(get_num_allels, self.genotype_vals)) if not self.ref_is_hap else len(self.genotype_vals)\n",
    "        self.MISSING_VALUE = self.allele_count if self.is_phased else len(self.genotype_vals)\n",
    "\n",
    "        def key_gen(v1, v2):\n",
    "            return f\"{v1}{final_allele_sep}{v2}\"\n",
    "\n",
    "        if self.is_phased:\n",
    "            self.hap_map = {str(i): i for i in range(self.allele_count)}\n",
    "            self.hap_map.update({\".\": self.allele_count})\n",
    "            self.r_hap_map = {i:k for k, i in self.hap_map.items()}\n",
    "            self.map_preds_2_allele = np.vectorize(lambda x: self.r_hap_map[x])\n",
    "        else:\n",
    "            unphased_missing_genotype = \"./.\"\n",
    "            self.replacement_dict = {g:i for i,g in enumerate(self.genotype_vals)}\n",
    "            self.replacement_dict[unphased_missing_genotype] = len(self.genotype_vals)\n",
    "            self.reverse_replacement_dict = {v:k for k,v in enumerate(self.replacement_dict)}\n",
    "\n",
    "        self.SEQ_DEPTH = self.allele_count + 1\n",
    "\n",
    "\n",
    "    def assign_test_set(self, file_path,\n",
    "                        variants_as_columns=False,\n",
    "                        delimiter=None,\n",
    "                        file_format=\"infer\",\n",
    "                        first_column_is_index=True,\n",
    "                        comments=\"##\") -> None:\n",
    "        \"\"\"\n",
    "        :param file_path: reference panel or the training file path. Currently, VCF, CSV, and TSV are supported\n",
    "        :param variants_as_columns: Whether the columns are variants and rows are samples or vice versa.\n",
    "        :param delimiter: the seperator used for the file\n",
    "        :param file_format: one of {\"vcf\", \"csv\", \"tsv\", \"infer\"}. If \"infer\" then the class will try to find the extension using the file name.\n",
    "        :param first_column_is_index: used for csv and tsv files to indicate if the first column should be used as identifier for samples/variants.\n",
    "        :param comments: The token to be used to filter out the lines indicating comments.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.reference_panel is None:\n",
    "            raise RuntimeError(\"First you need to use 'DataReader.assign_training_set(...) to assign a training set.' \")\n",
    "\n",
    "        self.target_file_extension, separator = self.find_file_extension(file_path, file_format, delimiter)\n",
    "\n",
    "        test_df = self.read_csv(file_path, is_reference=False, is_vcf=False, separator=separator, first_column_is_index=first_column_is_index, comments=comments) if self.ref_file_extension != 'vcf' else self.read_csv(file_path, is_reference=False, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "\n",
    "        if self.target_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                test_df = test_df.transpose()\n",
    "            test_df.reset_index(drop=False, inplace=True)\n",
    "            test_df.rename(columns={test_df.columns[0]: \"ID\"}, inplace=True)\n",
    "        else: # VCF\n",
    "            self.target_sample_value_index += 8\n",
    "\n",
    "        self.target_is_hap = not(\"|\" in test_df.iloc[0, self.target_sample_value_index] or \"/\"  in test_df.iloc[0, self.target_sample_value_index])\n",
    "        is_phased = \"|\" in test_df.iloc[0, self.target_sample_value_index]\n",
    "        if (self.target_is_hap or is_phased) and not (self.ref_is_phased or self.ref_is_hap):\n",
    "            raise RuntimeError(\"The training set contains unphased data. The target must be unphased as well.\")\n",
    "        if self.ref_is_hap and not (self.target_is_hap or is_phased):\n",
    "            raise RuntimeError(\"The training set contains haploids. The current software version supports phased or haploids as the target set.\")\n",
    "\n",
    "        self.target_set = test_df.merge(right=self.reference_panel[\"ID\"], on='ID', how='right')\n",
    "        if self.target_file_extension == \"vcf\" == self.ref_file_extension:\n",
    "            self.target_set[self.reference_panel.columns[:9]] = self.reference_panel[self.reference_panel.columns[:9]]\n",
    "        self.target_set.fillna(\".\" if self.target_is_hap else \".|.\" if self.is_phased else \"./.\", inplace=True)\n",
    "        \n",
    "    def map_hap_2_ind_parent_1(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[0]]\n",
    "\n",
    "    def map_hap_2_ind_parent_2(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[1]]\n",
    "\n",
    "    def map_haps_2_ind(self, x) -> int:\n",
    "        return self.hap_map[x]\n",
    "\n",
    "    def __diploids_to_hap_vecs(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        _x = np.empty((data.shape[1] * 2, data.shape[0]), dtype=np.int32)\n",
    "        _x[0::2] = self.map_values_1_vec(data.values.T)\n",
    "        _x[1::2] = self.map_values_2_vec(data.values.T)\n",
    "        return _x\n",
    "\n",
    "    def __get_forward_data(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        if self.is_phased:\n",
    "            is_haps = \"|\" not in data.iloc[0, 0]\n",
    "            print(f\"__get_forward_data > data.iloc[0, 0]={data.iloc[0, 0]}, is_haps={is_haps}\")\n",
    "            if not is_haps:\n",
    "                return self.__diploids_to_hap_vecs(data)\n",
    "            else:\n",
    "                return self.map_haps_to_vec(data.values.T)\n",
    "        else:\n",
    "            return data.replace(self.replacement_dict).values.T.astype(np.int32)\n",
    "\n",
    "    def get_ref_set(self, starting_var_index=None, ending_var_index=None):\n",
    "        if starting_var_index>=0 and ending_var_index>=starting_var_index:\n",
    "            return self.__get_forward_data(data=self.reference_panel.iloc[starting_var_index:ending_var_index, self.ref_sample_value_index-1:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(data=self.reference_panel.iloc[:, self.ref_sample_value_index-1:])\n",
    "\n",
    "    def get_target_set(self, starting_var_index=None, ending_var_index=None):\n",
    "        if starting_var_index>=0 and ending_var_index>=starting_var_index:\n",
    "            return self.__get_forward_data(data=self.target_set.iloc[starting_var_index:ending_var_index, self.target_sample_value_index-1:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(data=self.target_set.iloc[:, self.target_sample_value_index-1:])\n",
    "\n",
    "    def convert_genotypes_to_vcf(self, genotypes, pred_format=\"GT:DS:GP\"):\n",
    "        n_samples, n_variants = genotypes.shape\n",
    "        new_vcf = self.target_set.copy()\n",
    "        new_vcf.iloc[:n_variants, 9:] = genotypes.T\n",
    "        new_vcf[\"FORMAT\"] = pred_format\n",
    "        new_vcf[\"QUAL\"] = \".\"\n",
    "        new_vcf[\"FILTER\"] = \".\"\n",
    "        new_vcf[\"INFO\"] = \"IMPUTED\"\n",
    "        return new_vcf\n",
    "\n",
    "    def convert_hap_probs_to_diploid_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        n_haploids, n_variants, n_alleles = allele_probs.shape\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "\n",
    "        if n_haploids % 2 != 0:\n",
    "            raise ValueError(\"Number of haploids should be even.\")\n",
    "\n",
    "        n_samples = n_haploids // 2\n",
    "        genotypes = np.zeros((n_samples, n_variants), dtype=object)\n",
    "\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            # haploid_1 = allele_probs_normalized[2 * i]\n",
    "            # haploid_2 = allele_probs_normalized[2 * i + 1]\n",
    "\n",
    "            for j in range(n_variants):\n",
    "                # phased_probs = np.multiply.outer(haploid_1[j], haploid_2[j]).flatten()\n",
    "                # unphased_probs = np.array([phased_probs[0], sum(phased_probs[1:3]), phased_probs[-1]])\n",
    "                # unphased_probs_str = \",\".join([f\"{v:.6f}\" for v in unphased_probs])\n",
    "                # alt_dosage = np.dot(unphased_probs, [0, 1, 2])\n",
    "                variant_genotypes = [str(v) for v in np.argmax(allele_probs_normalized[i*2:(i+1)*2, j], axis=-1)]\n",
    "                genotypes[i, j] = '|'.join(variant_genotypes) #+ f\":{alt_dosage:.3f}:{unphased_probs_str}\"\n",
    "\n",
    "        return genotypes\n",
    "\n",
    "\n",
    "    def convert_hap_probs_to_hap_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "        return np.argmax(allele_probs_normalized, axis=1).astype(str)\n",
    "\n",
    "    def convert_unphased_probs_to_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        n_samples, n_variants, n_alleles = allele_probs.shape\n",
    "        allele_probs_normalized = softmax(allele_probs, axis=-1)\n",
    "        genotypes = np.zeros((n_samples, n_variants), dtype=object)\n",
    "\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            for j in range(n_variants):\n",
    "                unphased_probs = allele_probs_normalized[i, j]\n",
    "                variant_genotypes = np.vectorize(self.reverse_replacement_dict.get)(np.argmax(unphased_probs, axis=-1)).flatten()\n",
    "                genotypes[i, j] = variant_genotypes\n",
    "\n",
    "        return genotypes\n",
    "\n",
    "    def __get_headers_for_output(self, contain_probs):\n",
    "        headers = [\"##fileformat=VCFv4.2\",\n",
    "           '''##source=STI v1.1.0''',\n",
    "           '''##INFO=<ID=IMPUTED,Number=0,Type=Flag,Description=\"Marker was imputed\">''',\n",
    "           '''##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">''',\n",
    "           ]\n",
    "        probs_headers = ['''##FORMAT=<ID=DS,Number=A,Type=Float,Description=\"Estimated Alternate Allele Dosage : [P(0/1)+2*P(1/1)]\">''',\n",
    "           '''##FORMAT=<ID=GP,Number=G,Type=Float,Description=\"Estimated Posterior Probabilities for Genotypes 0/0, 0/1 and 1/1\">''']\n",
    "        return headers.extend(probs_headers) if contain_probs else headers\n",
    "\n",
    "    def preds_to_genotypes(self, preds:np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param preds: numpy array of (n_samples, n_variants, n_alleles)\n",
    "        :return: numpy array of the same shape, with genotype calls, e.g., \"0/1\"\n",
    "        \"\"\"\n",
    "        target_df = self.target_set.copy()\n",
    "        if not self.is_phased:\n",
    "            target_df.values = self.convert_unphased_probs_to_genotypes(preds).T\n",
    "        elif self.target_is_hap:\n",
    "            target_df.values = self.convert_hap_probs_to_hap_genotypes(preds).T\n",
    "        else:\n",
    "            target_df.values = self.convert_hap_probs_to_diploid_genotypes(preds).T\n",
    "        return target_df\n",
    "\n",
    "    def write_ligated_results_to_file(self, df:pd.DataFrame, file_name:str) -> None:\n",
    "        with gzip.open(file_name, 'wt') if file_name.endswith(\".gz\") else open(file_name, 'wt') as f_out:\n",
    "            # write info\n",
    "            if self.ref_file_extension == \"vcf\":\n",
    "                f_out.write(\"\\n\".join(self.__get_headers_for_output(contain_probs=False))+\"\\n\")\n",
    "            else: # Not the best idea?\n",
    "                f_out.write(\"\\n\".join(self.ref_n_header_lines))\n",
    "        df.to_csv(file_name, sep=self.ref_separator, mode='a', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T14:56:43.084525200Z",
     "start_time": "2023-09-14T14:56:43.064527800Z"
    }
   },
   "id": "addd78d2ffdfdbf0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 'a', 2: 'b', 3: 'c', 4: 'd'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T17:35:44.892188Z",
     "start_time": "2023-09-14T17:35:44.873177200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  #CHROM       POS                      ID REF    ALT QUAL FILTER  \\\n0     22  16533236             SI_BD_17525   C  <CN0>  100   PASS   \n1     22  16577743          YL_CN_CEU_5170   T  <CN0>  100   PASS   \n2     22  16589908             SI_BD_17528   T  <CN0>  100   PASS   \n3     22  16633635          YL_CN_STU_4360   G  <CN0>  100   PASS   \n4     22  16940402  BI_GS_DEL1_B2_P2862_55   A  <CN0>  100   PASS   \n\n                                                INFO FORMAT HG00096  ...  \\\n0  AC=125;AF=0.0249601;AFR_AF=0.09;AMR_AF=0.0086;...     GT     0|0  ...   \n1  AC=29;AF=0.00579073;AFR_AF=0.0098;AMR_AF=0.001...     GT     0|0  ...   \n2  AC=186;AF=0.0371406;AFR_AF=0.1021;AMR_AF=0.014...     GT     0|0  ...   \n3  AC=2;AF=0.00039936;AFR_AF=0;AMR_AF=0;AN=5008;C...     GT     0|0  ...   \n4  AC=2;AF=0.00039936;AFR_AF=0;AMR_AF=0;AN=5008;C...     GT     0|0  ...   \n\n  NA21128 NA21129 NA21130 NA21133 NA21135 NA21137 NA21141 NA21142 NA21143  \\\n0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0   \n1     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0   \n2     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0   \n3     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0   \n4     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0     0|0   \n\n  NA21144  \n0     0|0  \n1     0|0  \n2     0|0  \n3     0|0  \n4     0|0  \n\n[5 rows x 2513 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#CHROM</th>\n      <th>POS</th>\n      <th>ID</th>\n      <th>REF</th>\n      <th>ALT</th>\n      <th>QUAL</th>\n      <th>FILTER</th>\n      <th>INFO</th>\n      <th>FORMAT</th>\n      <th>HG00096</th>\n      <th>...</th>\n      <th>NA21128</th>\n      <th>NA21129</th>\n      <th>NA21130</th>\n      <th>NA21133</th>\n      <th>NA21135</th>\n      <th>NA21137</th>\n      <th>NA21141</th>\n      <th>NA21142</th>\n      <th>NA21143</th>\n      <th>NA21144</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>16533236</td>\n      <td>SI_BD_17525</td>\n      <td>C</td>\n      <td>&lt;CN0&gt;</td>\n      <td>100</td>\n      <td>PASS</td>\n      <td>AC=125;AF=0.0249601;AFR_AF=0.09;AMR_AF=0.0086;...</td>\n      <td>GT</td>\n      <td>0|0</td>\n      <td>...</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22</td>\n      <td>16577743</td>\n      <td>YL_CN_CEU_5170</td>\n      <td>T</td>\n      <td>&lt;CN0&gt;</td>\n      <td>100</td>\n      <td>PASS</td>\n      <td>AC=29;AF=0.00579073;AFR_AF=0.0098;AMR_AF=0.001...</td>\n      <td>GT</td>\n      <td>0|0</td>\n      <td>...</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>16589908</td>\n      <td>SI_BD_17528</td>\n      <td>T</td>\n      <td>&lt;CN0&gt;</td>\n      <td>100</td>\n      <td>PASS</td>\n      <td>AC=186;AF=0.0371406;AFR_AF=0.1021;AMR_AF=0.014...</td>\n      <td>GT</td>\n      <td>0|0</td>\n      <td>...</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>16633635</td>\n      <td>YL_CN_STU_4360</td>\n      <td>G</td>\n      <td>&lt;CN0&gt;</td>\n      <td>100</td>\n      <td>PASS</td>\n      <td>AC=2;AF=0.00039936;AFR_AF=0;AMR_AF=0;AN=5008;C...</td>\n      <td>GT</td>\n      <td>0|0</td>\n      <td>...</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>16940402</td>\n      <td>BI_GS_DEL1_B2_P2862_55</td>\n      <td>A</td>\n      <td>&lt;CN0&gt;</td>\n      <td>100</td>\n      <td>PASS</td>\n      <td>AC=2;AF=0.00039936;AFR_AF=0;AMR_AF=0;AN=5008;C...</td>\n      <td>GT</td>\n      <td>0|0</td>\n      <td>...</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n      <td>0|0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2513 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vcf = DataReader().read_csv(\"./data/STI_benchmark_datasets/DELL.chr22.genotypes.full.vcf.gz\", is_reference=False, is_vcf=True, separator=\"\\t\", first_column_is_index=False, comments=\"##\")\n",
    "df_vcf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:27:00.953533Z",
     "start_time": "2023-09-14T15:26:56.593872700Z"
    }
   },
   "id": "fecfa6c805e681e8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file...\n"
     ]
    },
    {
     "data": {
      "text/plain": "      33070_chrI_33070_A_T 33147_chrI_33147_G_T 33152_chrI_33152_T_C  \\\nSAMID                                                                  \n40_73                    1                    1                    1   \n40_74                    1                    1                    1   \n40_75                    2                    2                    2   \n40_76                    2                    2                    2   \n40_77                    1                    1                    1   \n\n      33200_chrI_33200_C_T 33293_chrI_33293_A_T 33328_chrI_33328_C_A  \\\nSAMID                                                                  \n40_73                    1                    1                    1   \n40_74                    1                    1                    1   \n40_75                    2                    2                    2   \n40_76                    2                    2                    2   \n40_77                    1                    1                    1   \n\n      33348_chrI_33348_G_C 33403_chrI_33403_C_T 33502_chrI_33502_A_G  \\\nSAMID                                                                  \n40_73                    1                    1                    1   \n40_74                    1                    1                    1   \n40_75                    2                    2                    2   \n40_76                    2                    2                    2   \n40_77                    1                    1                    1   \n\n      33548_chrI_33548_A_C  ... 12048853_chrXVI_925593_G_C  \\\nSAMID                       ...                              \n40_73                    1  ...                          2   \n40_74                    1  ...                          1   \n40_75                    2  ...                          1   \n40_76                    2  ...                          2   \n40_77                    1  ...                          2   \n\n      12049199_chrXVI_925939_T_C 12049441_chrXVI_926181_C_T  \\\nSAMID                                                         \n40_73                          2                          2   \n40_74                          1                          1   \n40_75                          1                          1   \n40_76                          2                          2   \n40_77                          2                          2   \n\n      12050613_chrXVI_927353_T_G 12051167_chrXVI_927907_A_C  \\\nSAMID                                                         \n40_73                          2                          2   \n40_74                          1                          1   \n40_75                          1                          1   \n40_76                          2                          2   \n40_77                          2                          2   \n\n      12051240_chrXVI_927980_A_G 12051367_chrXVI_928107_C_T  \\\nSAMID                                                         \n40_73                          2                          2   \n40_74                          1                          1   \n40_75                          1                          1   \n40_76                          2                          2   \n40_77                          2                          2   \n\n      12052782_chrXVI_929522_C_T 12052988_chrXVI_929728_A_G  \\\nSAMID                                                         \n40_73                          2                          2   \n40_74                          1                          1   \n40_75                          1                          1   \n40_76                          2                          2   \n40_77                          2                          2   \n\n      12053130_chrXVI_929870_C_T  \nSAMID                             \n40_73                          2  \n40_74                          1  \n40_75                          1  \n40_76                          2  \n40_77                          2  \n\n[5 rows x 28220 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>33070_chrI_33070_A_T</th>\n      <th>33147_chrI_33147_G_T</th>\n      <th>33152_chrI_33152_T_C</th>\n      <th>33200_chrI_33200_C_T</th>\n      <th>33293_chrI_33293_A_T</th>\n      <th>33328_chrI_33328_C_A</th>\n      <th>33348_chrI_33348_G_C</th>\n      <th>33403_chrI_33403_C_T</th>\n      <th>33502_chrI_33502_A_G</th>\n      <th>33548_chrI_33548_A_C</th>\n      <th>...</th>\n      <th>12048853_chrXVI_925593_G_C</th>\n      <th>12049199_chrXVI_925939_T_C</th>\n      <th>12049441_chrXVI_926181_C_T</th>\n      <th>12050613_chrXVI_927353_T_G</th>\n      <th>12051167_chrXVI_927907_A_C</th>\n      <th>12051240_chrXVI_927980_A_G</th>\n      <th>12051367_chrXVI_928107_C_T</th>\n      <th>12052782_chrXVI_929522_C_T</th>\n      <th>12052988_chrXVI_929728_A_G</th>\n      <th>12053130_chrXVI_929870_C_T</th>\n    </tr>\n    <tr>\n      <th>SAMID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40_73</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>40_74</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>40_75</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>40_76</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>40_77</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28220 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = DataReader().read_csv(\"./data/yeast_genotype_test.txt\", is_reference=False, separator=\"\\t\", is_vcf=False, first_column_is_index=True, comments=\"##\")\n",
    "df_csv.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:01:36.488129800Z",
     "start_time": "2023-09-14T14:57:53.033208700Z"
    }
   },
   "id": "a3ff6dc40d21ee5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(\"./data/yeast_genotype_test.txt\", dtype=\"category\", sep=\"\\t\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-14T14:52:15.953029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['1', '1', '1', '1', '1'],\n       ['1', '1', '1', '1', '1'],\n       ['2', '2', '2', '2', '2'],\n       ['2', '2', '2', '2', '2'],\n       ['1', '1', '1', '1', '1']], dtype=object)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.values[:5, :5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:04:37.852842700Z",
     "start_time": "2023-09-14T15:04:36.753040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "SAMID                    ID 40_73 40_74 40_75 40_76 40_77 40_79 40_80 40_81  \\\n0      33070_chrI_33070_A_T     1     1     2     2     1     2     1     1   \n1      33147_chrI_33147_G_T     1     1     2     2     1     2     1     1   \n2      33152_chrI_33152_T_C     1     1     2     2     1     2     1     1   \n3      33200_chrI_33200_C_T     1     1     2     2     1     2     1     1   \n4      33293_chrI_33293_A_T     1     1     2     2     1     2     1     1   \n\nSAMID 40_82  ... 49_94 49_95 49_96 50_02 50_03 50_10 50_11 50_26 50_27 50_35  \n0         2  ...     1     1     2     2     1     2     2     1     2     2  \n1         2  ...     1     1     2     2     1     2     2     1     2     2  \n2         2  ...     1     1     2     2     1     2     2     1     2     2  \n3         2  ...     1     1     2     2     1     2     2     1     2     2  \n4         2  ...     1     1     2     2     1     2     2     1     2     2  \n\n[5 rows x 878 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>SAMID</th>\n      <th>ID</th>\n      <th>40_73</th>\n      <th>40_74</th>\n      <th>40_75</th>\n      <th>40_76</th>\n      <th>40_77</th>\n      <th>40_79</th>\n      <th>40_80</th>\n      <th>40_81</th>\n      <th>40_82</th>\n      <th>...</th>\n      <th>49_94</th>\n      <th>49_95</th>\n      <th>49_96</th>\n      <th>50_02</th>\n      <th>50_03</th>\n      <th>50_10</th>\n      <th>50_11</th>\n      <th>50_26</th>\n      <th>50_27</th>\n      <th>50_35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33070_chrI_33070_A_T</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33147_chrI_33147_G_T</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33152_chrI_33152_T_C</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33200_chrI_33200_C_T</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33293_chrI_33293_A_T</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 878 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = df_csv.transpose()\n",
    "dff.reset_index(drop=False, inplace=True)\n",
    "dff.rename(columns={dff.columns[0]: \"ID\"}, inplace=True)\n",
    "dff.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:06:45.161222700Z",
     "start_time": "2023-09-14T15:06:40.982262800Z"
    }
   },
   "id": "594146929ffa1715"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "SAMID                    ID 40_73\n0      33070_chrI_33070_A_T     1\n1      33147_chrI_33147_G_T     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>SAMID</th>\n      <th>ID</th>\n      <th>40_73</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33070_chrI_33070_A_T</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33147_chrI_33147_G_T</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.iloc[:2, :2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T16:29:35.329079400Z",
     "start_time": "2023-09-14T16:29:35.319079100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['33070_chrI_33070_A_T', '33147_chrI_33147_G_T', '33152_chrI_33152_T_C',\n       '33200_chrI_33200_C_T', '33293_chrI_33293_A_T', '33328_chrI_33328_C_A',\n       '33348_chrI_33348_G_C', '33403_chrI_33403_C_T', '33502_chrI_33502_A_G',\n       '33548_chrI_33548_A_C',\n       ...\n       '12048853_chrXVI_925593_G_C', '12049199_chrXVI_925939_T_C',\n       '12049441_chrXVI_926181_C_T', '12050613_chrXVI_927353_T_G',\n       '12051167_chrXVI_927907_A_C', '12051240_chrXVI_927980_A_G',\n       '12051367_chrXVI_928107_C_T', '12052782_chrXVI_929522_C_T',\n       '12052988_chrXVI_929728_A_G', '12053130_chrXVI_929870_C_T'],\n      dtype='object', length=28220)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:05:57.649810Z",
     "start_time": "2023-09-14T15:05:57.632779200Z"
    }
   },
   "id": "94d1eb11988a948a"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                          ID 40_73 40_74 40_75 40_76 40_77 40_79 40_80 40_81  \\\n0                SI_BD_17525   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n1             YL_CN_CEU_5170   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n2                SI_BD_17528   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n3             YL_CN_STU_4360   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n4     BI_GS_DEL1_B2_P2862_55   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n..                       ...   ...   ...   ...   ...   ...   ...   ...   ...   \n568  BI_GS_DEL1_B5_P2896_582   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n569  BI_GS_DEL1_B2_P2896_693   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n570              UW_VH_22595   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n571  BI_GS_DEL1_B2_P2897_127   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n572   BI_GS_DEL1_B4_P2897_43   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n\n    40_82  ... 49_94 49_95 49_96 50_02 50_03 50_10 50_11 50_26 50_27 50_35  \n0     NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n1     NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n2     NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n3     NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n4     NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n..    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n568   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n569   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n570   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n571   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n572   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n\n[573 rows x 878 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>40_73</th>\n      <th>40_74</th>\n      <th>40_75</th>\n      <th>40_76</th>\n      <th>40_77</th>\n      <th>40_79</th>\n      <th>40_80</th>\n      <th>40_81</th>\n      <th>40_82</th>\n      <th>...</th>\n      <th>49_94</th>\n      <th>49_95</th>\n      <th>49_96</th>\n      <th>50_02</th>\n      <th>50_03</th>\n      <th>50_10</th>\n      <th>50_11</th>\n      <th>50_26</th>\n      <th>50_27</th>\n      <th>50_35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SI_BD_17525</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>YL_CN_CEU_5170</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SI_BD_17528</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>YL_CN_STU_4360</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BI_GS_DEL1_B2_P2862_55</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>BI_GS_DEL1_B5_P2896_582</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>569</th>\n      <td>BI_GS_DEL1_B2_P2896_693</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>570</th>\n      <td>UW_VH_22595</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>571</th>\n      <td>BI_GS_DEL1_B2_P2897_127</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>572</th>\n      <td>BI_GS_DEL1_B4_P2897_43</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>573 rows × 878 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.merge(right=df_vcf[\"ID\"], on='ID', how='right')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T15:27:15.568551600Z",
     "start_time": "2023-09-14T15:27:15.475551200Z"
    }
   },
   "id": "ee8910705ccd50ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "980c8e2000b6097d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
