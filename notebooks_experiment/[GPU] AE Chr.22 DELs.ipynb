{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25164,"status":"ok","timestamp":1672270940092,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"doSuiA3dyTyg","outputId":"4467fdc3-ca93-4815-a9ff-12191b66a927"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13540,"status":"ok","timestamp":1672270953629,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"liJJGzQp4qzO","outputId":"9efd1778-a563-45d6-90cc-11e8791f04de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 31.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.19.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (0.12.0)\n","Collecting scikit-allel\n","  Downloading scikit_allel-1.3.5-cp38-cp38-manylinux2010_x86_64.whl (7.2 MB)\n","\u001b[K     |████████████████████████████████| 7.2 MB 173 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from scikit-allel) (1.21.6)\n","Requirement already satisfied: dask[array] in /usr/local/lib/python3.8/dist-packages (from scikit-allel) (2022.2.1)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (2022.11.0)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (1.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (6.0)\n","Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->dask[array]->scikit-allel) (3.0.9)\n","Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[array]->scikit-allel) (1.0.0)\n","Installing collected packages: scikit-allel\n","Successfully installed scikit-allel-1.3.5\n"]}],"source":["!pip install tensorflow-addons\n","!pip install pyyaml h5py\n","!pip install toolz scikit-allel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2136,"status":"ok","timestamp":1672270955758,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"ZWzi3Z3L5RO2","outputId":"12ba679f-6702-4bad-a07a-b2afd1e5fbfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Tensorflow version 2.9.2\n"]}],"source":["import numpy as np\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"RjGOO5PdFPf7"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1986,"status":"ok","timestamp":1672270957741,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"odmhCqSVFPf8","outputId":"dd475796-8f8a-4ece-df2a-faf24aa7b1d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version 2.9.2\n"]}],"source":["import os\n","# os.environ[\"MODIN_CPUS\"] = \"8\"\n","# from distributed import Client\n","# client = Client()\n","import numpy as np\n","import math\n","import re\n","import itertools\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_addons as tfa\n","from sklearn import metrics\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import constraints\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.applications import efficientnet as efn\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from tensorflow.keras.constraints import Constraint\n","# import allel\n","from scipy.spatial.distance import squareform\n","%matplotlib inline   \n","from toolz import interleave\n","from tqdm import tqdm\n","import allel\n","from scipy.spatial.distance import squareform\n","from matplotlib import pyplot as plt\n","import tensorflow_datasets as tfds\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LassoCV, ElasticNetCV\n","from sklearn.model_selection import KFold,StratifiedKFold\n","\n","print(\"Tensorflow version \" + tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"SLd26RspFhaS"},"source":["## Hardware Config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672270957741,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"SLd7mAFgFUnR","outputId":"edfcc3f7-d6f7-4ca2-b828-1cc06f4b566b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on GPU\n","N_REPLICAS: 1\n"]}],"source":["# Detect hardware, return appropriate distribution strategy\n","try:\n","    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","    print('Running on TPU ', TPU.master())\n","except ValueError:\n","    print('Running on GPU')\n","    TPU = None\n","\n","if TPU:\n","    tf.config.experimental_connect_to_cluster(TPU)\n","    tf.tpu.experimental.initialize_tpu_system(TPU)\n","    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n","else:\n","    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","N_REPLICAS = strategy.num_replicas_in_sync\n","# Number of computing cores, is 8 for a TPU V3-8\n","print(f'N_REPLICAS: {N_REPLICAS}')"]},{"cell_type":"markdown","metadata":{"id":"A77GFE3xFPf8"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3zy8i_8FPf_","colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"status":"ok","timestamp":1672270961425,"user_tz":300,"elapsed":3686,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"6ab9c46b-15f9-400c-9494-54da561a4207"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["             1    2    3    4    7    8   10   13   15   16  ...  834  835  \\\n","Sample_id                                                    ...             \n","HG00097    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","HG00099    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","HG00100    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","HG00101    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","HG00102    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0   \n","...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n","NA21137    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0   \n","NA21141    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","NA21142    0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0   \n","NA21143    0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0   \n","NA21144    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0   \n","\n","           836  837  838  839  840  842  843  846  \n","Sample_id                                          \n","HG00097    0|1  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00099    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00100    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00101    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00102    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","...        ...  ...  ...  ...  ...  ...  ...  ...  \n","NA21137    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21141    1|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21142    1|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21143    0|1  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21144    0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","\n","[2503 rows x 572 columns]"],"text/html":["\n","  <div id=\"df-42bd4eb5-4ddc-43b5-ad2b-a46613f88b77\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>10</th>\n","      <th>13</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>...</th>\n","      <th>834</th>\n","      <th>835</th>\n","      <th>836</th>\n","      <th>837</th>\n","      <th>838</th>\n","      <th>839</th>\n","      <th>840</th>\n","      <th>842</th>\n","      <th>843</th>\n","      <th>846</th>\n","    </tr>\n","    <tr>\n","      <th>Sample_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>HG00097</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00099</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00100</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00101</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00102</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>NA21137</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21141</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21142</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21143</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21144</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2503 rows × 572 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42bd4eb5-4ddc-43b5-ad2b-a46613f88b77')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-42bd4eb5-4ddc-43b5-ad2b-a46613f88b77 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-42bd4eb5-4ddc-43b5-ad2b-a46613f88b77');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["root_dir = '[path]/'\n","new_data_header = \"\"\n","# get header\n","with open(root_dir + \"DELL.chr22.genotypes.for.modeling.vcf\", 'r') as f_in:\n","    # skip info\n","    for line_num in range(70):\n","        f_in.readline()\n","\n","    new_data_header = f_in.readline()\n","# load data\n","\n","# load genotype\n","genotypes = pd.read_csv(root_dir + \"DELL.chr22.genotypes.for.modeling.vcf\", comment='#', sep='\\t', names=new_data_header.strip().split('\\t'), header=1, index_col='Sample_id', dtype={'Sample_id':str}).iloc[:, :-1]\n","headers = genotypes.columns[:]\n","genotypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gFMrUnhV80n"},"outputs":[],"source":["ped_file = '[path]/integrated_call_samples.20130502.ALL.ped'\n","pedigree = pd.read_csv(ped_file, sep='\\t', index_col='Individual ID')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1672270961426,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"Zgb8Fn-EV_PU","outputId":"175b0b60-05a6-4955-dfb1-d34afd18b3fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              Family ID Paternal ID Maternal ID  Gender  Phenotype Population  \\\n","Individual ID                                                                   \n","HG00096         HG00096           0           0       1          0        GBR   \n","HG00097         HG00097           0           0       2          0        GBR   \n","HG00098         HG00098           0           0       1          0        GBR   \n","HG00099         HG00099           0           0       2          0        GBR   \n","HG00100         HG00100           0           0       2          0        GBR   \n","\n","              Relationship Siblings Second Order Third Order Children  \\\n","Individual ID                                                           \n","HG00096              unrel        0            0           0        0   \n","HG00097              unrel        0            0           0        0   \n","HG00098              unrel        0            0           0        0   \n","HG00099              unrel        0            0           0        0   \n","HG00100              unrel        0            0           0        0   \n","\n","              Other Comments  \n","Individual ID                 \n","HG00096                    0  \n","HG00097                    0  \n","HG00098                    0  \n","HG00099                    0  \n","HG00100                    0  "],"text/html":["\n","  <div id=\"df-ed5a6a69-336c-4eb8-8dcc-7a928c94217c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Family ID</th>\n","      <th>Paternal ID</th>\n","      <th>Maternal ID</th>\n","      <th>Gender</th>\n","      <th>Phenotype</th>\n","      <th>Population</th>\n","      <th>Relationship</th>\n","      <th>Siblings</th>\n","      <th>Second Order</th>\n","      <th>Third Order</th>\n","      <th>Children</th>\n","      <th>Other Comments</th>\n","    </tr>\n","    <tr>\n","      <th>Individual ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>HG00096</th>\n","      <td>HG00096</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>GBR</td>\n","      <td>unrel</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00097</th>\n","      <td>HG00097</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>GBR</td>\n","      <td>unrel</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00098</th>\n","      <td>HG00098</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>GBR</td>\n","      <td>unrel</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00099</th>\n","      <td>HG00099</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>GBR</td>\n","      <td>unrel</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00100</th>\n","      <td>HG00100</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>GBR</td>\n","      <td>unrel</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed5a6a69-336c-4eb8-8dcc-7a928c94217c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ed5a6a69-336c-4eb8-8dcc-7a928c94217c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed5a6a69-336c-4eb8-8dcc-7a928c94217c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["pedigree.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1672270961427,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"GBSfo4FlV_A_","outputId":"6258fd12-d440-48c5-bb32-5e87f176925e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2503,)"]},"metadata":{},"execution_count":9}],"source":["Y_train = pedigree.loc[genotypes.index]['Population']\n","# Y_train = pedigree.loc[genotypes.index][pedigree.loc[genotypes.index]['Population'] == \"YRI\"]['Population']\n","# Y_train = pd.concat((Y_train, Y_train))\n","Y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1672270961427,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"hiP0EEwsEMNP","outputId":"1da2a70e-c117-4d8c-db49-42a83a2dbd8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2503, 572)"]},"metadata":{},"execution_count":10}],"source":["X = genotypes[genotypes.index.isin(Y_train.index)]\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1043,"status":"ok","timestamp":1672270962462,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"DNU6deVHXIdE","outputId":"c2f2684e-a531-476d-ebde-58677ce3a809"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2503, 572)"]},"metadata":{},"execution_count":11}],"source":["X = X.replace({\n","    '0|0': 0,\n","    '0|1': 1,\n","    '1|0': 2,\n","    '1|1': 3\n","})\n","X.shape"]},{"cell_type":"markdown","source":["## LD"],"metadata":{"id":"Utkz6Di_pMRJ"}},{"cell_type":"code","source":["r = allel.rogers_huff_r(X.T)\n","LD = squareform(r ** 2)\n","LD.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkgK-B2vnSqg","executionInfo":{"status":"ok","timestamp":1672270963101,"user_tz":300,"elapsed":641,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"e4d9a62c-b589-4df0-b89a-a76864f7ced7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(572, 572)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.imshow(LD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"juhDH1Hfnb5Q","executionInfo":{"status":"ok","timestamp":1672270963102,"user_tz":300,"elapsed":7,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"fbfceda0-6e8b-4e29-ca3a-37122751f43a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fac3b3c0a00>"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n","plt.hist(np.amax(LD, axis=1), bins=bins)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"VPcxBWGbn1dq","executionInfo":{"status":"ok","timestamp":1672270963676,"user_tz":300,"elapsed":579,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"7818d406-fddd-4196-90fd-3546a7d3294c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([421.,  44.,  29.,  31.,  47.]),\n"," array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]),\n"," <a list of 5 Patch objects>)"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["LD_max_freqs = np.amax(LD, axis=1)\n","bin_labels = np.digitize(LD_max_freqs, bins=bins, right=True)\n","bin_general_labels, bin_counts = np.unique(bin_labels, return_counts=True)\n","bin_general_labels, bin_counts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWiEzZuzeABb","executionInfo":{"status":"ok","timestamp":1672270963677,"user_tz":300,"elapsed":6,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"b3a10fd3-b0b3-4898-fc99-259d333fb6f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1, 2, 3, 4, 5]), array([421,  44,  29,  31,  47]))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pZoO-FvKdr3"},"outputs":[],"source":["# hyperparameters\n","feature_size = X.shape[1]\n","inChannel = 5\n","# optimizer learning rate\n","learning_rate = 0.001\n","epochs = 60 \n","#epochs = 100   # chr20 LOS 5K\n","\n","\n","# training batch size\n","#batch_size = 32   # u19, 4984 samples\n","bs = 32\n","\n","\n","lr = 1e-3\n","\n","\n","\n","# l1 regulalization\n","kr = 1e-4\n","k_initial = 'glorot_uniform'\n","\n","\n","# channel = inChannel\n","channel = inChannel\n","\n","ndf_num = 128\n","kernel_len = 32\n","num_latent = ndf_num*4\n","p_size = 2\n","\n","\n","\n","#dr_rate = drop_prec\n","dr_rate = 0.2  # avoid overfitting for missing ratio of 0.7"]},{"cell_type":"markdown","metadata":{"id":"OCkow4q5MJk_"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgGz9_6u8u-O"},"outputs":[],"source":["# AE model in one cell\n","# # V1: verify our own ae model with yeast genotype data\n","\n","# # 2.2.8 build variatial autoencoder for snp with subclassing function. \n","\n","class SNP_ENCODER(tf.keras.Model):    \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):    \n","        super(SNP_ENCODER, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate\n","\n","        # object, can be saved in tf mode\n","        self.stride=1\n","        self.kl = kernel_len\n","\n","        \n","    def build(self, inputs): \n","        #encoder\n","        # dense layer 1\n","        self.c1 = layers.Conv1D(filters=self.ndf, kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr), \n","                      input_shape=(self.feature_size, self.channel))\n","        self.p1 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop1 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 2\n","        self.c2 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n","                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n","        self.p2 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop2 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 3\n","        self.c3 = layers.Conv1D(filters=(4*self.ndf), kernel_size=self.kl, strides=1, padding=\"same\", \n","                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n","                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n","        self.p3 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop3 = layers.Dropout(rate=self.dr)\n","\n","        super(SNP_ENCODER, self).build(inputs)\n","    \n","    def call(self, inputs, training=True):\n","        #print('SNP_ENCODER training flag: ', training)\n","        x = self.c1(inputs)\n","        x = self.p1(x)\n","        x = self.drop1(x, training=training)\n","\n","        x = self.c2(x)\n","        x = self.p2(x)\n","        x = self.drop2(x, training=training)\n","        \n","        x = self.c3(x)\n","        return x\n","\n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size, self.channel))\n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    \n","    \n","    \n","# SNP_DECODER(keras.Model):   \n","class SNP_DECODER(tf.keras.Model):     \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):\n","        super(SNP_DECODER, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate\n","        \n","        # object, can be saved in tf mode\n","        self.stride=1\n","        self.kl=kernel_len\n","        \n","    def build(self, inputs):\n","        #decoder        \n","        self.c1 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr),\n","                      input_shape=((self.feature_size>>2), self.n_latent))\n","        \n","        \n","        self.s1 = layers.UpSampling1D(size=p_size)\n","        self.drop1 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 2\n","        self.c2 = layers.Conv1D(filters=(1*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr))\n","\n","        self.s2 = layers.UpSampling1D(size=p_size)\n","        self.drop2 = layers.Dropout(rate=self.dr)\n","        \n","\n","        # dense layer6\n","        self.c3 = layers.Conv1D(filters=self.channel, kernel_size=self.kl, strides=1, padding=\"same\", \n","                      activation='softmax', use_bias=True)\n","        \n","        super(SNP_DECODER, self).build(inputs)\n","        \n","    def call(self, inputs, training = True):\n","        #print('SNP_DECODER training flag: ', training)\n","        x = self.c1(inputs)\n","        x = self.s1(x)\n","        x = self.drop1(x, training=training)\n","        \n","        x = self.c2(x)\n","        x = self.s2(x)\n","        x = self.drop2(x, training=training)\n","\n","        \n","        d_out = self.c3(x)\n","        return d_out\n","    \n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size>>2, self.n_latent))\n","        \n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    \n","\n","    \n","#class SNP_AE(keras.Model):\n","class SNP_AE(tf.keras.Model):    \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):    \n","        super(SNP_AE, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate      \n","        \n","        self.encoder = SNP_ENCODER(self.feature_size)\n","        self.decoder = SNP_DECODER(self.feature_size)\n","    \n","\n","    def call(self, x, training=True): \n","        latent = self.encoder(x, training)     \n","        res = self.decoder(latent, training)\n","\n","        return res, latent\n","    \n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size, self.channel))\n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    "]},{"cell_type":"code","source":["# plot snp vae encoder model\n","\n","SNP_encoder = SNP_ENCODER(feature_size)\n","SNP_encoder.build((None, feature_size, channel))\n","SNP_encoder.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaAkHl0XKyIM","executionInfo":{"status":"ok","timestamp":1672270967054,"user_tz":300,"elapsed":7,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"2054d755-d5e5-44ab-e55a-6ae6c74d811d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 572, 5)]          0         \n","                                                                 \n"," conv1d (Conv1D)             (None, 572, 128)          20608     \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 286, 128)         0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 286, 128)          0         \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 286, 256)          1048832   \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 143, 256)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 143, 256)          0         \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 143, 512)          4194816   \n","                                                                 \n","=================================================================\n","Total params: 5,264,256\n","Trainable params: 5,264,256\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# plot snp vae decoder model\n","\n","SNP_decoder = SNP_DECODER(feature_size)\n","\n","SNP_decoder.build((None, feature_size>>2, num_latent))\n","SNP_decoder.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SEoV7YaLzwv","executionInfo":{"status":"ok","timestamp":1672270967639,"user_tz":300,"elapsed":588,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"7fccb495-be38-4628-9cbf-0fe2ab590a97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 143, 512)]        0         \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 143, 256)          4194560   \n","                                                                 \n"," up_sampling1d (UpSampling1D  (None, 286, 256)         0         \n"," )                                                               \n","                                                                 \n"," dropout_3 (Dropout)         (None, 286, 256)          0         \n","                                                                 \n"," conv1d_4 (Conv1D)           (None, 286, 128)          1048704   \n","                                                                 \n"," up_sampling1d_1 (UpSampling  (None, 572, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 572, 128)          0         \n","                                                                 \n"," conv1d_5 (Conv1D)           (None, 572, 5)            20485     \n","                                                                 \n","=================================================================\n","Total params: 5,263,749\n","Trainable params: 5,263,749\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# plot snp vae model\n","\n","SNP_ae = SNP_AE(feature_size)\n","SNP_ae.build((None, feature_size, channel))\n","SNP_ae.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmP5dBy-L2Ep","executionInfo":{"status":"ok","timestamp":1672270968383,"user_tz":300,"elapsed":746,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"dfcd1102-d5a5-43ff-d586-3f299026f4d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 572, 5)]          0         \n","                                                                 \n"," snp_encoder_1 (SNP_ENCODER)  (None, 143, 512)         5264256   \n","                                                                 \n"," snp_decoder_1 (SNP_DECODER)  (None, 572, 5)           5263749   \n","                                                                 \n","=================================================================\n","Total params: 10,528,005\n","Trainable params: 10,528,005\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# AE model sub-functions\n","\n","def generate_fake_missing(x_in, missing_ratio=0.5):\n","\n","        # Generates missing genotypes\n","        # different missing loci for each individuals\n","        x_fake = x_in.copy()   # with .copy() to not overwrite the original data\n","        \n","        for i in range(x_in.shape[0]):\n","            missing_size = int(missing_ratio * x_in.shape[1])\n","          \n","            # without repeat random numbers: set replace with false\n","            missing_index = np.random.choice(x_in.shape[1], size=missing_size, replace=False)\n","            \n","            # missing loci are encoded as [0, 0]\n","            # x_fake[i, missing_index, :] = [0, 0, 1]  # yeast\n","            x_fake[i, missing_index, :] = [0, 0, 0, 0, 1]  # human\n","\n","        return x_fake\n","        #return x_fake, x_in\n","    \n","    \n","\n","def loss_function_cce(recon_x, x):\n","    # orders: y_true, y_pred\n","    cce = tf.keras.losses.categorical_crossentropy(x, recon_x)\n","\n","    #cce = np.double(cce)\n","    cce = K.cast(cce, dtype='float32')\n","\n","    lamb1 = 1.0\n","    loss = lamb1*cce\n","    #print('loss:', loss)\n","    \n","    loss = tf.reduce_mean(loss)\n","    #print('ave loss:', loss)\n","    \n","    return loss\n"],"metadata":{"id":"-EXc-erqK16d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TOA_NexzN5Qq"},"source":["## Training"]},{"cell_type":"code","source":["# With constraint\n","N_SPLITS=3\n","results = None\n","kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=2022)\n","fold = 0\n","_x = tf.keras.utils.to_categorical(X[X.index.isin(Y_train.index)].to_numpy(), inChannel)\n","_y = Y_train.to_numpy()\n","for train_index, test_index in kf.split(_x):\n","  fold += 1\n","  accuracies = []\n","  print(f\"Training using fold {fold}\")\n","  print(\"*******************************************\")\n","  print(\"*******************************************\")\n","  \n","  x_train, y_train, test_dataset, test_indices = _x[train_index], _y[train_index], (_x[test_index], _y[test_index]),Y_train.index[test_index]\n","  x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10,\n","                                      random_state=fold,\n","                                      shuffle=True,\n","                                      stratify=y_train)\n","  \n","  \n","  for missing_ratio in [\n","                        0.05,\n","                        0.1,\n","                        0.2\n","                        ]:\n","    train_X = np.copy(x_train)\n","    valid_X = np.copy(x_valid)\n","    print(f\"Missing rate {missing_ratio}\")\n","    print(\"=====================================================\")\n","    train_X_fake = generate_fake_missing(train_X, missing_ratio)\n","\n","    diff = np.absolute(np.array(train_X) - np.array(train_X_fake))\n","    print('train_X_fake diff:', np.sum(diff))\n","\n","\n","    valid_X_fake = generate_fake_missing(valid_X, missing_ratio)\n","    diff = np.absolute(np.array(valid_X) - np.array(valid_X_fake))\n","    print('valid_X_fake diff:', np.sum(diff))\n","\n","\n","    K.clear_session()\n","    with strategy.scope():\n","      model = SNP_AE(feature_size)\n","      optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","      train_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n","      val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n","\n","      train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","      val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","\n","      for epoch in range(epochs):\n","        print('Start of epoch %d' % (epoch,))\n","\n","        #----shuffle train data and lablel for each epoch-----------------------------------------------------------------------------------#    \n","        # shuffle data and labels at the same time\n","        idx = train_X.shape[0]\n","\n","\n","        indices = tf.range(start=0, limit=idx, dtype=tf.int32)\n","\n","        shuffled_indices = tf.random.shuffle(indices)  \n","      \n","        train_X_fake = tf.gather(train_X_fake, shuffled_indices)\n","        train_X = tf.gather(train_X, shuffled_indices)        \n","        \n","        snp_x = tf.data.Dataset.from_tensor_slices(train_X_fake).batch(bs, drop_remainder=True) \n","        snp_y = tf.data.Dataset.from_tensor_slices(train_X).batch(bs, drop_remainder=True)\n","\n","        snp_x_v = tf.data.Dataset.from_tensor_slices(valid_X_fake).batch(bs, drop_remainder=True)\n","        snp_y_v = tf.data.Dataset.from_tensor_slices(valid_X).batch(bs, drop_remainder=True)\n","\n","        loss_batch = []\n","\n","        # Iterate over the batches of the dataset.\n","        for step, (snp_fake_batch, snp_label_batch) in enumerate(zip(snp_x, snp_y)): \n","          with tf.GradientTape() as tape:\n","              recon_inputs, latents= model(snp_fake_batch, training=True)\n","              loss = loss_function_cce(recon_inputs, snp_label_batch)\n","\n","\n","          grads = tape.gradient(loss, model.trainable_variables)\n","          optimizer.apply_gradients(zip(grads, model.trainable_variables))    \n","\n","          loss_batch.append(loss.numpy())\n","\n","\n","          # Update training metric.\n","          train_acc_metric.update_state(snp_label_batch, recon_inputs)\n","          train_loss_metric.update_state(snp_label_batch, recon_inputs)\n","\n","\n","        # Display metrics at the end of each epoch.\n","        train_loss = train_loss_metric.result()\n","        print(\"Training loss over epoch: \", epoch, train_loss.numpy())\n","\n","        train_acc = train_acc_metric.result()\n","        print(\"Training acc over epoch: \", epoch, train_acc.numpy())\n","\n","\n","        # Reset training metrics at the end of each epoch\n","        train_loss_metric.reset_states()\n","        train_acc_metric.reset_states()\n","\n","\n","        # Run a validation loop at the end of each epoch.\n","        for x_batch_val, y_batch_val in zip(snp_x_v, snp_y_v):   \n","\n","            val_recons, latents = model(x_batch_val, training=False)\n","            # Update val metrics\n","            val_loss_metric.update_state(y_batch_val, val_recons)\n","            val_acc_metric.update_state(y_batch_val, val_recons)\n","\n","        val_loss = val_loss_metric.result()\n","        val_acc = val_acc_metric.result()\n","\n","        val_loss_metric.reset_states()\n","        val_acc_metric.reset_states()\n","        #print(\"Validation acc: %.4f\" % (float(val_acc),))\n","        print(\"Validation loss: \", epoch, val_loss.numpy())\n","        print(\"Validation acc: \", epoch, val_acc.numpy())\n","\n","\n","        #print('epoch %s: batch loss = %s' % (epoch, loss_batch))\n","        loss_epoch = np.mean(loss_batch)    \n","        print('epoch %s: loss = %s' % (epoch, loss_epoch))    \n","    \n","    save_name = f\"[path]/Chr.22.DELS/AE/preds_mixed_mr_{missing_ratio}_fold_{fold}_.csv\"\n","    avg_accuracy = []\n","    preds = []\n","    true_labels = []\n","    \n","    to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n","    test_X_missing = np.copy(test_dataset[0])\n","    for i in tqdm(list(range(test_dataset[0].shape[0]))):\n","      missing_index, _ = train_test_split(np.arange(x_train.shape[1]), train_size=missing_ratio,\n","                                    random_state=i + fold,\n","                                    shuffle=True,\n","                                    stratify=bin_labels\n","                                    )\n","      test_X_missing[i:i+1, missing_index, :] = [0, 0, 0, 0, 1]\n","      # predict\n","      predict_onehot, _ = model(test_X_missing[i:i+1, :, :], training=False)\n","      predict_onehot = predict_onehot.numpy()\n","      # only care the missing position\n","      predict_missing_onehot = predict_onehot[0:1, missing_index, :]\n","      # predict label\n","      predict_missing = np.argmax(predict_missing_onehot, axis=2)\n","      \n","      preds.extend(predict_missing.ravel().tolist())\n","      \n","      predict_haplotypes = np.argmax(predict_onehot, axis=2)\n","      # Only for haploids\n","      to_save_array[i] = predict_haplotypes\n","      # real label\n","      label_missing_onehot = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n","      label_missing = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n","      true_labels.extend(label_missing.ravel().tolist())\n","      # accuracy\n","      correct_prediction = np.equal(predict_missing, label_missing)\n","      accuracy = np.mean(correct_prediction)\n","\n","      avg_accuracy.append(accuracy)\n","\n","    df = pd.DataFrame(to_save_array, columns= headers[:], index = Y_train.index[test_index])\n","    df.to_csv(save_name)\n","    print('The average imputation accuracy' \\\n","          'on test data with {} missing genotypes is {:.4f}: '\n","        .format(missing_ratio, np.mean(avg_accuracy)))\n","    cnf_matrix = confusion_matrix(true_labels, preds)\n","    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n","    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n","    TP = np.diag(cnf_matrix)\n","    TN = cnf_matrix.sum() - (FP + FN + TP)\n","    FP = FP.astype(float)\n","    FN = FN.astype(float)\n","    TP = TP.astype(float)\n","    TN = TN.astype(float)\n","    # Sensitivity, hit rate, recall, or true positive rate\n","    TPR = TP/(TP+FN)\n","    # Specificity or true negative rate\n","    TNR = TN/(TN+FP)\n","    print(f\"Sensitivity: {np.mean(TPR)}\")\n","    print(f\"Specificity: {np.mean(TNR)}\")\n","    print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n","    print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n","    accuracies.append(np.mean(avg_accuracy))\n","        \n","    "],"metadata":{"id":"Gq5VWuKWVZVx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672272914252,"user_tz":300,"elapsed":1945873,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"170bafb2-cea6-43e7-c662-a6e8293b67ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training using fold 1\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 84056.0\n","valid_X_fake diff: 9352.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.0853944\n","Training acc over epoch:  0 0.88192886\n","Validation loss:  0 0.24688941\n","Validation acc:  0 0.95725524\n","epoch 0: loss = 1.0853946\n","Start of epoch 1\n","Training loss over epoch:  1 0.23371829\n","Training acc over epoch:  1 0.9581773\n","Validation loss:  1 0.21783091\n","Validation acc:  1 0.95725524\n","epoch 1: loss = 0.23371828\n","Start of epoch 2\n","Training loss over epoch:  2 0.21441758\n","Training acc over epoch:  2 0.95818806\n","Validation loss:  2 0.2108404\n","Validation acc:  2 0.95725524\n","epoch 2: loss = 0.21441762\n","Start of epoch 3\n","Training loss over epoch:  3 0.21018921\n","Training acc over epoch:  3 0.9582035\n","Validation loss:  3 0.2078056\n","Validation acc:  3 0.95725524\n","epoch 3: loss = 0.21018918\n","Start of epoch 4\n","Training loss over epoch:  4 0.20508015\n","Training acc over epoch:  4 0.95819277\n","Validation loss:  4 0.19786431\n","Validation acc:  4 0.95725524\n","epoch 4: loss = 0.20508018\n","Start of epoch 5\n","Training loss over epoch:  5 0.19666807\n","Training acc over epoch:  5 0.95819515\n","Validation loss:  5 0.19122715\n","Validation acc:  5 0.95725524\n","epoch 5: loss = 0.19666806\n","Start of epoch 6\n","Training loss over epoch:  6 0.19276655\n","Training acc over epoch:  6 0.958213\n","Validation loss:  6 0.18604517\n","Validation acc:  6 0.95725524\n","epoch 6: loss = 0.19276655\n","Start of epoch 7\n","Training loss over epoch:  7 0.1859513\n","Training acc over epoch:  7 0.95818686\n","Validation loss:  7 0.18108582\n","Validation acc:  7 0.95725524\n","epoch 7: loss = 0.18595134\n","Start of epoch 8\n","Training loss over epoch:  8 0.17964727\n","Training acc over epoch:  8 0.9582011\n","Validation loss:  8 0.17145461\n","Validation acc:  8 0.95725524\n","epoch 8: loss = 0.17964725\n","Start of epoch 9\n","Training loss over epoch:  9 0.17450948\n","Training acc over epoch:  9 0.958156\n","Validation loss:  9 0.16644667\n","Validation acc:  9 0.95725524\n","epoch 9: loss = 0.17450944\n","Start of epoch 10\n","Training loss over epoch:  10 0.16981344\n","Training acc over epoch:  10 0.9582807\n","Validation loss:  10 0.16101512\n","Validation acc:  10 0.9573099\n","epoch 10: loss = 0.16981342\n","Start of epoch 11\n","Training loss over epoch:  11 0.16462779\n","Training acc over epoch:  11 0.95848256\n","Validation loss:  11 0.15158625\n","Validation acc:  11 0.9590909\n","epoch 11: loss = 0.1646278\n","Start of epoch 12\n","Training loss over epoch:  12 0.1563665\n","Training acc over epoch:  12 0.95909184\n","Validation loss:  12 0.14195672\n","Validation acc:  12 0.96019447\n","epoch 12: loss = 0.15636648\n","Start of epoch 13\n","Training loss over epoch:  13 0.14865698\n","Training acc over epoch:  13 0.95951825\n","Validation loss:  13 0.134305\n","Validation acc:  13 0.9609156\n","epoch 13: loss = 0.148657\n","Start of epoch 14\n","Training loss over epoch:  14 0.14007512\n","Training acc over epoch:  14 0.960194\n","Validation loss:  14 0.12335449\n","Validation acc:  14 0.9618116\n","epoch 14: loss = 0.14007512\n","Start of epoch 15\n","Training loss over epoch:  15 0.12971081\n","Training acc over epoch:  15 0.96160376\n","Validation loss:  15 0.10788987\n","Validation acc:  15 0.96584356\n","epoch 15: loss = 0.1297108\n","Start of epoch 16\n","Training loss over epoch:  16 0.11560164\n","Training acc over epoch:  16 0.9644447\n","Validation loss:  16 0.09001382\n","Validation acc:  16 0.9721482\n","epoch 16: loss = 0.115601644\n","Start of epoch 17\n","Training loss over epoch:  17 0.10273589\n","Training acc over epoch:  17 0.96771437\n","Validation loss:  17 0.07805221\n","Validation acc:  17 0.97695583\n","epoch 17: loss = 0.10273589\n","Start of epoch 18\n","Training loss over epoch:  18 0.09274181\n","Training acc over epoch:  18 0.97050416\n","Validation loss:  18 0.06822779\n","Validation acc:  18 0.9795564\n","epoch 18: loss = 0.0927418\n","Start of epoch 19\n","Training loss over epoch:  19 0.08407017\n","Training acc over epoch:  19 0.9732869\n","Validation loss:  19 0.061480116\n","Validation acc:  19 0.98150134\n","epoch 19: loss = 0.08407017\n","Start of epoch 20\n","Training loss over epoch:  20 0.076334566\n","Training acc over epoch:  20 0.9757869\n","Validation loss:  20 0.053221334\n","Validation acc:  20 0.9837303\n","epoch 20: loss = 0.07633457\n","Start of epoch 21\n","Training loss over epoch:  21 0.06988325\n","Training acc over epoch:  21 0.9780079\n","Validation loss:  21 0.048826605\n","Validation acc:  21 0.9856753\n","epoch 21: loss = 0.06988323\n","Start of epoch 22\n","Training loss over epoch:  22 0.065030485\n","Training acc over epoch:  22 0.9797134\n","Validation loss:  22 0.04552731\n","Validation acc:  22 0.98678976\n","epoch 22: loss = 0.065030485\n","Start of epoch 23\n","Training loss over epoch:  23 0.060845636\n","Training acc over epoch:  23 0.9810614\n","Validation loss:  23 0.04179816\n","Validation acc:  23 0.9885817\n","epoch 23: loss = 0.060845625\n","Start of epoch 24\n","Training loss over epoch:  24 0.058205593\n","Training acc over epoch:  24 0.981907\n","Validation loss:  24 0.03968989\n","Validation acc:  24 0.9888003\n","epoch 24: loss = 0.058205593\n","Start of epoch 25\n","Training loss over epoch:  25 0.055150915\n","Training acc over epoch:  25 0.98305076\n","Validation loss:  25 0.036277138\n","Validation acc:  25 0.9902753\n","epoch 25: loss = 0.055150922\n","Start of epoch 26\n","Training loss over epoch:  26 0.05274959\n","Training acc over epoch:  26 0.9837693\n","Validation loss:  26 0.035408095\n","Validation acc:  26 0.9902972\n","epoch 26: loss = 0.05274958\n","Start of epoch 27\n","Training loss over epoch:  27 0.050398946\n","Training acc over epoch:  27 0.9847313\n","Validation loss:  27 0.033358574\n","Validation acc:  27 0.9911495\n","epoch 27: loss = 0.050398942\n","Start of epoch 28\n","Training loss over epoch:  28 0.048877127\n","Training acc over epoch:  28 0.985122\n","Validation loss:  28 0.03261692\n","Validation acc:  28 0.9913899\n","epoch 28: loss = 0.048877124\n","Start of epoch 29\n","Training loss over epoch:  29 0.046807095\n","Training acc over epoch:  29 0.98573965\n","Validation loss:  29 0.031810824\n","Validation acc:  29 0.9917286\n","epoch 29: loss = 0.046807107\n","Start of epoch 30\n","Training loss over epoch:  30 0.045051295\n","Training acc over epoch:  30 0.986495\n","Validation loss:  30 0.030217623\n","Validation acc:  30 0.99182695\n","epoch 30: loss = 0.0450513\n","Start of epoch 31\n","Training loss over epoch:  31 0.043445837\n","Training acc over epoch:  31 0.9869392\n","Validation loss:  31 0.0302855\n","Validation acc:  31 0.99212193\n","epoch 31: loss = 0.043445848\n","Start of epoch 32\n","Training loss over epoch:  32 0.04205252\n","Training acc over epoch:  32 0.9873501\n","Validation loss:  32 0.02865644\n","Validation acc:  32 0.99273384\n","epoch 32: loss = 0.04205251\n","Start of epoch 33\n","Training loss over epoch:  33 0.040230434\n","Training acc over epoch:  33 0.987982\n","Validation loss:  33 0.028183794\n","Validation acc:  33 0.9930616\n","epoch 33: loss = 0.040230434\n","Start of epoch 34\n","Training loss over epoch:  34 0.039164767\n","Training acc over epoch:  34 0.9883632\n","Validation loss:  34 0.027626865\n","Validation acc:  34 0.9932474\n","epoch 34: loss = 0.039164774\n","Start of epoch 35\n","Training loss over epoch:  35 0.037931286\n","Training acc over epoch:  35 0.9886898\n","Validation loss:  35 0.025550941\n","Validation acc:  35 0.9936407\n","epoch 35: loss = 0.037931286\n","Start of epoch 36\n","Training loss over epoch:  36 0.03658586\n","Training acc over epoch:  36 0.98903424\n","Validation loss:  36 0.025285017\n","Validation acc:  36 0.9936189\n","epoch 36: loss = 0.036585856\n","Start of epoch 37\n","Training loss over epoch:  37 0.035648882\n","Training acc over epoch:  37 0.9893157\n","Validation loss:  37 0.023840535\n","Validation acc:  37 0.9939467\n","epoch 37: loss = 0.035648886\n","Start of epoch 38\n","Training loss over epoch:  38 0.034337427\n","Training acc over epoch:  38 0.9897682\n","Validation loss:  38 0.023032032\n","Validation acc:  38 0.99401224\n","epoch 38: loss = 0.034337427\n","Start of epoch 39\n","Training loss over epoch:  39 0.033595886\n","Training acc over epoch:  39 0.98992974\n","Validation loss:  39 0.02409574\n","Validation acc:  39 0.99396855\n","epoch 39: loss = 0.03359589\n","Start of epoch 40\n","Training loss over epoch:  40 0.033120245\n","Training acc over epoch:  40 0.9901293\n","Validation loss:  40 0.023119984\n","Validation acc:  40 0.9942854\n","epoch 40: loss = 0.033120237\n","Start of epoch 41\n","Training loss over epoch:  41 0.032482438\n","Training acc over epoch:  41 0.9903086\n","Validation loss:  41 0.022757988\n","Validation acc:  41 0.9940341\n","epoch 41: loss = 0.03248244\n","Start of epoch 42\n","Training loss over epoch:  42 0.03134989\n","Training acc over epoch:  42 0.99082047\n","Validation loss:  42 0.02240128\n","Validation acc:  42 0.99434006\n","epoch 42: loss = 0.031349882\n","Start of epoch 43\n","Training loss over epoch:  43 0.030717699\n","Training acc over epoch:  43 0.9908324\n","Validation loss:  43 0.02259218\n","Validation acc:  43 0.99434006\n","epoch 43: loss = 0.030717695\n","Start of epoch 44\n","Training loss over epoch:  44 0.030136677\n","Training acc over epoch:  44 0.99099624\n","Validation loss:  44 0.021944301\n","Validation acc:  44 0.99435097\n","epoch 44: loss = 0.03013668\n","Start of epoch 45\n","Training loss over epoch:  45 0.029123936\n","Training acc over epoch:  45 0.991368\n","Validation loss:  45 0.021714013\n","Validation acc:  45 0.9945367\n","epoch 45: loss = 0.029123936\n","Start of epoch 46\n","Training loss over epoch:  46 0.028857207\n","Training acc over epoch:  46 0.9914547\n","Validation loss:  46 0.020508684\n","Validation acc:  46 0.99484265\n","epoch 46: loss = 0.028857207\n","Start of epoch 47\n","Training loss over epoch:  47 0.027888073\n","Training acc over epoch:  47 0.9915794\n","Validation loss:  47 0.020504845\n","Validation acc:  47 0.99485356\n","epoch 47: loss = 0.02788807\n","Start of epoch 48\n","Training loss over epoch:  48 0.027809076\n","Training acc over epoch:  48 0.99169344\n","Validation loss:  48 0.020792434\n","Validation acc:  48 0.9947115\n","epoch 48: loss = 0.027809082\n","Start of epoch 49\n","Training loss over epoch:  49 0.026887037\n","Training acc over epoch:  49 0.9919155\n","Validation loss:  49 0.02040833\n","Validation acc:  49 0.9949082\n","epoch 49: loss = 0.026887035\n","Start of epoch 50\n","Training loss over epoch:  50 0.02623758\n","Training acc over epoch:  50 0.99224687\n","Validation loss:  50 0.019610172\n","Validation acc:  50 0.99509394\n","epoch 50: loss = 0.026237579\n","Start of epoch 51\n","Training loss over epoch:  51 0.025572103\n","Training acc over epoch:  51 0.99246305\n","Validation loss:  51 0.019550847\n","Validation acc:  51 0.99529064\n","epoch 51: loss = 0.025572103\n","Start of epoch 52\n","Training loss over epoch:  52 0.024881868\n","Training acc over epoch:  52 0.99260795\n","Validation loss:  52 0.019229647\n","Validation acc:  52 0.99530154\n","epoch 52: loss = 0.024881873\n","Start of epoch 53\n","Training loss over epoch:  53 0.02362093\n","Training acc over epoch:  53 0.9929156\n","Validation loss:  53 0.019349193\n","Validation acc:  53 0.9953453\n","epoch 53: loss = 0.023620928\n","Start of epoch 54\n","Training loss over epoch:  54 0.022846933\n","Training acc over epoch:  54 0.9930616\n","Validation loss:  54 0.018573817\n","Validation acc:  54 0.99546546\n","epoch 54: loss = 0.022846932\n","Start of epoch 55\n","Training loss over epoch:  55 0.022922117\n","Training acc over epoch:  55 0.99299157\n","Validation loss:  55 0.019024797\n","Validation acc:  55 0.99549824\n","epoch 55: loss = 0.022922117\n","Start of epoch 56\n","Training loss over epoch:  56 0.022451159\n","Training acc over epoch:  56 0.9931828\n","Validation loss:  56 0.018692473\n","Validation acc:  56 0.99548733\n","epoch 56: loss = 0.022451151\n","Start of epoch 57\n","Training loss over epoch:  57 0.021915194\n","Training acc over epoch:  57 0.9933728\n","Validation loss:  57 0.018924007\n","Validation acc:  57 0.99541086\n","epoch 57: loss = 0.02191519\n","Start of epoch 58\n","Training loss over epoch:  58 0.021106303\n","Training acc over epoch:  58 0.99364716\n","Validation loss:  58 0.01842889\n","Validation acc:  58 0.9955857\n","epoch 58: loss = 0.0211063\n","Start of epoch 59\n","Training loss over epoch:  59 0.02101914\n","Training acc over epoch:  59 0.9935676\n","Validation loss:  59 0.018448085\n","Validation acc:  59 0.9953562\n","epoch 59: loss = 0.02101914\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:09<00:00, 90.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9621: \n","Sensitivity: 0.3715841180601184\n","Specificity: 0.8056639220163793\n","F1-score macro: 0.424443362719693\n","F1-score micro: 0.962147134302823\n","Missing rate 0.1\n","=====================================================\n","train_X_fake diff: 171114.0\n","valid_X_fake diff: 19038.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.2919366\n","Training acc over epoch:  0 0.89929855\n","Validation loss:  0 0.25116706\n","Validation acc:  0 0.95725524\n","epoch 0: loss = 1.2919364\n","Start of epoch 1\n","Training loss over epoch:  1 0.23733999\n","Training acc over epoch:  1 0.958213\n","Validation loss:  1 0.22303623\n","Validation acc:  1 0.95725524\n","epoch 1: loss = 0.23734002\n","Start of epoch 2\n","Training loss over epoch:  2 0.21692754\n","Training acc over epoch:  2 0.9582106\n","Validation loss:  2 0.21252362\n","Validation acc:  2 0.95725524\n","epoch 2: loss = 0.21692757\n","Start of epoch 3\n","Training loss over epoch:  3 0.20989211\n","Training acc over epoch:  3 0.95816547\n","Validation loss:  3 0.20667566\n","Validation acc:  3 0.95725524\n","epoch 3: loss = 0.20989212\n","Start of epoch 4\n","Training loss over epoch:  4 0.202644\n","Training acc over epoch:  4 0.95819634\n","Validation loss:  4 0.19706514\n","Validation acc:  4 0.95725524\n","epoch 4: loss = 0.20264402\n","Start of epoch 5\n","Training loss over epoch:  5 0.19742173\n","Training acc over epoch:  5 0.9582071\n","Validation loss:  5 0.19527097\n","Validation acc:  5 0.95725524\n","epoch 5: loss = 0.19742174\n","Start of epoch 6\n","Training loss over epoch:  6 0.19460645\n","Training acc over epoch:  6 0.95818686\n","Validation loss:  6 0.19041774\n","Validation acc:  6 0.95725524\n","epoch 6: loss = 0.1946065\n","Start of epoch 7\n","Training loss over epoch:  7 0.19089392\n","Training acc over epoch:  7 0.95821536\n","Validation loss:  7 0.18633671\n","Validation acc:  7 0.95725524\n","epoch 7: loss = 0.19089393\n","Start of epoch 8\n","Training loss over epoch:  8 0.18556851\n","Training acc over epoch:  8 0.9581738\n","Validation loss:  8 0.177539\n","Validation acc:  8 0.95725524\n","epoch 8: loss = 0.18556853\n","Start of epoch 9\n","Training loss over epoch:  9 0.1784138\n","Training acc over epoch:  9 0.9582237\n","Validation loss:  9 0.17046255\n","Validation acc:  9 0.95725524\n","epoch 9: loss = 0.17841378\n","Start of epoch 10\n","Training loss over epoch:  10 0.17258617\n","Training acc over epoch:  10 0.95828897\n","Validation loss:  10 0.16454455\n","Validation acc:  10 0.9573099\n","epoch 10: loss = 0.17258619\n","Start of epoch 11\n","Training loss over epoch:  11 0.16778335\n","Training acc over epoch:  11 0.9583222\n","Validation loss:  11 0.15654257\n","Validation acc:  11 0.95809656\n","epoch 11: loss = 0.16778333\n","Start of epoch 12\n","Training loss over epoch:  12 0.16129892\n","Training acc over epoch:  12 0.95877594\n","Validation loss:  12 0.14815533\n","Validation acc:  12 0.95932037\n","epoch 12: loss = 0.16129892\n","Start of epoch 13\n","Training loss over epoch:  13 0.15513471\n","Training acc over epoch:  13 0.95923793\n","Validation loss:  13 0.14151698\n","Validation acc:  13 0.95980114\n","epoch 13: loss = 0.1551347\n","Start of epoch 14\n","Training loss over epoch:  14 0.14895608\n","Training acc over epoch:  14 0.9596049\n","Validation loss:  14 0.13347672\n","Validation acc:  14 0.9609047\n","epoch 14: loss = 0.1489561\n","Start of epoch 15\n","Training loss over epoch:  15 0.14128533\n","Training acc over epoch:  15 0.960156\n","Validation loss:  15 0.12289609\n","Validation acc:  15 0.9624126\n","epoch 15: loss = 0.14128532\n","Start of epoch 16\n","Training loss over epoch:  16 0.13180289\n","Training acc over epoch:  16 0.9612819\n","Validation loss:  16 0.11115579\n","Validation acc:  16 0.9639423\n","epoch 16: loss = 0.1318029\n","Start of epoch 17\n","Training loss over epoch:  17 0.12313632\n","Training acc over epoch:  17 0.96262044\n","Validation loss:  17 0.10101528\n","Validation acc:  17 0.9672858\n","epoch 17: loss = 0.123136334\n","Start of epoch 18\n","Training loss over epoch:  18 0.118023045\n","Training acc over epoch:  18 0.9639304\n","Validation loss:  18 0.097124174\n","Validation acc:  18 0.96904504\n","epoch 18: loss = 0.11802305\n","Start of epoch 19\n","Training loss over epoch:  19 0.109821975\n","Training acc over epoch:  19 0.9659376\n","Validation loss:  19 0.08657896\n","Validation acc:  19 0.97265077\n","epoch 19: loss = 0.10982198\n","Start of epoch 20\n","Training loss over epoch:  20 0.10319737\n","Training acc over epoch:  20 0.96789724\n","Validation loss:  20 0.08040256\n","Validation acc:  20 0.9754589\n","epoch 20: loss = 0.103197366\n","Start of epoch 21\n","Training loss over epoch:  21 0.09697814\n","Training acc over epoch:  21 0.9700113\n","Validation loss:  21 0.074698776\n","Validation acc:  21 0.97838724\n","epoch 21: loss = 0.09697816\n","Start of epoch 22\n","Training loss over epoch:  22 0.0912803\n","Training acc over epoch:  22 0.9722097\n","Validation loss:  22 0.06843228\n","Validation acc:  22 0.98067087\n","epoch 22: loss = 0.0912803\n","Start of epoch 23\n","Training loss over epoch:  23 0.082796045\n","Training acc over epoch:  23 0.97485816\n","Validation loss:  23 0.06224965\n","Validation acc:  23 0.982736\n","epoch 23: loss = 0.082796045\n","Start of epoch 24\n","Training loss over epoch:  24 0.07493639\n","Training acc over epoch:  24 0.9776896\n","Validation loss:  24 0.055902217\n","Validation acc:  24 0.98510706\n","epoch 24: loss = 0.0749364\n","Start of epoch 25\n","Training loss over epoch:  25 0.06851961\n","Training acc over epoch:  25 0.97979534\n","Validation loss:  25 0.05253268\n","Validation acc:  25 0.9862653\n","epoch 25: loss = 0.068519615\n","Start of epoch 26\n","Training loss over epoch:  26 0.06390979\n","Training acc over epoch:  26 0.9811968\n","Validation loss:  26 0.047170766\n","Validation acc:  26 0.98793703\n","epoch 26: loss = 0.063909784\n","Start of epoch 27\n","Training loss over epoch:  27 0.060469624\n","Training acc over epoch:  27 0.982413\n","Validation loss:  27 0.046200704\n","Validation acc:  27 0.9886145\n","epoch 27: loss = 0.060469624\n","Start of epoch 28\n","Training loss over epoch:  28 0.05750093\n","Training acc over epoch:  28 0.98357093\n","Validation loss:  28 0.04199874\n","Validation acc:  28 0.98952144\n","epoch 28: loss = 0.05750092\n","Start of epoch 29\n","Training loss over epoch:  29 0.055696297\n","Training acc over epoch:  29 0.9839807\n","Validation loss:  29 0.039318774\n","Validation acc:  29 0.98999125\n","epoch 29: loss = 0.0556963\n","Start of epoch 30\n","Training loss over epoch:  30 0.05417152\n","Training acc over epoch:  30 0.9845995\n","Validation loss:  30 0.038703594\n","Validation acc:  30 0.99036276\n","epoch 30: loss = 0.05417152\n","Start of epoch 31\n","Training loss over epoch:  31 0.052003257\n","Training acc over epoch:  31 0.98528236\n","Validation loss:  31 0.037734006\n","Validation acc:  31 0.99076706\n","epoch 31: loss = 0.052003264\n","Start of epoch 32\n","Training loss over epoch:  32 0.05042269\n","Training acc over epoch:  32 0.9856351\n","Validation loss:  32 0.036668707\n","Validation acc:  32 0.99092\n","epoch 32: loss = 0.05042269\n","Start of epoch 33\n","Training loss over epoch:  33 0.049624387\n","Training acc over epoch:  33 0.9858453\n","Validation loss:  33 0.03523145\n","Validation acc:  33 0.9911495\n","epoch 33: loss = 0.049624395\n","Start of epoch 34\n","Training loss over epoch:  34 0.048128143\n","Training acc over epoch:  34 0.98639524\n","Validation loss:  34 0.035473235\n","Validation acc:  34 0.9912806\n","epoch 34: loss = 0.04812813\n","Start of epoch 35\n","Training loss over epoch:  35 0.046926633\n","Training acc over epoch:  35 0.9868382\n","Validation loss:  35 0.034547057\n","Validation acc:  35 0.9913571\n","epoch 35: loss = 0.04692663\n","Start of epoch 36\n","Training loss over epoch:  36 0.046347965\n","Training acc over epoch:  36 0.98697716\n","Validation loss:  36 0.033835147\n","Validation acc:  36 0.99206734\n","epoch 36: loss = 0.046347965\n","Start of epoch 37\n","Training loss over epoch:  37 0.045763914\n","Training acc over epoch:  37 0.9870841\n","Validation loss:  37 0.03262622\n","Validation acc:  37 0.9920018\n","epoch 37: loss = 0.045763917\n","Start of epoch 38\n","Training loss over epoch:  38 0.044820808\n","Training acc over epoch:  38 0.98741186\n","Validation loss:  38 0.03326127\n","Validation acc:  38 0.99203455\n","epoch 38: loss = 0.044820808\n","Start of epoch 39\n","Training loss over epoch:  39 0.043952156\n","Training acc over epoch:  39 0.9875615\n","Validation loss:  39 0.032846313\n","Validation acc:  39 0.9920236\n","epoch 39: loss = 0.043952163\n","Start of epoch 40\n","Training loss over epoch:  40 0.042981252\n","Training acc over epoch:  40 0.98782516\n","Validation loss:  40 0.031748094\n","Validation acc:  40 0.99229676\n","epoch 40: loss = 0.042981263\n","Start of epoch 41\n","Training loss over epoch:  41 0.04217681\n","Training acc over epoch:  41 0.98792374\n","Validation loss:  41 0.031587258\n","Validation acc:  41 0.99232954\n","epoch 41: loss = 0.042176813\n","Start of epoch 42\n","Training loss over epoch:  42 0.04150521\n","Training acc over epoch:  42 0.98821235\n","Validation loss:  42 0.030482464\n","Validation acc:  42 0.9927775\n","epoch 42: loss = 0.041505206\n","Start of epoch 43\n","Training loss over epoch:  43 0.041317347\n","Training acc over epoch:  43 0.9882907\n","Validation loss:  43 0.03047124\n","Validation acc:  43 0.9926136\n","epoch 43: loss = 0.041317344\n","Start of epoch 44\n","Training loss over epoch:  44 0.040333107\n","Training acc over epoch:  44 0.9887302\n","Validation loss:  44 0.029835328\n","Validation acc:  44 0.99290866\n","epoch 44: loss = 0.04033311\n","Start of epoch 45\n","Training loss over epoch:  45 0.039653342\n","Training acc over epoch:  45 0.98859245\n","Validation loss:  45 0.029630167\n","Validation acc:  45 0.99286497\n","epoch 45: loss = 0.039653342\n","Start of epoch 46\n","Training loss over epoch:  46 0.039327435\n","Training acc over epoch:  46 0.98873377\n","Validation loss:  46 0.029828595\n","Validation acc:  46 0.99299604\n","epoch 46: loss = 0.03932743\n","Start of epoch 47\n","Training loss over epoch:  47 0.038687345\n","Training acc over epoch:  47 0.9889392\n","Validation loss:  47 0.029741628\n","Validation acc:  47 0.9928759\n","epoch 47: loss = 0.038687345\n","Start of epoch 48\n","Training loss over epoch:  48 0.037816655\n","Training acc over epoch:  48 0.98913044\n","Validation loss:  48 0.029649239\n","Validation acc:  48 0.9926792\n","epoch 48: loss = 0.037816655\n","Start of epoch 49\n","Training loss over epoch:  49 0.03738018\n","Training acc over epoch:  49 0.9892813\n","Validation loss:  49 0.029152626\n","Validation acc:  49 0.99315995\n","epoch 49: loss = 0.03738018\n","Start of epoch 50\n","Training loss over epoch:  50 0.03682224\n","Training acc over epoch:  50 0.9893644\n","Validation loss:  50 0.02926092\n","Validation acc:  50 0.9930179\n","epoch 50: loss = 0.036822237\n","Start of epoch 51\n","Training loss over epoch:  51 0.036688127\n","Training acc over epoch:  51 0.98949265\n","Validation loss:  51 0.029483892\n","Validation acc:  51 0.9933348\n","epoch 51: loss = 0.036688123\n","Start of epoch 52\n","Training loss over epoch:  52 0.0363487\n","Training acc over epoch:  52 0.9894226\n","Validation loss:  52 0.029403377\n","Validation acc:  52 0.9930944\n","epoch 52: loss = 0.036348697\n","Start of epoch 53\n","Training loss over epoch:  53 0.03576506\n","Training acc over epoch:  53 0.9896079\n","Validation loss:  53 0.028997358\n","Validation acc:  53 0.99315995\n","epoch 53: loss = 0.03576505\n","Start of epoch 54\n","Training loss over epoch:  54 0.03544988\n","Training acc over epoch:  54 0.9896732\n","Validation loss:  54 0.028906757\n","Validation acc:  54 0.99295235\n","epoch 54: loss = 0.035449885\n","Start of epoch 55\n","Training loss over epoch:  55 0.034368634\n","Training acc over epoch:  55 0.99007106\n","Validation loss:  55 0.028141424\n","Validation acc:  55 0.9934222\n","epoch 55: loss = 0.03436864\n","Start of epoch 56\n","Training loss over epoch:  56 0.033013046\n","Training acc over epoch:  56 0.99043924\n","Validation loss:  56 0.02906771\n","Validation acc:  56 0.99294144\n","epoch 56: loss = 0.03301305\n","Start of epoch 57\n","Training loss over epoch:  57 0.032660328\n","Training acc over epoch:  57 0.9905663\n","Validation loss:  57 0.028795244\n","Validation acc:  57 0.9930616\n","epoch 57: loss = 0.03266033\n","Start of epoch 58\n","Training loss over epoch:  58 0.031809356\n","Training acc over epoch:  58 0.9908288\n","Validation loss:  58 0.026828883\n","Validation acc:  58 0.9936298\n","epoch 58: loss = 0.031809356\n","Start of epoch 59\n","Training loss over epoch:  59 0.031143188\n","Training acc over epoch:  59 0.9908846\n","Validation loss:  59 0.027146287\n","Validation acc:  59 0.9938483\n","epoch 59: loss = 0.031143188\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:09<00:00, 90.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9616: \n","Sensitivity: 0.38089801137165913\n","Specificity: 0.8076388834950148\n","F1-score macro: 0.42770818668115357\n","F1-score micro: 0.9615505830444374\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 342228.0\n","valid_X_fake diff: 38076.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.0217011\n","Training acc over epoch:  0 0.90546256\n","Validation loss:  0 0.26182538\n","Validation acc:  0 0.95725524\n","epoch 0: loss = 1.0217011\n","Start of epoch 1\n","Training loss over epoch:  1 0.23926672\n","Training acc over epoch:  1 0.9581999\n","Validation loss:  1 0.22450897\n","Validation acc:  1 0.95725524\n","epoch 1: loss = 0.23926668\n","Start of epoch 2\n","Training loss over epoch:  2 0.22070871\n","Training acc over epoch:  2 0.95823914\n","Validation loss:  2 0.22117749\n","Validation acc:  2 0.95725524\n","epoch 2: loss = 0.22070874\n","Start of epoch 3\n","Training loss over epoch:  3 0.21824297\n","Training acc over epoch:  3 0.9582213\n","Validation loss:  3 0.21788287\n","Validation acc:  3 0.95725524\n","epoch 3: loss = 0.21824302\n","Start of epoch 4\n","Training loss over epoch:  4 0.21644269\n","Training acc over epoch:  4 0.95821416\n","Validation loss:  4 0.21686395\n","Validation acc:  4 0.95725524\n","epoch 4: loss = 0.21644275\n","Start of epoch 5\n","Training loss over epoch:  5 0.21459249\n","Training acc over epoch:  5 0.95819044\n","Validation loss:  5 0.21344982\n","Validation acc:  5 0.95725524\n","epoch 5: loss = 0.21459255\n","Start of epoch 6\n","Training loss over epoch:  6 0.21276355\n","Training acc over epoch:  6 0.95816904\n","Validation loss:  6 0.21123657\n","Validation acc:  6 0.95725524\n","epoch 6: loss = 0.21276353\n","Start of epoch 7\n","Training loss over epoch:  7 0.21084914\n","Training acc over epoch:  7 0.95819277\n","Validation loss:  7 0.21051016\n","Validation acc:  7 0.95725524\n","epoch 7: loss = 0.21084912\n","Start of epoch 8\n","Training loss over epoch:  8 0.2093446\n","Training acc over epoch:  8 0.9581619\n","Validation loss:  8 0.20855531\n","Validation acc:  8 0.95725524\n","epoch 8: loss = 0.20934461\n","Start of epoch 9\n","Training loss over epoch:  9 0.20747511\n","Training acc over epoch:  9 0.95821774\n","Validation loss:  9 0.20659436\n","Validation acc:  9 0.95725524\n","epoch 9: loss = 0.20747508\n","Start of epoch 10\n","Training loss over epoch:  10 0.20361435\n","Training acc over epoch:  10 0.95823795\n","Validation loss:  10 0.19943409\n","Validation acc:  10 0.95725524\n","epoch 10: loss = 0.20361432\n","Start of epoch 11\n","Training loss over epoch:  11 0.19753452\n","Training acc over epoch:  11 0.9581536\n","Validation loss:  11 0.19424601\n","Validation acc:  11 0.95725524\n","epoch 11: loss = 0.19753455\n","Start of epoch 12\n","Training loss over epoch:  12 0.18986237\n","Training acc over epoch:  12 0.95821655\n","Validation loss:  12 0.18367702\n","Validation acc:  12 0.95725524\n","epoch 12: loss = 0.18986236\n","Start of epoch 13\n","Training loss over epoch:  13 0.18240248\n","Training acc over epoch:  13 0.95818925\n","Validation loss:  13 0.17665371\n","Validation acc:  13 0.95725524\n","epoch 13: loss = 0.18240249\n","Start of epoch 14\n","Training loss over epoch:  14 0.17714687\n","Training acc over epoch:  14 0.9581714\n","Validation loss:  14 0.1714065\n","Validation acc:  14 0.95725524\n","epoch 14: loss = 0.17714685\n","Start of epoch 15\n","Training loss over epoch:  15 0.17105134\n","Training acc over epoch:  15 0.95823675\n","Validation loss:  15 0.16238247\n","Validation acc:  15 0.9575503\n","epoch 15: loss = 0.17105132\n","Start of epoch 16\n","Training loss over epoch:  16 0.16268213\n","Training acc over epoch:  16 0.95896477\n","Validation loss:  16 0.15011328\n","Validation acc:  16 0.95954984\n","epoch 16: loss = 0.16268207\n","Start of epoch 17\n","Training loss over epoch:  17 0.1501688\n","Training acc over epoch:  17 0.9602546\n","Validation loss:  17 0.14249066\n","Validation acc:  17 0.9619209\n","epoch 17: loss = 0.15016879\n","Start of epoch 18\n","Training loss over epoch:  18 0.13441353\n","Training acc over epoch:  18 0.9620717\n","Validation loss:  18 0.11423692\n","Validation acc:  18 0.96587634\n","epoch 18: loss = 0.13441354\n","Start of epoch 19\n","Training loss over epoch:  19 0.11655189\n","Training acc over epoch:  19 0.96532595\n","Validation loss:  19 0.098264255\n","Validation acc:  19 0.97095716\n","epoch 19: loss = 0.116551906\n","Start of epoch 20\n","Training loss over epoch:  20 0.10252473\n","Training acc over epoch:  20 0.9691455\n","Validation loss:  20 0.085328124\n","Validation acc:  20 0.97454107\n","epoch 20: loss = 0.102524705\n","Start of epoch 21\n","Training loss over epoch:  21 0.09239297\n","Training acc over epoch:  21 0.97244126\n","Validation loss:  21 0.07936198\n","Validation acc:  21 0.9774257\n","epoch 21: loss = 0.092392966\n","Start of epoch 22\n","Training loss over epoch:  22 0.08517527\n","Training acc over epoch:  22 0.9748439\n","Validation loss:  22 0.06899121\n","Validation acc:  22 0.98006994\n","epoch 22: loss = 0.085175276\n","Start of epoch 23\n","Training loss over epoch:  23 0.079192385\n","Training acc over epoch:  23 0.9769022\n","Validation loss:  23 0.066917896\n","Validation acc:  23 0.98168707\n","epoch 23: loss = 0.07919239\n","Start of epoch 24\n","Training loss over epoch:  24 0.073424675\n","Training acc over epoch:  24 0.97889507\n","Validation loss:  24 0.06022945\n","Validation acc:  24 0.9824738\n","epoch 24: loss = 0.07342468\n","Start of epoch 25\n","Training loss over epoch:  25 0.068782195\n","Training acc over epoch:  25 0.98041177\n","Validation loss:  25 0.05781187\n","Validation acc:  25 0.9837194\n","epoch 25: loss = 0.06878219\n","Start of epoch 26\n","Training loss over epoch:  26 0.06569918\n","Training acc over epoch:  26 0.98153526\n","Validation loss:  26 0.05386232\n","Validation acc:  26 0.9850415\n","epoch 26: loss = 0.06569918\n","Start of epoch 27\n","Training loss over epoch:  27 0.061774883\n","Training acc over epoch:  27 0.982736\n","Validation loss:  27 0.051475886\n","Validation acc:  27 0.98532563\n","epoch 27: loss = 0.061774876\n","Start of epoch 28\n","Training loss over epoch:  28 0.058641594\n","Training acc over epoch:  28 0.9837491\n","Validation loss:  28 0.049911693\n","Validation acc:  28 0.98579544\n","epoch 28: loss = 0.0586416\n","Start of epoch 29\n","Training loss over epoch:  29 0.055670846\n","Training acc over epoch:  29 0.98467785\n","Validation loss:  29 0.049956534\n","Validation acc:  29 0.9861123\n","epoch 29: loss = 0.055670857\n","Start of epoch 30\n","Training loss over epoch:  30 0.05341102\n","Training acc over epoch:  30 0.9853655\n","Validation loss:  30 0.048371002\n","Validation acc:  30 0.98665863\n","epoch 30: loss = 0.05341103\n","Start of epoch 31\n","Training loss over epoch:  31 0.05158948\n","Training acc over epoch:  31 0.98571825\n","Validation loss:  31 0.046579875\n","Validation acc:  31 0.9873689\n","epoch 31: loss = 0.051589474\n","Start of epoch 32\n","Training loss over epoch:  32 0.04978669\n","Training acc over epoch:  32 0.9864249\n","Validation loss:  32 0.043536548\n","Validation acc:  32 0.9881337\n","epoch 32: loss = 0.049786698\n","Start of epoch 33\n","Training loss over epoch:  33 0.047590107\n","Training acc over epoch:  33 0.9869392\n","Validation loss:  33 0.043812364\n","Validation acc:  33 0.9882102\n","epoch 33: loss = 0.04759012\n","Start of epoch 34\n","Training loss over epoch:  34 0.04609164\n","Training acc over epoch:  34 0.9873679\n","Validation loss:  34 0.042872045\n","Validation acc:  34 0.9885271\n","epoch 34: loss = 0.046091635\n","Start of epoch 35\n","Training loss over epoch:  35 0.04404208\n","Training acc over epoch:  35 0.9877931\n","Validation loss:  35 0.041332565\n","Validation acc:  35 0.98912805\n","epoch 35: loss = 0.044042084\n","Start of epoch 36\n","Training loss over epoch:  36 0.04276919\n","Training acc over epoch:  36 0.9880449\n","Validation loss:  36 0.041189488\n","Validation acc:  36 0.9889969\n","epoch 36: loss = 0.042769186\n","Start of epoch 37\n","Training loss over epoch:  37 0.04122157\n","Training acc over epoch:  37 0.988381\n","Validation loss:  37 0.04041427\n","Validation acc:  37 0.98937935\n","epoch 37: loss = 0.04122158\n","Start of epoch 38\n","Training loss over epoch:  38 0.040732484\n","Training acc over epoch:  38 0.98854727\n","Validation loss:  38 0.040241342\n","Validation acc:  38 0.98952144\n","epoch 38: loss = 0.04073249\n","Start of epoch 39\n","Training loss over epoch:  39 0.038983464\n","Training acc over epoch:  39 0.9889677\n","Validation loss:  39 0.040003233\n","Validation acc:  39 0.9895105\n","epoch 39: loss = 0.03898346\n","Start of epoch 40\n","Training loss over epoch:  40 0.037841838\n","Training acc over epoch:  40 0.9891708\n","Validation loss:  40 0.0399194\n","Validation acc:  40 0.9894559\n","epoch 40: loss = 0.03784183\n","Start of epoch 41\n","Training loss over epoch:  41 0.03660941\n","Training acc over epoch:  41 0.989425\n","Validation loss:  41 0.039486013\n","Validation acc:  41 0.9898055\n","epoch 41: loss = 0.036609408\n","Start of epoch 42\n","Training loss over epoch:  42 0.035237\n","Training acc over epoch:  42 0.9898359\n","Validation loss:  42 0.039154116\n","Validation acc:  42 0.9896307\n","epoch 42: loss = 0.035236996\n","Start of epoch 43\n","Training loss over epoch:  43 0.034079876\n","Training acc over epoch:  43 0.98987985\n","Validation loss:  43 0.039275564\n","Validation acc:  43 0.9896635\n","epoch 43: loss = 0.03407988\n","Start of epoch 44\n","Training loss over epoch:  44 0.032843996\n","Training acc over epoch:  44 0.99025637\n","Validation loss:  44 0.039396863\n","Validation acc:  44 0.9900022\n","epoch 44: loss = 0.032844003\n","Start of epoch 45\n","Training loss over epoch:  45 0.03198441\n","Training acc over epoch:  45 0.99043095\n","Validation loss:  45 0.039749913\n","Validation acc:  45 0.9898164\n","epoch 45: loss = 0.031984415\n","Start of epoch 46\n","Training loss over epoch:  46 0.03126358\n","Training acc over epoch:  46 0.9906198\n","Validation loss:  46 0.03951988\n","Validation acc:  46 0.98995847\n","epoch 46: loss = 0.03126358\n","Start of epoch 47\n","Training loss over epoch:  47 0.02970944\n","Training acc over epoch:  47 0.9909939\n","Validation loss:  47 0.039762676\n","Validation acc:  47 0.99038464\n","epoch 47: loss = 0.029709438\n","Start of epoch 48\n","Training loss over epoch:  48 0.029418102\n","Training acc over epoch:  48 0.9910996\n","Validation loss:  48 0.04007621\n","Validation acc:  48 0.9896635\n","epoch 48: loss = 0.029418098\n","Start of epoch 49\n","Training loss over epoch:  49 0.027985973\n","Training acc over epoch:  49 0.9913419\n","Validation loss:  49 0.040503763\n","Validation acc:  49 0.98916084\n","epoch 49: loss = 0.02798597\n","Start of epoch 50\n","Training loss over epoch:  50 0.027251167\n","Training acc over epoch:  50 0.99162453\n","Validation loss:  50 0.039834943\n","Validation acc:  50 0.98969626\n","epoch 50: loss = 0.02725117\n","Start of epoch 51\n","Training loss over epoch:  51 0.026603691\n","Training acc over epoch:  51 0.99166137\n","Validation loss:  51 0.0407215\n","Validation acc:  51 0.9896088\n","epoch 51: loss = 0.026603691\n","Start of epoch 52\n","Training loss over epoch:  52 0.025216501\n","Training acc over epoch:  52 0.99200696\n","Validation loss:  52 0.04088099\n","Validation acc:  52 0.9898929\n","epoch 52: loss = 0.0252165\n","Start of epoch 53\n","Training loss over epoch:  53 0.024743238\n","Training acc over epoch:  53 0.992121\n","Validation loss:  53 0.04105321\n","Validation acc:  53 0.9900459\n","epoch 53: loss = 0.024743233\n","Start of epoch 54\n","Training loss over epoch:  54 0.024004892\n","Training acc over epoch:  54 0.99239415\n","Validation loss:  54 0.041790407\n","Validation acc:  54 0.98998034\n","epoch 54: loss = 0.024004895\n","Start of epoch 55\n","Training loss over epoch:  55 0.023467345\n","Training acc over epoch:  55 0.99245\n","Validation loss:  55 0.04264771\n","Validation acc:  55 0.9894012\n","epoch 55: loss = 0.023467343\n","Start of epoch 56\n","Training loss over epoch:  56 0.022360763\n","Training acc over epoch:  56 0.9927659\n","Validation loss:  56 0.041419648\n","Validation acc:  56 0.9894231\n","epoch 56: loss = 0.022360768\n","Start of epoch 57\n","Training loss over epoch:  57 0.021608168\n","Training acc over epoch:  57 0.9929963\n","Validation loss:  57 0.0424408\n","Validation acc:  57 0.9900896\n","epoch 57: loss = 0.021608172\n","Start of epoch 58\n","Training loss over epoch:  58 0.021065194\n","Training acc over epoch:  58 0.99309015\n","Validation loss:  58 0.042324692\n","Validation acc:  58 0.9895979\n","epoch 58: loss = 0.021065196\n","Start of epoch 59\n","Training loss over epoch:  59 0.02020711\n","Training acc over epoch:  59 0.9934191\n","Validation loss:  59 0.045583494\n","Validation acc:  59 0.9900677\n","epoch 59: loss = 0.020207113\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:09<00:00, 91.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9607: \n","Sensitivity: 0.3987749212631229\n","Specificity: 0.8204495861928602\n","F1-score macro: 0.4393986247242958\n","F1-score micro: 0.9606681374093917\n","Training using fold 2\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 84112.0\n","valid_X_fake diff: 9352.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.3363605\n","Training acc over epoch:  0 0.89593506\n","Validation loss:  0 0.2517835\n","Validation acc:  0 0.95823866\n","epoch 0: loss = 1.3363606\n","Start of epoch 1\n","Training loss over epoch:  1 0.2365734\n","Training acc over epoch:  1 0.9580823\n","Validation loss:  1 0.22185548\n","Validation acc:  1 0.95823866\n","epoch 1: loss = 0.23657341\n","Start of epoch 2\n","Training loss over epoch:  2 0.22075075\n","Training acc over epoch:  2 0.95810014\n","Validation loss:  2 0.21487027\n","Validation acc:  2 0.95823866\n","epoch 2: loss = 0.22075076\n","Start of epoch 3\n","Training loss over epoch:  3 0.21591069\n","Training acc over epoch:  3 0.95809186\n","Validation loss:  3 0.21008898\n","Validation acc:  3 0.95823866\n","epoch 3: loss = 0.21591069\n","Start of epoch 4\n","Training loss over epoch:  4 0.21250291\n","Training acc over epoch:  4 0.9580776\n","Validation loss:  4 0.20789276\n","Validation acc:  4 0.95823866\n","epoch 4: loss = 0.2125029\n","Start of epoch 5\n","Training loss over epoch:  5 0.21055979\n","Training acc over epoch:  5 0.9580669\n","Validation loss:  5 0.2064859\n","Validation acc:  5 0.95823866\n","epoch 5: loss = 0.21055976\n","Start of epoch 6\n","Training loss over epoch:  6 0.20847169\n","Training acc over epoch:  6 0.95811915\n","Validation loss:  6 0.20481573\n","Validation acc:  6 0.95823866\n","epoch 6: loss = 0.20847176\n","Start of epoch 7\n","Training loss over epoch:  7 0.20521636\n","Training acc over epoch:  7 0.9580811\n","Validation loss:  7 0.19796126\n","Validation acc:  7 0.95823866\n","epoch 7: loss = 0.20521636\n","Start of epoch 8\n","Training loss over epoch:  8 0.20010515\n","Training acc over epoch:  8 0.95809066\n","Validation loss:  8 0.19466655\n","Validation acc:  8 0.95823866\n","epoch 8: loss = 0.20010515\n","Start of epoch 9\n","Training loss over epoch:  9 0.19778267\n","Training acc over epoch:  9 0.95809656\n","Validation loss:  9 0.19290921\n","Validation acc:  9 0.95823866\n","epoch 9: loss = 0.19778268\n","Start of epoch 10\n","Training loss over epoch:  10 0.19617623\n","Training acc over epoch:  10 0.9580788\n","Validation loss:  10 0.19139025\n","Validation acc:  10 0.95823866\n","epoch 10: loss = 0.19617625\n","Start of epoch 11\n","Training loss over epoch:  11 0.19475168\n","Training acc over epoch:  11 0.95807993\n","Validation loss:  11 0.18941191\n","Validation acc:  11 0.95823866\n","epoch 11: loss = 0.19475172\n","Start of epoch 12\n","Training loss over epoch:  12 0.19198781\n","Training acc over epoch:  12 0.9580669\n","Validation loss:  12 0.18669263\n","Validation acc:  12 0.95823866\n","epoch 12: loss = 0.19198787\n","Start of epoch 13\n","Training loss over epoch:  13 0.18861407\n","Training acc over epoch:  13 0.95804906\n","Validation loss:  13 0.18182784\n","Validation acc:  13 0.95823866\n","epoch 13: loss = 0.18861406\n","Start of epoch 14\n","Training loss over epoch:  14 0.18386598\n","Training acc over epoch:  14 0.95810133\n","Validation loss:  14 0.17610933\n","Validation acc:  14 0.95823866\n","epoch 14: loss = 0.18386601\n","Start of epoch 15\n","Training loss over epoch:  15 0.17905432\n","Training acc over epoch:  15 0.95805025\n","Validation loss:  15 0.16799854\n","Validation acc:  15 0.95823866\n","epoch 15: loss = 0.17905432\n","Start of epoch 16\n","Training loss over epoch:  16 0.16973388\n","Training acc over epoch:  16 0.9581322\n","Validation loss:  16 0.15243979\n","Validation acc:  16 0.95831513\n","epoch 16: loss = 0.16973391\n","Start of epoch 17\n","Training loss over epoch:  17 0.15517128\n","Training acc over epoch:  17 0.95865005\n","Validation loss:  17 0.13788663\n","Validation acc:  17 0.95981205\n","epoch 17: loss = 0.15517128\n","Start of epoch 18\n","Training loss over epoch:  18 0.1445131\n","Training acc over epoch:  18 0.9592213\n","Validation loss:  18 0.12772249\n","Validation acc:  18 0.96083915\n","epoch 18: loss = 0.14451312\n","Start of epoch 19\n","Training loss over epoch:  19 0.13427463\n","Training acc over epoch:  19 0.9602415\n","Validation loss:  19 0.11325463\n","Validation acc:  19 0.9637238\n","epoch 19: loss = 0.1342746\n","Start of epoch 20\n","Training loss over epoch:  20 0.12399968\n","Training acc over epoch:  20 0.9620646\n","Validation loss:  20 0.10202105\n","Validation acc:  20 0.96712196\n","epoch 20: loss = 0.12399968\n","Start of epoch 21\n","Training loss over epoch:  21 0.11701381\n","Training acc over epoch:  21 0.9638485\n","Validation loss:  21 0.09474345\n","Validation acc:  21 0.9693182\n","epoch 21: loss = 0.11701385\n","Start of epoch 22\n","Training loss over epoch:  22 0.10947622\n","Training acc over epoch:  22 0.965668\n","Validation loss:  22 0.08556162\n","Validation acc:  22 0.971722\n","epoch 22: loss = 0.10947623\n","Start of epoch 23\n","Training loss over epoch:  23 0.10269666\n","Training acc over epoch:  23 0.9675433\n","Validation loss:  23 0.07739785\n","Validation acc:  23 0.9759178\n","epoch 23: loss = 0.102696665\n","Start of epoch 24\n","Training loss over epoch:  24 0.09605545\n","Training acc over epoch:  24 0.969541\n","Validation loss:  24 0.07144758\n","Validation acc:  24 0.9784856\n","epoch 24: loss = 0.09605545\n","Start of epoch 25\n","Training loss over epoch:  25 0.09056009\n","Training acc over epoch:  25 0.9712631\n","Validation loss:  25 0.066235475\n","Validation acc:  25 0.97963285\n","epoch 25: loss = 0.09056009\n","Start of epoch 26\n","Training loss over epoch:  26 0.08651885\n","Training acc over epoch:  26 0.97258973\n","Validation loss:  26 0.06490766\n","Validation acc:  26 0.98109704\n","epoch 26: loss = 0.08651886\n","Start of epoch 27\n","Training loss over epoch:  27 0.08237333\n","Training acc over epoch:  27 0.9738249\n","Validation loss:  27 0.059769526\n","Validation acc:  27 0.9823973\n","epoch 27: loss = 0.08237332\n","Start of epoch 28\n","Training loss over epoch:  28 0.079227425\n","Training acc over epoch:  28 0.9750779\n","Validation loss:  28 0.05639242\n","Validation acc:  28 0.9839379\n","epoch 28: loss = 0.079227425\n","Start of epoch 29\n","Training loss over epoch:  29 0.076293856\n","Training acc over epoch:  29 0.9759758\n","Validation loss:  29 0.05380028\n","Validation acc:  29 0.9848339\n","epoch 29: loss = 0.076293856\n","Start of epoch 30\n","Training loss over epoch:  30 0.07357635\n","Training acc over epoch:  30 0.9771183\n","Validation loss:  30 0.05080941\n","Validation acc:  30 0.98579544\n","epoch 30: loss = 0.073576346\n","Start of epoch 31\n","Training loss over epoch:  31 0.071483985\n","Training acc over epoch:  31 0.9778155\n","Validation loss:  31 0.05005925\n","Validation acc:  31 0.98600304\n","epoch 31: loss = 0.071483985\n","Start of epoch 32\n","Training loss over epoch:  32 0.068990104\n","Training acc over epoch:  32 0.97858393\n","Validation loss:  32 0.04676898\n","Validation acc:  32 0.9860577\n","epoch 32: loss = 0.068990104\n","Start of epoch 33\n","Training loss over epoch:  33 0.067343324\n","Training acc over epoch:  33 0.9792799\n","Validation loss:  33 0.045076564\n","Validation acc:  33 0.986757\n","epoch 33: loss = 0.06734333\n","Start of epoch 34\n","Training loss over epoch:  34 0.06570846\n","Training acc over epoch:  34 0.97959226\n","Validation loss:  34 0.044827234\n","Validation acc:  34 0.98744535\n","epoch 34: loss = 0.06570845\n","Start of epoch 35\n","Training loss over epoch:  35 0.06415913\n","Training acc over epoch:  35 0.980135\n","Validation loss:  35 0.043367304\n","Validation acc:  35 0.9877185\n","epoch 35: loss = 0.06415915\n","Start of epoch 36\n","Training loss over epoch:  36 0.063444\n","Training acc over epoch:  36 0.98046875\n","Validation loss:  36 0.043180026\n","Validation acc:  36 0.9882321\n","epoch 36: loss = 0.063443996\n","Start of epoch 37\n","Training loss over epoch:  37 0.061864294\n","Training acc over epoch:  37 0.9807645\n","Validation loss:  37 0.041139964\n","Validation acc:  37 0.98850524\n","epoch 37: loss = 0.061864287\n","Start of epoch 38\n","Training loss over epoch:  38 0.060484838\n","Training acc over epoch:  38 0.9811612\n","Validation loss:  38 0.04161552\n","Validation acc:  38 0.98871285\n","epoch 38: loss = 0.060484834\n","Start of epoch 39\n","Training loss over epoch:  39 0.059198298\n","Training acc over epoch:  39 0.9816921\n","Validation loss:  39 0.039525785\n","Validation acc:  39 0.98912805\n","epoch 39: loss = 0.0591983\n","Start of epoch 40\n","Training loss over epoch:  40 0.05762466\n","Training acc over epoch:  40 0.98225856\n","Validation loss:  40 0.038136378\n","Validation acc:  40 0.98917174\n","epoch 40: loss = 0.05762467\n","Start of epoch 41\n","Training loss over epoch:  41 0.057386298\n","Training acc over epoch:  41 0.9824082\n","Validation loss:  41 0.039001577\n","Validation acc:  41 0.98961973\n","epoch 41: loss = 0.05738631\n","Start of epoch 42\n","Training loss over epoch:  42 0.055797707\n","Training acc over epoch:  42 0.9827075\n","Validation loss:  42 0.038834766\n","Validation acc:  42 0.98973995\n","epoch 42: loss = 0.05579771\n","Start of epoch 43\n","Training loss over epoch:  43 0.05478908\n","Training acc over epoch:  43 0.98317784\n","Validation loss:  43 0.03671452\n","Validation acc:  43 0.9902207\n","epoch 43: loss = 0.054789077\n","Start of epoch 44\n","Training loss over epoch:  44 0.05420219\n","Training acc over epoch:  44 0.9833631\n","Validation loss:  44 0.035853896\n","Validation acc:  44 0.99017704\n","epoch 44: loss = 0.054202188\n","Start of epoch 45\n","Training loss over epoch:  45 0.053314116\n","Training acc over epoch:  45 0.98360777\n","Validation loss:  45 0.036157895\n","Validation acc:  45 0.9906796\n","epoch 45: loss = 0.053314116\n","Start of epoch 46\n","Training loss over epoch:  46 0.052634213\n","Training acc over epoch:  46 0.98394984\n","Validation loss:  46 0.035984248\n","Validation acc:  46 0.9904283\n","epoch 46: loss = 0.052634217\n","Start of epoch 47\n","Training loss over epoch:  47 0.04997358\n","Training acc over epoch:  47 0.9848596\n","Validation loss:  47 0.032600958\n","Validation acc:  47 0.9914991\n","epoch 47: loss = 0.04997358\n","Start of epoch 48\n","Training loss over epoch:  48 0.046520486\n","Training acc over epoch:  48 0.98584175\n","Validation loss:  48 0.03063418\n","Validation acc:  48 0.99166304\n","epoch 48: loss = 0.046520483\n","Start of epoch 49\n","Training loss over epoch:  49 0.044871863\n","Training acc over epoch:  49 0.986476\n","Validation loss:  49 0.032899745\n","Validation acc:  49 0.99250436\n","epoch 49: loss = 0.044871863\n","Start of epoch 50\n","Training loss over epoch:  50 0.043425627\n","Training acc over epoch:  50 0.98692966\n","Validation loss:  50 0.028954988\n","Validation acc:  50 0.993007\n","epoch 50: loss = 0.04342562\n","Start of epoch 51\n","Training loss over epoch:  51 0.04166453\n","Training acc over epoch:  51 0.98741543\n","Validation loss:  51 0.027306493\n","Validation acc:  51 0.99289775\n","epoch 51: loss = 0.04166453\n","Start of epoch 52\n","Training loss over epoch:  52 0.040858004\n","Training acc over epoch:  52 0.9878347\n","Validation loss:  52 0.027561529\n","Validation acc:  52 0.9932911\n","epoch 52: loss = 0.040858\n","Start of epoch 53\n","Training loss over epoch:  53 0.03864651\n","Training acc over epoch:  53 0.9884428\n","Validation loss:  53 0.026366144\n","Validation acc:  53 0.9934768\n","epoch 53: loss = 0.038646515\n","Start of epoch 54\n","Training loss over epoch:  54 0.0369353\n","Training acc over epoch:  54 0.98895943\n","Validation loss:  54 0.024606915\n","Validation acc:  54 0.99377185\n","epoch 54: loss = 0.036935303\n","Start of epoch 55\n","Training loss over epoch:  55 0.035008896\n","Training acc over epoch:  55 0.9896518\n","Validation loss:  55 0.023292579\n","Validation acc:  55 0.9942854\n","epoch 55: loss = 0.035008896\n","Start of epoch 56\n","Training loss over epoch:  56 0.033311237\n","Training acc over epoch:  56 0.9902421\n","Validation loss:  56 0.022875715\n","Validation acc:  56 0.99413246\n","epoch 56: loss = 0.033311237\n","Start of epoch 57\n","Training loss over epoch:  57 0.032463044\n","Training acc over epoch:  57 0.99051523\n","Validation loss:  57 0.022582188\n","Validation acc:  57 0.99435097\n","epoch 57: loss = 0.03246305\n","Start of epoch 58\n","Training loss over epoch:  58 0.030857896\n","Training acc over epoch:  58 0.99089414\n","Validation loss:  58 0.021991078\n","Validation acc:  58 0.9947006\n","epoch 58: loss = 0.030857896\n","Start of epoch 59\n","Training loss over epoch:  59 0.029319586\n","Training acc over epoch:  59 0.99135375\n","Validation loss:  59 0.02097665\n","Validation acc:  59 0.99464595\n","epoch 59: loss = 0.02931959\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 91.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9603: \n","Sensitivity: 0.359106831894637\n","Specificity: 0.8029689400655825\n","F1-score macro: 0.4101269945963467\n","F1-score micro: 0.9603460089071599\n","Missing rate 0.1\n","=====================================================\n","train_X_fake diff: 171228.0\n","valid_X_fake diff: 19038.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.757268\n","Training acc over epoch:  0 0.93507713\n","Validation loss:  0 0.23714647\n","Validation acc:  0 0.95823866\n","epoch 0: loss = 0.7572678\n","Start of epoch 1\n","Training loss over epoch:  1 0.22793907\n","Training acc over epoch:  1 0.95809186\n","Validation loss:  1 0.21692306\n","Validation acc:  1 0.95823866\n","epoch 1: loss = 0.22793907\n","Start of epoch 2\n","Training loss over epoch:  2 0.21809664\n","Training acc over epoch:  2 0.9580538\n","Validation loss:  2 0.21453907\n","Validation acc:  2 0.95823866\n","epoch 2: loss = 0.21809667\n","Start of epoch 3\n","Training loss over epoch:  3 0.21260317\n","Training acc over epoch:  3 0.95811087\n","Validation loss:  3 0.20433909\n","Validation acc:  3 0.95823866\n","epoch 3: loss = 0.21260315\n","Start of epoch 4\n","Training loss over epoch:  4 0.20215163\n","Training acc over epoch:  4 0.95809186\n","Validation loss:  4 0.19383942\n","Validation acc:  4 0.95823866\n","epoch 4: loss = 0.20215158\n","Start of epoch 5\n","Training loss over epoch:  5 0.19451872\n","Training acc over epoch:  5 0.9580883\n","Validation loss:  5 0.18881784\n","Validation acc:  5 0.95823866\n","epoch 5: loss = 0.19451872\n","Start of epoch 6\n","Training loss over epoch:  6 0.1896193\n","Training acc over epoch:  6 0.95807046\n","Validation loss:  6 0.182919\n","Validation acc:  6 0.95823866\n","epoch 6: loss = 0.18961933\n","Start of epoch 7\n","Training loss over epoch:  7 0.18152493\n","Training acc over epoch:  7 0.9580669\n","Validation loss:  7 0.1713164\n","Validation acc:  7 0.95823866\n","epoch 7: loss = 0.18152498\n","Start of epoch 8\n","Training loss over epoch:  8 0.17287518\n","Training acc over epoch:  8 0.95809656\n","Validation loss:  8 0.16432093\n","Validation acc:  8 0.95823866\n","epoch 8: loss = 0.17287518\n","Start of epoch 9\n","Training loss over epoch:  9 0.16551328\n","Training acc over epoch:  9 0.9583496\n","Validation loss:  9 0.15414082\n","Validation acc:  9 0.9591237\n","epoch 9: loss = 0.16551329\n","Start of epoch 10\n","Training loss over epoch:  10 0.15567707\n","Training acc over epoch:  10 0.9592177\n","Validation loss:  10 0.14572236\n","Validation acc:  10 0.96042395\n","epoch 10: loss = 0.15567705\n","Start of epoch 11\n","Training loss over epoch:  11 0.14363818\n","Training acc over epoch:  11 0.96034366\n","Validation loss:  11 0.1275035\n","Validation acc:  11 0.96248907\n","epoch 11: loss = 0.14363815\n","Start of epoch 12\n","Training loss over epoch:  12 0.13004756\n","Training acc over epoch:  12 0.9620064\n","Validation loss:  12 0.10973953\n","Validation acc:  12 0.9663789\n","epoch 12: loss = 0.13004756\n","Start of epoch 13\n","Training loss over epoch:  13 0.115331575\n","Training acc over epoch:  13 0.96474516\n","Validation loss:  13 0.096660584\n","Validation acc:  13 0.9725524\n","epoch 13: loss = 0.115331575\n","Start of epoch 14\n","Training loss over epoch:  14 0.10201027\n","Training acc over epoch:  14 0.96849227\n","Validation loss:  14 0.08048439\n","Validation acc:  14 0.9773601\n","epoch 14: loss = 0.10201028\n","Start of epoch 15\n","Training loss over epoch:  15 0.08971991\n","Training acc over epoch:  15 0.97219306\n","Validation loss:  15 0.068830095\n","Validation acc:  15 0.9805398\n","epoch 15: loss = 0.0897199\n","Start of epoch 16\n","Training loss over epoch:  16 0.08160592\n","Training acc over epoch:  16 0.97480357\n","Validation loss:  16 0.06353037\n","Validation acc:  16 0.98258305\n","epoch 16: loss = 0.08160591\n","Start of epoch 17\n","Training loss over epoch:  17 0.07470893\n","Training acc over epoch:  17 0.9771694\n","Validation loss:  17 0.05709281\n","Validation acc:  17 0.9845935\n","epoch 17: loss = 0.07470893\n","Start of epoch 18\n","Training loss over epoch:  18 0.068458974\n","Training acc over epoch:  18 0.97955424\n","Validation loss:  18 0.05112592\n","Validation acc:  18 0.9859375\n","epoch 18: loss = 0.06845897\n","Start of epoch 19\n","Training loss over epoch:  19 0.06260588\n","Training acc over epoch:  19 0.98168373\n","Validation loss:  19 0.04658021\n","Validation acc:  19 0.9875\n","epoch 19: loss = 0.062605865\n","Start of epoch 20\n","Training loss over epoch:  20 0.058332253\n","Training acc over epoch:  20 0.98327994\n","Validation loss:  20 0.04346283\n","Validation acc:  20 0.98854893\n","epoch 20: loss = 0.058332235\n","Start of epoch 21\n","Training loss over epoch:  21 0.053795554\n","Training acc over epoch:  21 0.98487973\n","Validation loss:  21 0.0398446\n","Validation acc:  21 0.9898492\n","epoch 21: loss = 0.05379555\n","Start of epoch 22\n","Training loss over epoch:  22 0.04995486\n","Training acc over epoch:  22 0.98595816\n","Validation loss:  22 0.03875353\n","Validation acc:  22 0.99055946\n","epoch 22: loss = 0.04995486\n","Start of epoch 23\n","Training loss over epoch:  23 0.047055483\n","Training acc over epoch:  23 0.98702466\n","Validation loss:  23 0.036657132\n","Validation acc:  23 0.990625\n","epoch 23: loss = 0.04705548\n","Start of epoch 24\n","Training loss over epoch:  24 0.044777874\n","Training acc over epoch:  24 0.9877409\n","Validation loss:  24 0.034252375\n","Validation acc:  24 0.99137896\n","epoch 24: loss = 0.04477788\n","Start of epoch 25\n","Training loss over epoch:  25 0.04227027\n","Training acc over epoch:  25 0.98845106\n","Validation loss:  25 0.032705747\n","Validation acc:  25 0.99189246\n","epoch 25: loss = 0.042270273\n","Start of epoch 26\n","Training loss over epoch:  26 0.04098091\n","Training acc over epoch:  26 0.98886913\n","Validation loss:  26 0.031237083\n","Validation acc:  26 0.99228585\n","epoch 26: loss = 0.040980913\n","Start of epoch 27\n","Training loss over epoch:  27 0.038798235\n","Training acc over epoch:  27 0.9894974\n","Validation loss:  27 0.030277655\n","Validation acc:  27 0.992417\n","epoch 27: loss = 0.038798235\n","Start of epoch 28\n","Training loss over epoch:  28 0.03779666\n","Training acc over epoch:  28 0.98985493\n","Validation loss:  28 0.029238163\n","Validation acc:  28 0.99273384\n","epoch 28: loss = 0.037796658\n","Start of epoch 29\n","Training loss over epoch:  29 0.036407202\n","Training acc over epoch:  29 0.9902338\n","Validation loss:  29 0.028844519\n","Validation acc:  29 0.9927557\n","epoch 29: loss = 0.036407202\n","Start of epoch 30\n","Training loss over epoch:  30 0.035018705\n","Training acc over epoch:  30 0.99053305\n","Validation loss:  30 0.028039835\n","Validation acc:  30 0.99314904\n","epoch 30: loss = 0.035018712\n","Start of epoch 31\n","Training loss over epoch:  31 0.034418594\n","Training acc over epoch:  31 0.9908027\n","Validation loss:  31 0.028261635\n","Validation acc:  31 0.99286497\n","epoch 31: loss = 0.03441859\n","Start of epoch 32\n","Training loss over epoch:  32 0.032686144\n","Training acc over epoch:  32 0.9911673\n","Validation loss:  32 0.02703581\n","Validation acc:  32 0.9930398\n","epoch 32: loss = 0.032686144\n","Start of epoch 33\n","Training loss over epoch:  33 0.031726155\n","Training acc over epoch:  33 0.9913918\n","Validation loss:  33 0.027466344\n","Validation acc:  33 0.9930835\n","epoch 33: loss = 0.031726155\n","Start of epoch 34\n","Training loss over epoch:  34 0.030793816\n","Training acc over epoch:  34 0.99169105\n","Validation loss:  34 0.02566653\n","Validation acc:  34 0.9934987\n","epoch 34: loss = 0.03079382\n","Start of epoch 35\n","Training loss over epoch:  35 0.030122284\n","Training acc over epoch:  35 0.99179673\n","Validation loss:  35 0.026555238\n","Validation acc:  35 0.9935861\n","epoch 35: loss = 0.03012228\n","Start of epoch 36\n","Training loss over epoch:  36 0.029697774\n","Training acc over epoch:  36 0.9918205\n","Validation loss:  36 0.025037082\n","Validation acc:  36 0.99376094\n","epoch 36: loss = 0.029697776\n","Start of epoch 37\n","Training loss over epoch:  37 0.028158935\n","Training acc over epoch:  37 0.9922873\n","Validation loss:  37 0.025199866\n","Validation acc:  37 0.99380463\n","epoch 37: loss = 0.028158935\n","Start of epoch 38\n","Training loss over epoch:  38 0.027750436\n","Training acc over epoch:  38 0.9922267\n","Validation loss:  38 0.024522059\n","Validation acc:  38 0.9938593\n","epoch 38: loss = 0.027750434\n","Start of epoch 39\n","Training loss over epoch:  39 0.026908882\n","Training acc over epoch:  39 0.99254024\n","Validation loss:  39 0.024569266\n","Validation acc:  39 0.9938483\n","epoch 39: loss = 0.026908876\n","Start of epoch 40\n","Training loss over epoch:  40 0.0261914\n","Training acc over epoch:  40 0.99265665\n","Validation loss:  40 0.024812099\n","Validation acc:  40 0.9938811\n","epoch 40: loss = 0.0261914\n","Start of epoch 41\n","Training loss over epoch:  41 0.026232162\n","Training acc over epoch:  41 0.9925949\n","Validation loss:  41 0.024856104\n","Validation acc:  41 0.99396855\n","epoch 41: loss = 0.026232159\n","Start of epoch 42\n","Training loss over epoch:  42 0.024657466\n","Training acc over epoch:  42 0.99295473\n","Validation loss:  42 0.024367675\n","Validation acc:  42 0.9941543\n","epoch 42: loss = 0.02465747\n","Start of epoch 43\n","Training loss over epoch:  43 0.024115693\n","Training acc over epoch:  43 0.99317443\n","Validation loss:  43 0.02403229\n","Validation acc:  43 0.99420893\n","epoch 43: loss = 0.024115697\n","Start of epoch 44\n","Training loss over epoch:  44 0.023459753\n","Training acc over epoch:  44 0.99326235\n","Validation loss:  44 0.023984224\n","Validation acc:  44 0.99418706\n","epoch 44: loss = 0.023459753\n","Start of epoch 45\n","Training loss over epoch:  45 0.023286162\n","Training acc over epoch:  45 0.9932564\n","Validation loss:  45 0.023730867\n","Validation acc:  45 0.9940778\n","epoch 45: loss = 0.023286162\n","Start of epoch 46\n","Training loss over epoch:  46 0.022400176\n","Training acc over epoch:  46 0.99346423\n","Validation loss:  46 0.023624051\n","Validation acc:  46 0.99435097\n","epoch 46: loss = 0.022400174\n","Start of epoch 47\n","Training loss over epoch:  47 0.02209161\n","Training acc over epoch:  47 0.9935438\n","Validation loss:  47 0.023613678\n","Validation acc:  47 0.9943728\n","epoch 47: loss = 0.022091608\n","Start of epoch 48\n","Training loss over epoch:  48 0.021438051\n","Training acc over epoch:  48 0.99375284\n","Validation loss:  48 0.024468392\n","Validation acc:  48 0.99442744\n","epoch 48: loss = 0.021438047\n","Start of epoch 49\n","Training loss over epoch:  49 0.021159906\n","Training acc over epoch:  49 0.99364\n","Validation loss:  49 0.023556383\n","Validation acc:  49 0.9942854\n","epoch 49: loss = 0.021159904\n","Start of epoch 50\n","Training loss over epoch:  50 0.020332992\n","Training acc over epoch:  50 0.99389654\n","Validation loss:  50 0.024423987\n","Validation acc:  50 0.99458045\n","epoch 50: loss = 0.020332994\n","Start of epoch 51\n","Training loss over epoch:  51 0.020235155\n","Training acc over epoch:  51 0.9939963\n","Validation loss:  51 0.023933474\n","Validation acc:  51 0.9945039\n","epoch 51: loss = 0.020235153\n","Start of epoch 52\n","Training loss over epoch:  52 0.0191506\n","Training acc over epoch:  52 0.9943158\n","Validation loss:  52 0.023755766\n","Validation acc:  52 0.99397945\n","epoch 52: loss = 0.019150596\n","Start of epoch 53\n","Training loss over epoch:  53 0.018919602\n","Training acc over epoch:  53 0.9942944\n","Validation loss:  53 0.023750367\n","Validation acc:  53 0.9945695\n","epoch 53: loss = 0.0189196\n","Start of epoch 54\n","Training loss over epoch:  54 0.018712994\n","Training acc over epoch:  54 0.994355\n","Validation loss:  54 0.024430767\n","Validation acc:  54 0.9944821\n","epoch 54: loss = 0.018712992\n","Start of epoch 55\n","Training loss over epoch:  55 0.018209727\n","Training acc over epoch:  55 0.9943906\n","Validation loss:  55 0.023942024\n","Validation acc:  55 0.9942963\n","epoch 55: loss = 0.018209727\n","Start of epoch 56\n","Training loss over epoch:  56 0.017834133\n","Training acc over epoch:  56 0.9945284\n","Validation loss:  56 0.024037391\n","Validation acc:  56 0.99463505\n","epoch 56: loss = 0.017834133\n","Start of epoch 57\n","Training loss over epoch:  57 0.01736071\n","Training acc over epoch:  57 0.9946032\n","Validation loss:  57 0.02510934\n","Validation acc:  57 0.99467874\n","epoch 57: loss = 0.017360713\n","Start of epoch 58\n","Training loss over epoch:  58 0.01731263\n","Training acc over epoch:  58 0.994602\n","Validation loss:  58 0.024751706\n","Validation acc:  58 0.99463505\n","epoch 58: loss = 0.017312633\n","Start of epoch 59\n","Training loss over epoch:  59 0.01671263\n","Training acc over epoch:  59 0.9948966\n","Validation loss:  59 0.02450304\n","Validation acc:  59 0.9945149\n","epoch 59: loss = 0.01671263\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 90.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9604: \n","Sensitivity: 0.3868174270519518\n","Specificity: 0.8201993801449821\n","F1-score macro: 0.4380486629533765\n","F1-score micro: 0.960431654676259\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 342456.0\n","valid_X_fake diff: 38076.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.1137608\n","Training acc over epoch:  0 0.8058977\n","Validation loss:  0 0.23633438\n","Validation acc:  0 0.95823866\n","epoch 0: loss = 1.113761\n","Start of epoch 1\n","Training loss over epoch:  1 0.23693964\n","Training acc over epoch:  1 0.95809776\n","Validation loss:  1 0.2243462\n","Validation acc:  1 0.95823866\n","epoch 1: loss = 0.23693964\n","Start of epoch 2\n","Training loss over epoch:  2 0.22145322\n","Training acc over epoch:  2 0.95811087\n","Validation loss:  2 0.2172662\n","Validation acc:  2 0.95823866\n","epoch 2: loss = 0.22145319\n","Start of epoch 3\n","Training loss over epoch:  3 0.21871644\n","Training acc over epoch:  3 0.9581251\n","Validation loss:  3 0.21705967\n","Validation acc:  3 0.95823866\n","epoch 3: loss = 0.21871647\n","Start of epoch 4\n","Training loss over epoch:  4 0.21839972\n","Training acc over epoch:  4 0.9580847\n","Validation loss:  4 0.21790697\n","Validation acc:  4 0.95823866\n","epoch 4: loss = 0.21839975\n","Start of epoch 5\n","Training loss over epoch:  5 0.21818644\n","Training acc over epoch:  5 0.9580764\n","Validation loss:  5 0.21649638\n","Validation acc:  5 0.95823866\n","epoch 5: loss = 0.21818638\n","Start of epoch 6\n","Training loss over epoch:  6 0.21761166\n","Training acc over epoch:  6 0.95811087\n","Validation loss:  6 0.21605684\n","Validation acc:  6 0.95823866\n","epoch 6: loss = 0.21761164\n","Start of epoch 7\n","Training loss over epoch:  7 0.21752685\n","Training acc over epoch:  7 0.95804197\n","Validation loss:  7 0.21584955\n","Validation acc:  7 0.95823866\n","epoch 7: loss = 0.21752685\n","Start of epoch 8\n","Training loss over epoch:  8 0.21711597\n","Training acc over epoch:  8 0.9580408\n","Validation loss:  8 0.2153072\n","Validation acc:  8 0.95823866\n","epoch 8: loss = 0.21711598\n","Start of epoch 9\n","Training loss over epoch:  9 0.21613154\n","Training acc over epoch:  9 0.9581346\n","Validation loss:  9 0.21460578\n","Validation acc:  9 0.95823866\n","epoch 9: loss = 0.2161315\n","Start of epoch 10\n","Training loss over epoch:  10 0.2155783\n","Training acc over epoch:  10 0.9581334\n","Validation loss:  10 0.21403266\n","Validation acc:  10 0.95823866\n","epoch 10: loss = 0.2155783\n","Start of epoch 11\n","Training loss over epoch:  11 0.21524239\n","Training acc over epoch:  11 0.9580835\n","Validation loss:  11 0.21365525\n","Validation acc:  11 0.95823866\n","epoch 11: loss = 0.21524239\n","Start of epoch 12\n","Training loss over epoch:  12 0.21474081\n","Training acc over epoch:  12 0.95812035\n","Validation loss:  12 0.2137651\n","Validation acc:  12 0.95823866\n","epoch 12: loss = 0.2147408\n","Start of epoch 13\n","Training loss over epoch:  13 0.2144674\n","Training acc over epoch:  13 0.958112\n","Validation loss:  13 0.21270145\n","Validation acc:  13 0.95823866\n","epoch 13: loss = 0.21446742\n","Start of epoch 14\n","Training loss over epoch:  14 0.21406625\n","Training acc over epoch:  14 0.9580883\n","Validation loss:  14 0.2125073\n","Validation acc:  14 0.95823866\n","epoch 14: loss = 0.21406625\n","Start of epoch 15\n","Training loss over epoch:  15 0.21385464\n","Training acc over epoch:  15 0.9580586\n","Validation loss:  15 0.21208744\n","Validation acc:  15 0.95823866\n","epoch 15: loss = 0.21385463\n","Start of epoch 16\n","Training loss over epoch:  16 0.21338458\n","Training acc over epoch:  16 0.95814174\n","Validation loss:  16 0.21203074\n","Validation acc:  16 0.95823866\n","epoch 16: loss = 0.21338458\n","Start of epoch 17\n","Training loss over epoch:  17 0.21319376\n","Training acc over epoch:  17 0.95806926\n","Validation loss:  17 0.21241114\n","Validation acc:  17 0.95823866\n","epoch 17: loss = 0.2131937\n","Start of epoch 18\n","Training loss over epoch:  18 0.21267714\n","Training acc over epoch:  18 0.95815\n","Validation loss:  18 0.21139304\n","Validation acc:  18 0.95823866\n","epoch 18: loss = 0.21267717\n","Start of epoch 19\n","Training loss over epoch:  19 0.21240829\n","Training acc over epoch:  19 0.95814884\n","Validation loss:  19 0.21172139\n","Validation acc:  19 0.95823866\n","epoch 19: loss = 0.21240829\n","Start of epoch 20\n","Training loss over epoch:  20 0.21205668\n","Training acc over epoch:  20 0.9581631\n","Validation loss:  20 0.21076986\n","Validation acc:  20 0.95823866\n","epoch 20: loss = 0.2120567\n","Start of epoch 21\n","Training loss over epoch:  21 0.21199287\n","Training acc over epoch:  21 0.958131\n","Validation loss:  21 0.20974812\n","Validation acc:  21 0.95823866\n","epoch 21: loss = 0.21199284\n","Start of epoch 22\n","Training loss over epoch:  22 0.21133013\n","Training acc over epoch:  22 0.958226\n","Validation loss:  22 0.20938486\n","Validation acc:  22 0.95823866\n","epoch 22: loss = 0.21133016\n","Start of epoch 23\n","Training loss over epoch:  23 0.21121472\n","Training acc over epoch:  23 0.9582403\n","Validation loss:  23 0.21013412\n","Validation acc:  23 0.95823866\n","epoch 23: loss = 0.21121474\n","Start of epoch 24\n","Training loss over epoch:  24 0.21094519\n","Training acc over epoch:  24 0.95822483\n","Validation loss:  24 0.20976955\n","Validation acc:  24 0.95823866\n","epoch 24: loss = 0.21094514\n","Start of epoch 25\n","Training loss over epoch:  25 0.21083085\n","Training acc over epoch:  25 0.95824504\n","Validation loss:  25 0.2084399\n","Validation acc:  25 0.95823866\n","epoch 25: loss = 0.2108308\n","Start of epoch 26\n","Training loss over epoch:  26 0.21035886\n","Training acc over epoch:  26 0.9582771\n","Validation loss:  26 0.208338\n","Validation acc:  26 0.95823866\n","epoch 26: loss = 0.21035887\n","Start of epoch 27\n","Training loss over epoch:  27 0.20997325\n","Training acc over epoch:  27 0.9583282\n","Validation loss:  27 0.20769128\n","Validation acc:  27 0.95823866\n","epoch 27: loss = 0.20997325\n","Start of epoch 28\n","Training loss over epoch:  28 0.2098347\n","Training acc over epoch:  28 0.9583745\n","Validation loss:  28 0.20886794\n","Validation acc:  28 0.95823866\n","epoch 28: loss = 0.20983472\n","Start of epoch 29\n","Training loss over epoch:  29 0.20940438\n","Training acc over epoch:  29 0.9584481\n","Validation loss:  29 0.2072697\n","Validation acc:  29 0.95823866\n","epoch 29: loss = 0.20940441\n","Start of epoch 30\n","Training loss over epoch:  30 0.20932652\n","Training acc over epoch:  30 0.95835197\n","Validation loss:  30 0.2073952\n","Validation acc:  30 0.95823866\n","epoch 30: loss = 0.20932643\n","Start of epoch 31\n","Training loss over epoch:  31 0.20908368\n","Training acc over epoch:  31 0.9583484\n","Validation loss:  31 0.20687969\n","Validation acc:  31 0.95823866\n","epoch 31: loss = 0.20908368\n","Start of epoch 32\n","Training loss over epoch:  32 0.20895533\n","Training acc over epoch:  32 0.95838517\n","Validation loss:  32 0.20701289\n","Validation acc:  32 0.95823866\n","epoch 32: loss = 0.2089553\n","Start of epoch 33\n","Training loss over epoch:  33 0.20903791\n","Training acc over epoch:  33 0.95833296\n","Validation loss:  33 0.20659688\n","Validation acc:  33 0.95823866\n","epoch 33: loss = 0.2090379\n","Start of epoch 34\n","Training loss over epoch:  34 0.20826067\n","Training acc over epoch:  34 0.9584541\n","Validation loss:  34 0.20786639\n","Validation acc:  34 0.95892704\n","epoch 34: loss = 0.20826069\n","Start of epoch 35\n","Training loss over epoch:  35 0.20852938\n","Training acc over epoch:  35 0.95843035\n","Validation loss:  35 0.20607084\n","Validation acc:  35 0.95823866\n","epoch 35: loss = 0.20852938\n","Start of epoch 36\n","Training loss over epoch:  36 0.20807533\n","Training acc over epoch:  36 0.9584268\n","Validation loss:  36 0.20601869\n","Validation acc:  36 0.95892704\n","epoch 36: loss = 0.20807537\n","Start of epoch 37\n","Training loss over epoch:  37 0.20798904\n","Training acc over epoch:  37 0.95841724\n","Validation loss:  37 0.20559773\n","Validation acc:  37 0.95823866\n","epoch 37: loss = 0.20798902\n","Start of epoch 38\n","Training loss over epoch:  38 0.20805438\n","Training acc over epoch:  38 0.9583983\n","Validation loss:  38 0.20537767\n","Validation acc:  38 0.95823866\n","epoch 38: loss = 0.20805438\n","Start of epoch 39\n","Training loss over epoch:  39 0.20795463\n","Training acc over epoch:  39 0.9584612\n","Validation loss:  39 0.20513362\n","Validation acc:  39 0.95823866\n","epoch 39: loss = 0.20795463\n","Start of epoch 40\n","Training loss over epoch:  40 0.20701362\n","Training acc over epoch:  40 0.95845646\n","Validation loss:  40 0.20465676\n","Validation acc:  40 0.95892704\n","epoch 40: loss = 0.20701362\n","Start of epoch 41\n","Training loss over epoch:  41 0.20678496\n","Training acc over epoch:  41 0.9584707\n","Validation loss:  41 0.20461413\n","Validation acc:  41 0.95892704\n","epoch 41: loss = 0.206785\n","Start of epoch 42\n","Training loss over epoch:  42 0.20671774\n","Training acc over epoch:  42 0.9584006\n","Validation loss:  42 0.20413558\n","Validation acc:  42 0.95823866\n","epoch 42: loss = 0.2067177\n","Start of epoch 43\n","Training loss over epoch:  43 0.20655467\n","Training acc over epoch:  43 0.9584006\n","Validation loss:  43 0.20325501\n","Validation acc:  43 0.95892704\n","epoch 43: loss = 0.2065547\n","Start of epoch 44\n","Training loss over epoch:  44 0.20548344\n","Training acc over epoch:  44 0.9584743\n","Validation loss:  44 0.20323655\n","Validation acc:  44 0.95892704\n","epoch 44: loss = 0.20548344\n","Start of epoch 45\n","Training loss over epoch:  45 0.20505115\n","Training acc over epoch:  45 0.95841724\n","Validation loss:  45 0.20303549\n","Validation acc:  45 0.95892704\n","epoch 45: loss = 0.20505117\n","Start of epoch 46\n","Training loss over epoch:  46 0.20474142\n","Training acc over epoch:  46 0.9584707\n","Validation loss:  46 0.2016641\n","Validation acc:  46 0.95892704\n","epoch 46: loss = 0.20474146\n","Start of epoch 47\n","Training loss over epoch:  47 0.20413834\n","Training acc over epoch:  47 0.95847666\n","Validation loss:  47 0.20124485\n","Validation acc:  47 0.95892704\n","epoch 47: loss = 0.20413828\n","Start of epoch 48\n","Training loss over epoch:  48 0.2034391\n","Training acc over epoch:  48 0.95848614\n","Validation loss:  48 0.20090136\n","Validation acc:  48 0.95892704\n","epoch 48: loss = 0.20343912\n","Start of epoch 49\n","Training loss over epoch:  49 0.20323336\n","Training acc over epoch:  49 0.958498\n","Validation loss:  49 0.20006365\n","Validation acc:  49 0.95892704\n","epoch 49: loss = 0.20323335\n","Start of epoch 50\n","Training loss over epoch:  50 0.20294207\n","Training acc over epoch:  50 0.9584244\n","Validation loss:  50 0.19936526\n","Validation acc:  50 0.95892704\n","epoch 50: loss = 0.20294209\n","Start of epoch 51\n","Training loss over epoch:  51 0.20200917\n","Training acc over epoch:  51 0.95853245\n","Validation loss:  51 0.19930872\n","Validation acc:  51 0.95892704\n","epoch 51: loss = 0.20200916\n","Start of epoch 52\n","Training loss over epoch:  52 0.20143132\n","Training acc over epoch:  52 0.9584755\n","Validation loss:  52 0.19811314\n","Validation acc:  52 0.95892704\n","epoch 52: loss = 0.20143132\n","Start of epoch 53\n","Training loss over epoch:  53 0.20098042\n","Training acc over epoch:  53 0.95848256\n","Validation loss:  53 0.19794965\n","Validation acc:  53 0.95892704\n","epoch 53: loss = 0.2009804\n","Start of epoch 54\n","Training loss over epoch:  54 0.20050119\n","Training acc over epoch:  54 0.9584612\n","Validation loss:  54 0.19688341\n","Validation acc:  54 0.95892704\n","epoch 54: loss = 0.20050117\n","Start of epoch 55\n","Training loss over epoch:  55 0.19959427\n","Training acc over epoch:  55 0.95855266\n","Validation loss:  55 0.19694184\n","Validation acc:  55 0.95892704\n","epoch 55: loss = 0.19959429\n","Start of epoch 56\n","Training loss over epoch:  56 0.19939035\n","Training acc over epoch:  56 0.958523\n","Validation loss:  56 0.19589506\n","Validation acc:  56 0.95892704\n","epoch 56: loss = 0.19939037\n","Start of epoch 57\n","Training loss over epoch:  57 0.19875397\n","Training acc over epoch:  57 0.95845526\n","Validation loss:  57 0.19536251\n","Validation acc:  57 0.95892704\n","epoch 57: loss = 0.19875395\n","Start of epoch 58\n","Training loss over epoch:  58 0.1984297\n","Training acc over epoch:  58 0.9585016\n","Validation loss:  58 0.1945701\n","Validation acc:  58 0.95892704\n","epoch 58: loss = 0.19842969\n","Start of epoch 59\n","Training loss over epoch:  59 0.19751406\n","Training acc over epoch:  59 0.95845646\n","Validation loss:  59 0.19445238\n","Validation acc:  59 0.95892704\n","epoch 59: loss = 0.19751404\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 86.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9596: \n","Sensitivity: 0.2806598121959617\n","Specificity: 0.7604754688851609\n","F1-score macro: 0.2948768508073101\n","F1-score micro: 0.9595902225587951\n","Training using fold 3\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 84112.0\n","valid_X_fake diff: 9352.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.8857779\n","Training acc over epoch:  0 0.9301661\n","Validation loss:  0 0.24072148\n","Validation acc:  0 0.9585992\n","epoch 0: loss = 0.88577807\n","Start of epoch 1\n","Training loss over epoch:  1 0.22722366\n","Training acc over epoch:  1 0.9580645\n","Validation loss:  1 0.2107494\n","Validation acc:  1 0.9585992\n","epoch 1: loss = 0.22722365\n","Start of epoch 2\n","Training loss over epoch:  2 0.21273936\n","Training acc over epoch:  2 0.95802176\n","Validation loss:  2 0.20565227\n","Validation acc:  2 0.9585992\n","epoch 2: loss = 0.21273936\n","Start of epoch 3\n","Training loss over epoch:  3 0.20706664\n","Training acc over epoch:  3 0.9580123\n","Validation loss:  3 0.19711147\n","Validation acc:  3 0.9585992\n","epoch 3: loss = 0.20706664\n","Start of epoch 4\n","Training loss over epoch:  4 0.19411156\n","Training acc over epoch:  4 0.95799804\n","Validation loss:  4 0.18214032\n","Validation acc:  4 0.9585992\n","epoch 4: loss = 0.19411157\n","Start of epoch 5\n","Training loss over epoch:  5 0.18340719\n","Training acc over epoch:  5 0.9579838\n","Validation loss:  5 0.17285451\n","Validation acc:  5 0.9585992\n","epoch 5: loss = 0.18340722\n","Start of epoch 6\n","Training loss over epoch:  6 0.17624038\n","Training acc over epoch:  6 0.95797783\n","Validation loss:  6 0.16623268\n","Validation acc:  6 0.9585992\n","epoch 6: loss = 0.17624038\n","Start of epoch 7\n","Training loss over epoch:  7 0.17096394\n","Training acc over epoch:  7 0.95801467\n","Validation loss:  7 0.16023648\n","Validation acc:  7 0.9585992\n","epoch 7: loss = 0.17096394\n","Start of epoch 8\n","Training loss over epoch:  8 0.16451693\n","Training acc over epoch:  8 0.9580301\n","Validation loss:  8 0.15002955\n","Validation acc:  8 0.9587631\n","epoch 8: loss = 0.16451693\n","Start of epoch 9\n","Training loss over epoch:  9 0.1501761\n","Training acc over epoch:  9 0.95950514\n","Validation loss:  9 0.13091785\n","Validation acc:  9 0.96247816\n","epoch 9: loss = 0.15017611\n","Start of epoch 10\n","Training loss over epoch:  10 0.13654903\n","Training acc over epoch:  10 0.96097195\n","Validation loss:  10 0.11716206\n","Validation acc:  10 0.9651552\n","epoch 10: loss = 0.13654903\n","Start of epoch 11\n","Training loss over epoch:  11 0.12358555\n","Training acc over epoch:  11 0.9628152\n","Validation loss:  11 0.09992001\n","Validation acc:  11 0.96949303\n","epoch 11: loss = 0.12358556\n","Start of epoch 12\n","Training loss over epoch:  12 0.10821782\n","Training acc over epoch:  12 0.9658747\n","Validation loss:  12 0.07807044\n","Validation acc:  12 0.9755245\n","epoch 12: loss = 0.10821783\n","Start of epoch 13\n","Training loss over epoch:  13 0.09212136\n","Training acc over epoch:  13 0.9704674\n","Validation loss:  13 0.0623445\n","Validation acc:  13 0.9819056\n","epoch 13: loss = 0.09212134\n","Start of epoch 14\n","Training loss over epoch:  14 0.08886045\n","Training acc over epoch:  14 0.9728106\n","Validation loss:  14 0.05828498\n","Validation acc:  14 0.98340255\n","epoch 14: loss = 0.08886046\n","Start of epoch 15\n","Training loss over epoch:  15 0.07500095\n","Training acc over epoch:  15 0.9766801\n","Validation loss:  15 0.05031539\n","Validation acc:  15 0.9860249\n","epoch 15: loss = 0.07500095\n","Start of epoch 16\n","Training loss over epoch:  16 0.06850706\n","Training acc over epoch:  16 0.97877157\n","Validation loss:  16 0.046017706\n","Validation acc:  16 0.9872596\n","epoch 16: loss = 0.06850706\n","Start of epoch 17\n","Training loss over epoch:  17 0.063720465\n","Training acc over epoch:  17 0.98040104\n","Validation loss:  17 0.04303901\n","Validation acc:  17 0.9879917\n","epoch 17: loss = 0.06372047\n","Start of epoch 18\n","Training loss over epoch:  18 0.060269177\n","Training acc over epoch:  18 0.9814925\n","Validation loss:  18 0.03996596\n","Validation acc:  18 0.98929197\n","epoch 18: loss = 0.06026918\n","Start of epoch 19\n","Training loss over epoch:  19 0.056632683\n","Training acc over epoch:  19 0.9827372\n","Validation loss:  19 0.039302275\n","Validation acc:  19 0.99024254\n","epoch 19: loss = 0.056632675\n","Start of epoch 20\n","Training loss over epoch:  20 0.054069165\n","Training acc over epoch:  20 0.9836493\n","Validation loss:  20 0.03545382\n","Validation acc:  20 0.9905376\n","epoch 20: loss = 0.054069176\n","Start of epoch 21\n","Training loss over epoch:  21 0.05147933\n","Training acc over epoch:  21 0.9843976\n","Validation loss:  21 0.034120757\n","Validation acc:  21 0.99123687\n","epoch 21: loss = 0.05147934\n","Start of epoch 22\n","Training loss over epoch:  22 0.04996828\n","Training acc over epoch:  22 0.9848168\n","Validation loss:  22 0.032879997\n","Validation acc:  22 0.9914882\n","epoch 22: loss = 0.049968284\n","Start of epoch 23\n","Training loss over epoch:  23 0.0479489\n","Training acc over epoch:  23 0.9853406\n","Validation loss:  23 0.032051247\n","Validation acc:  23 0.9919471\n","epoch 23: loss = 0.0479489\n","Start of epoch 24\n","Training loss over epoch:  24 0.04655044\n","Training acc over epoch:  24 0.9859641\n","Validation loss:  24 0.030773232\n","Validation acc:  24 0.992264\n","epoch 24: loss = 0.04655044\n","Start of epoch 25\n","Training loss over epoch:  25 0.04463421\n","Training acc over epoch:  25 0.9864107\n","Validation loss:  25 0.029276414\n","Validation acc:  25 0.99248254\n","epoch 25: loss = 0.044634208\n","Start of epoch 26\n","Training loss over epoch:  26 0.04337614\n","Training acc over epoch:  26 0.9867682\n","Validation loss:  26 0.028602168\n","Validation acc:  26 0.9928212\n","epoch 26: loss = 0.043376137\n","Start of epoch 27\n","Training loss over epoch:  27 0.04181902\n","Training acc over epoch:  27 0.9872753\n","Validation loss:  27 0.027703604\n","Validation acc:  27 0.99271196\n","epoch 27: loss = 0.041819025\n","Start of epoch 28\n","Training loss over epoch:  28 0.041002914\n","Training acc over epoch:  28 0.9874938\n","Validation loss:  28 0.027911166\n","Validation acc:  28 0.9927994\n","epoch 28: loss = 0.041002907\n","Start of epoch 29\n","Training loss over epoch:  29 0.040205278\n","Training acc over epoch:  29 0.9877147\n","Validation loss:  29 0.026826203\n","Validation acc:  29 0.99340034\n","epoch 29: loss = 0.040205278\n","Start of epoch 30\n","Training loss over epoch:  30 0.038930487\n","Training acc over epoch:  30 0.98808056\n","Validation loss:  30 0.025570294\n","Validation acc:  30 0.9936844\n","epoch 30: loss = 0.038930498\n","Start of epoch 31\n","Training loss over epoch:  31 0.03781464\n","Training acc over epoch:  31 0.9885045\n","Validation loss:  31 0.02489725\n","Validation acc:  31 0.99397945\n","epoch 31: loss = 0.037814643\n","Start of epoch 32\n","Training loss over epoch:  32 0.036551014\n","Training acc over epoch:  32 0.98887867\n","Validation loss:  32 0.025146516\n","Validation acc:  32 0.9938374\n","epoch 32: loss = 0.036551014\n","Start of epoch 33\n","Training loss over epoch:  33 0.036180295\n","Training acc over epoch:  33 0.9889535\n","Validation loss:  33 0.02469165\n","Validation acc:  33 0.99380463\n","epoch 33: loss = 0.036180295\n","Start of epoch 34\n","Training loss over epoch:  34 0.035337944\n","Training acc over epoch:  34 0.9891174\n","Validation loss:  34 0.02389321\n","Validation acc:  34 0.9942635\n","epoch 34: loss = 0.035337944\n","Start of epoch 35\n","Training loss over epoch:  35 0.033950165\n","Training acc over epoch:  35 0.9895568\n","Validation loss:  35 0.024133995\n","Validation acc:  35 0.9941652\n","epoch 35: loss = 0.03395016\n","Start of epoch 36\n","Training loss over epoch:  36 0.033784788\n","Training acc over epoch:  36 0.98959124\n","Validation loss:  36 0.023154087\n","Validation acc:  36 0.9944602\n","epoch 36: loss = 0.033784788\n","Start of epoch 37\n","Training loss over epoch:  37 0.033273716\n","Training acc over epoch:  37 0.98973733\n","Validation loss:  37 0.02335723\n","Validation acc:  37 0.9944493\n","epoch 37: loss = 0.03327372\n","Start of epoch 38\n","Training loss over epoch:  38 0.032210987\n","Training acc over epoch:  38 0.99005204\n","Validation loss:  38 0.022905443\n","Validation acc:  38 0.99467874\n","epoch 38: loss = 0.032210995\n","Start of epoch 39\n","Training loss over epoch:  39 0.0317109\n","Training acc over epoch:  39 0.99011976\n","Validation loss:  39 0.023552658\n","Validation acc:  39 0.99463505\n","epoch 39: loss = 0.031710904\n","Start of epoch 40\n","Training loss over epoch:  40 0.031295028\n","Training acc over epoch:  40 0.990324\n","Validation loss:  40 0.0225387\n","Validation acc:  40 0.994941\n","epoch 40: loss = 0.03129503\n","Start of epoch 41\n","Training loss over epoch:  41 0.03052186\n","Training acc over epoch:  41 0.990545\n","Validation loss:  41 0.022048054\n","Validation acc:  41 0.9950721\n","epoch 41: loss = 0.030521858\n","Start of epoch 42\n","Training loss over epoch:  42 0.030449238\n","Training acc over epoch:  42 0.9904476\n","Validation loss:  42 0.022441532\n","Validation acc:  42 0.9947115\n","epoch 42: loss = 0.030449236\n","Start of epoch 43\n","Training loss over epoch:  43 0.02956178\n","Training acc over epoch:  43 0.9908371\n","Validation loss:  43 0.022090064\n","Validation acc:  43 0.9949082\n","epoch 43: loss = 0.029561779\n","Start of epoch 44\n","Training loss over epoch:  44 0.028078184\n","Training acc over epoch:  44 0.9913443\n","Validation loss:  44 0.021665549\n","Validation acc:  44 0.9949191\n","epoch 44: loss = 0.028078187\n","Start of epoch 45\n","Training loss over epoch:  45 0.027546883\n","Training acc over epoch:  45 0.99140006\n","Validation loss:  45 0.021038227\n","Validation acc:  45 0.99530154\n","epoch 45: loss = 0.02754688\n","Start of epoch 46\n","Training loss over epoch:  46 0.026301427\n","Training acc over epoch:  46 0.9917445\n","Validation loss:  46 0.020473668\n","Validation acc:  46 0.9955092\n","epoch 46: loss = 0.02630142\n","Start of epoch 47\n","Training loss over epoch:  47 0.024960486\n","Training acc over epoch:  47 0.9922326\n","Validation loss:  47 0.020970976\n","Validation acc:  47 0.9953781\n","epoch 47: loss = 0.024960492\n","Start of epoch 48\n","Training loss over epoch:  48 0.025345964\n","Training acc over epoch:  48 0.992083\n","Validation loss:  48 0.020220717\n","Validation acc:  48 0.9955747\n","epoch 48: loss = 0.025345962\n","Start of epoch 49\n","Training loss over epoch:  49 0.02376394\n","Training acc over epoch:  49 0.99243337\n","Validation loss:  49 0.020013943\n","Validation acc:  49 0.99554193\n","epoch 49: loss = 0.023763936\n","Start of epoch 50\n","Training loss over epoch:  50 0.023553716\n","Training acc over epoch:  50 0.9925319\n","Validation loss:  50 0.019539254\n","Validation acc:  50 0.9955529\n","epoch 50: loss = 0.023553714\n","Start of epoch 51\n","Training loss over epoch:  51 0.02275883\n","Training acc over epoch:  51 0.99282885\n","Validation loss:  51 0.01999858\n","Validation acc:  51 0.9957386\n","epoch 51: loss = 0.02275883\n","Start of epoch 52\n","Training loss over epoch:  52 0.022029236\n","Training acc over epoch:  52 0.9929844\n","Validation loss:  52 0.01984743\n","Validation acc:  52 0.9957823\n","epoch 52: loss = 0.02202924\n","Start of epoch 53\n","Training loss over epoch:  53 0.021927781\n","Training acc over epoch:  53 0.99302953\n","Validation loss:  53 0.019021926\n","Validation acc:  53 0.9958042\n","epoch 53: loss = 0.021927785\n","Start of epoch 54\n","Training loss over epoch:  54 0.021036142\n","Training acc over epoch:  54 0.9933633\n","Validation loss:  54 0.019606037\n","Validation acc:  54 0.99588066\n","epoch 54: loss = 0.021036142\n","Start of epoch 55\n","Training loss over epoch:  55 0.02087645\n","Training acc over epoch:  55 0.9933039\n","Validation loss:  55 0.019367544\n","Validation acc:  55 0.995837\n","epoch 55: loss = 0.020876449\n","Start of epoch 56\n","Training loss over epoch:  56 0.02080322\n","Training acc over epoch:  56 0.9933455\n","Validation loss:  56 0.020109857\n","Validation acc:  56 0.99582607\n","epoch 56: loss = 0.020803222\n","Start of epoch 57\n","Training loss over epoch:  57 0.019743912\n","Training acc over epoch:  57 0.9936317\n","Validation loss:  57 0.018784873\n","Validation acc:  57 0.9960555\n","epoch 57: loss = 0.019743908\n","Start of epoch 58\n","Training loss over epoch:  58 0.019692117\n","Training acc over epoch:  58 0.9936222\n","Validation loss:  58 0.019007586\n","Validation acc:  58 0.9961866\n","epoch 58: loss = 0.019692117\n","Start of epoch 59\n","Training loss over epoch:  59 0.019328982\n","Training acc over epoch:  59 0.9938075\n","Validation loss:  59 0.019285375\n","Validation acc:  59 0.99611014\n","epoch 59: loss = 0.019328984\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 92.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9612: \n","Sensitivity: 0.36118906856387295\n","Specificity: 0.8016566127509914\n","F1-score macro: 0.4091862031554161\n","F1-score micro: 0.96120246659815\n","Missing rate 0.1\n","=====================================================\n","train_X_fake diff: 171228.0\n","valid_X_fake diff: 19038.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.001059\n","Training acc over epoch:  0 0.94716054\n","Validation loss:  0 0.24007763\n","Validation acc:  0 0.9585992\n","epoch 0: loss = 1.0010589\n","Start of epoch 1\n","Training loss over epoch:  1 0.23214081\n","Training acc over epoch:  1 0.9580194\n","Validation loss:  1 0.21606444\n","Validation acc:  1 0.9585992\n","epoch 1: loss = 0.23214076\n","Start of epoch 2\n","Training loss over epoch:  2 0.2159439\n","Training acc over epoch:  2 0.9579861\n","Validation loss:  2 0.20500214\n","Validation acc:  2 0.9585992\n","epoch 2: loss = 0.2159439\n","Start of epoch 3\n","Training loss over epoch:  3 0.20923713\n","Training acc over epoch:  3 0.9579636\n","Validation loss:  3 0.20092458\n","Validation acc:  3 0.9585992\n","epoch 3: loss = 0.20923711\n","Start of epoch 4\n","Training loss over epoch:  4 0.20190665\n","Training acc over epoch:  4 0.9579873\n","Validation loss:  4 0.19146536\n","Validation acc:  4 0.9585992\n","epoch 4: loss = 0.20190667\n","Start of epoch 5\n","Training loss over epoch:  5 0.1957033\n","Training acc over epoch:  5 0.9580313\n","Validation loss:  5 0.18699524\n","Validation acc:  5 0.9585992\n","epoch 5: loss = 0.19570331\n","Start of epoch 6\n","Training loss over epoch:  6 0.18980941\n","Training acc over epoch:  6 0.95797783\n","Validation loss:  6 0.1783824\n","Validation acc:  6 0.9585992\n","epoch 6: loss = 0.18980941\n","Start of epoch 7\n","Training loss over epoch:  7 0.17958738\n","Training acc over epoch:  7 0.9579683\n","Validation loss:  7 0.1660282\n","Validation acc:  7 0.9585992\n","epoch 7: loss = 0.17958736\n","Start of epoch 8\n","Training loss over epoch:  8 0.17028615\n","Training acc over epoch:  8 0.9580063\n","Validation loss:  8 0.15654442\n","Validation acc:  8 0.9585992\n","epoch 8: loss = 0.17028613\n","Start of epoch 9\n","Training loss over epoch:  9 0.16068372\n","Training acc over epoch:  9 0.9583579\n","Validation loss:  9 0.14357197\n","Validation acc:  9 0.9600743\n","epoch 9: loss = 0.16068374\n","Start of epoch 10\n","Training loss over epoch:  10 0.14880301\n","Training acc over epoch:  10 0.9593104\n","Validation loss:  10 0.12951553\n","Validation acc:  10 0.9624344\n","epoch 10: loss = 0.14880304\n","Start of epoch 11\n","Training loss over epoch:  11 0.13778424\n","Training acc over epoch:  11 0.9606311\n","Validation loss:  11 0.11957453\n","Validation acc:  11 0.965177\n","epoch 11: loss = 0.13778427\n","Start of epoch 12\n","Training loss over epoch:  12 0.12993766\n","Training acc over epoch:  12 0.9619518\n","Validation loss:  12 0.10987239\n","Validation acc:  12 0.9670127\n","epoch 12: loss = 0.12993768\n","Start of epoch 13\n","Training loss over epoch:  13 0.122573234\n","Training acc over epoch:  13 0.9633069\n","Validation loss:  13 0.10041505\n","Validation acc:  13 0.9693947\n","epoch 13: loss = 0.12257324\n","Start of epoch 14\n","Training loss over epoch:  14 0.115228705\n","Training acc over epoch:  14 0.9651371\n","Validation loss:  14 0.09232116\n","Validation acc:  14 0.9719515\n","epoch 14: loss = 0.1152287\n","Start of epoch 15\n","Training loss over epoch:  15 0.109020166\n","Training acc over epoch:  15 0.96633905\n","Validation loss:  15 0.08614983\n","Validation acc:  15 0.9741368\n","epoch 15: loss = 0.10902015\n","Start of epoch 16\n","Training loss over epoch:  16 0.10306498\n","Training acc over epoch:  16 0.9680968\n","Validation loss:  16 0.08201188\n","Validation acc:  16 0.97630024\n","epoch 16: loss = 0.10306497\n","Start of epoch 17\n","Training loss over epoch:  17 0.09807075\n","Training acc over epoch:  17 0.96950656\n","Validation loss:  17 0.074741304\n","Validation acc:  17 0.9776333\n","epoch 17: loss = 0.09807075\n","Start of epoch 18\n","Training loss over epoch:  18 0.0930045\n","Training acc over epoch:  18 0.97094005\n","Validation loss:  18 0.0714978\n","Validation acc:  18 0.9799607\n","epoch 18: loss = 0.09300449\n","Start of epoch 19\n","Training loss over epoch:  19 0.08749893\n","Training acc over epoch:  19 0.9729864\n","Validation loss:  19 0.06408533\n","Validation acc:  19 0.98197114\n","epoch 19: loss = 0.08749893\n","Start of epoch 20\n","Training loss over epoch:  20 0.08064466\n","Training acc over epoch:  20 0.97529405\n","Validation loss:  20 0.057150953\n","Validation acc:  20 0.98427665\n","epoch 20: loss = 0.08064467\n","Start of epoch 21\n","Training loss over epoch:  21 0.07249733\n","Training acc over epoch:  21 0.9781611\n","Validation loss:  21 0.051309206\n","Validation acc:  21 0.9857736\n","epoch 21: loss = 0.07249733\n","Start of epoch 22\n","Training loss over epoch:  22 0.06618141\n","Training acc over epoch:  22 0.98030365\n","Validation loss:  22 0.049140014\n","Validation acc:  22 0.98720497\n","epoch 22: loss = 0.066181414\n","Start of epoch 23\n","Training loss over epoch:  23 0.060230106\n","Training acc over epoch:  23 0.98252225\n","Validation loss:  23 0.04313023\n","Validation acc:  23 0.98890954\n","epoch 23: loss = 0.06023011\n","Start of epoch 24\n","Training loss over epoch:  24 0.056343183\n","Training acc over epoch:  24 0.983717\n","Validation loss:  24 0.04038454\n","Validation acc:  24 0.9896416\n","epoch 24: loss = 0.0563432\n","Start of epoch 25\n","Training loss over epoch:  25 0.05291438\n","Training acc over epoch:  25 0.9849083\n","Validation loss:  25 0.038551506\n","Validation acc:  25 0.990472\n","epoch 25: loss = 0.052914392\n","Start of epoch 26\n","Training loss over epoch:  26 0.05031055\n","Training acc over epoch:  26 0.98574793\n","Validation loss:  26 0.035707247\n","Validation acc:  26 0.991073\n","epoch 26: loss = 0.050310563\n","Start of epoch 27\n","Training loss over epoch:  27 0.047175582\n","Training acc over epoch:  27 0.98673016\n","Validation loss:  27 0.034442473\n","Validation acc:  27 0.9916521\n","epoch 27: loss = 0.047175575\n","Start of epoch 28\n","Training loss over epoch:  28 0.04490715\n","Training acc over epoch:  28 0.9874249\n","Validation loss:  28 0.03232899\n","Validation acc:  28 0.9920236\n","epoch 28: loss = 0.044907145\n","Start of epoch 29\n","Training loss over epoch:  29 0.042643283\n","Training acc over epoch:  29 0.9881209\n","Validation loss:  29 0.03148801\n","Validation acc:  29 0.99252623\n","epoch 29: loss = 0.04264328\n","Start of epoch 30\n","Training loss over epoch:  30 0.040939044\n","Training acc over epoch:  30 0.9885853\n","Validation loss:  30 0.030557381\n","Validation acc:  30 0.99293053\n","epoch 30: loss = 0.040939044\n","Start of epoch 31\n","Training loss over epoch:  31 0.038392454\n","Training acc over epoch:  31 0.9893086\n","Validation loss:  31 0.029648582\n","Validation acc:  31 0.99313813\n","epoch 31: loss = 0.038392454\n","Start of epoch 32\n","Training loss over epoch:  32 0.036866374\n","Training acc over epoch:  32 0.9896696\n","Validation loss:  32 0.02891242\n","Validation acc:  32 0.9930288\n","epoch 32: loss = 0.036866378\n","Start of epoch 33\n","Training loss over epoch:  33 0.036027234\n","Training acc over epoch:  33 0.98992854\n","Validation loss:  33 0.02790549\n","Validation acc:  33 0.99336755\n","epoch 33: loss = 0.03602724\n","Start of epoch 34\n","Training loss over epoch:  34 0.03530027\n","Training acc over epoch:  34 0.9901459\n","Validation loss:  34 0.028479146\n","Validation acc:  34 0.99310535\n","epoch 34: loss = 0.035300266\n","Start of epoch 35\n","Training loss over epoch:  35 0.033841383\n","Training acc over epoch:  35 0.9904452\n","Validation loss:  35 0.02737528\n","Validation acc:  35 0.9934441\n","epoch 35: loss = 0.033841383\n","Start of epoch 36\n","Training loss over epoch:  36 0.032619096\n","Training acc over epoch:  36 0.9907421\n","Validation loss:  36 0.02684171\n","Validation acc:  36 0.9938265\n","epoch 36: loss = 0.032619096\n","Start of epoch 37\n","Training loss over epoch:  37 0.03184243\n","Training acc over epoch:  37 0.9909583\n","Validation loss:  37 0.026334085\n","Validation acc:  37 0.99381554\n","epoch 37: loss = 0.031842425\n","Start of epoch 38\n","Training loss over epoch:  38 0.030911997\n","Training acc over epoch:  38 0.99116254\n","Validation loss:  38 0.025892077\n","Validation acc:  38 0.9939139\n","epoch 38: loss = 0.030911995\n","Start of epoch 39\n","Training loss over epoch:  39 0.030068727\n","Training acc over epoch:  39 0.9912825\n","Validation loss:  39 0.026192386\n","Validation acc:  39 0.9940013\n","epoch 39: loss = 0.03006873\n","Start of epoch 40\n","Training loss over epoch:  40 0.029070266\n","Training acc over epoch:  40 0.99152595\n","Validation loss:  40 0.02590824\n","Validation acc:  40 0.9938921\n","epoch 40: loss = 0.02907027\n","Start of epoch 41\n","Training loss over epoch:  41 0.028907783\n","Training acc over epoch:  41 0.9916376\n","Validation loss:  41 0.025815945\n","Validation acc:  41 0.9938702\n","epoch 41: loss = 0.028907781\n","Start of epoch 42\n","Training loss over epoch:  42 0.027722532\n","Training acc over epoch:  42 0.9919987\n","Validation loss:  42 0.0250777\n","Validation acc:  42 0.9940997\n","epoch 42: loss = 0.027722526\n","Start of epoch 43\n","Training loss over epoch:  43 0.027432239\n","Training acc over epoch:  43 0.9918989\n","Validation loss:  43 0.025193766\n","Validation acc:  43 0.99401224\n","epoch 43: loss = 0.027432242\n","Start of epoch 44\n","Training loss over epoch:  44 0.026720984\n","Training acc over epoch:  44 0.99221957\n","Validation loss:  44 0.026490645\n","Validation acc:  44 0.99421984\n","epoch 44: loss = 0.02672098\n","Start of epoch 45\n","Training loss over epoch:  45 0.026154757\n","Training acc over epoch:  45 0.99220055\n","Validation loss:  45 0.025573378\n","Validation acc:  45 0.9943728\n","epoch 45: loss = 0.026154751\n","Start of epoch 46\n","Training loss over epoch:  46 0.025471285\n","Training acc over epoch:  46 0.9924001\n","Validation loss:  46 0.02561376\n","Validation acc:  46 0.9941215\n","epoch 46: loss = 0.025471289\n","Start of epoch 47\n","Training loss over epoch:  47 0.024984846\n","Training acc over epoch:  47 0.99248916\n","Validation loss:  47 0.024971846\n","Validation acc:  47 0.99399036\n","epoch 47: loss = 0.024984848\n","Start of epoch 48\n","Training loss over epoch:  48 0.024435101\n","Training acc over epoch:  48 0.99257827\n","Validation loss:  48 0.025274973\n","Validation acc:  48 0.9941215\n","epoch 48: loss = 0.024435097\n","Start of epoch 49\n","Training loss over epoch:  49 0.024046397\n","Training acc over epoch:  49 0.9927837\n","Validation loss:  49 0.02509709\n","Validation acc:  49 0.9945149\n","epoch 49: loss = 0.024046393\n","Start of epoch 50\n","Training loss over epoch:  50 0.023743208\n","Training acc over epoch:  50 0.99277186\n","Validation loss:  50 0.024939675\n","Validation acc:  50 0.99419796\n","epoch 50: loss = 0.023743214\n","Start of epoch 51\n","Training loss over epoch:  51 0.023379596\n","Training acc over epoch:  51 0.9928669\n","Validation loss:  51 0.02535621\n","Validation acc:  51 0.9938265\n","epoch 51: loss = 0.0233796\n","Start of epoch 52\n","Training loss over epoch:  52 0.0226393\n","Training acc over epoch:  52 0.9931531\n","Validation loss:  52 0.025132727\n","Validation acc:  52 0.9942963\n","epoch 52: loss = 0.0226393\n","Start of epoch 53\n","Training loss over epoch:  53 0.022326833\n","Training acc over epoch:  53 0.9931032\n","Validation loss:  53 0.02532301\n","Validation acc:  53 0.9945039\n","epoch 53: loss = 0.022326833\n","Start of epoch 54\n","Training loss over epoch:  54 0.021868186\n","Training acc over epoch:  54 0.9932303\n","Validation loss:  54 0.025272353\n","Validation acc:  54 0.99414337\n","epoch 54: loss = 0.02186819\n","Start of epoch 55\n","Training loss over epoch:  55 0.02174736\n","Training acc over epoch:  55 0.99331933\n","Validation loss:  55 0.025111886\n","Validation acc:  55 0.99423075\n","epoch 55: loss = 0.021747364\n","Start of epoch 56\n","Training loss over epoch:  56 0.021321325\n","Training acc over epoch:  56 0.99331695\n","Validation loss:  56 0.02494675\n","Validation acc:  56 0.9942745\n","epoch 56: loss = 0.02132133\n","Start of epoch 57\n","Training loss over epoch:  57 0.020434324\n","Training acc over epoch:  57 0.9936364\n","Validation loss:  57 0.025550863\n","Validation acc:  57 0.9944602\n","epoch 57: loss = 0.020434322\n","Start of epoch 58\n","Training loss over epoch:  58 0.020526132\n","Training acc over epoch:  58 0.9936127\n","Validation loss:  58 0.025258016\n","Validation acc:  58 0.9941215\n","epoch 58: loss = 0.020526132\n","Start of epoch 59\n","Training loss over epoch:  59 0.019941358\n","Training acc over epoch:  59 0.9937303\n","Validation loss:  59 0.02550885\n","Validation acc:  59 0.9943291\n","epoch 59: loss = 0.01994136\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 89.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9604: \n","Sensitivity: 0.38508299617982267\n","Specificity: 0.824070487053773\n","F1-score macro: 0.42644932904597016\n","F1-score micro: 0.960431654676259\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 342456.0\n","valid_X_fake diff: 38076.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.9821295\n","Training acc over epoch:  0 0.89252055\n","Validation loss:  0 0.24094427\n","Validation acc:  0 0.9585992\n","epoch 0: loss = 0.9821294\n","Start of epoch 1\n","Training loss over epoch:  1 0.23953497\n","Training acc over epoch:  1 0.9580099\n","Validation loss:  1 0.22750436\n","Validation acc:  1 0.9585992\n","epoch 1: loss = 0.23953497\n","Start of epoch 2\n","Training loss over epoch:  2 0.22598037\n","Training acc over epoch:  2 0.9580348\n","Validation loss:  2 0.21740255\n","Validation acc:  2 0.9585992\n","epoch 2: loss = 0.2259804\n","Start of epoch 3\n","Training loss over epoch:  3 0.22057208\n","Training acc over epoch:  3 0.9580313\n","Validation loss:  3 0.21701379\n","Validation acc:  3 0.9585992\n","epoch 3: loss = 0.22057214\n","Start of epoch 4\n","Training loss over epoch:  4 0.21983542\n","Training acc over epoch:  4 0.9580099\n","Validation loss:  4 0.2160521\n","Validation acc:  4 0.9585992\n","epoch 4: loss = 0.21983545\n","Start of epoch 5\n","Training loss over epoch:  5 0.21951883\n","Training acc over epoch:  5 0.9579695\n","Validation loss:  5 0.21524662\n","Validation acc:  5 0.9585992\n","epoch 5: loss = 0.21951881\n","Start of epoch 6\n","Training loss over epoch:  6 0.21900783\n","Training acc over epoch:  6 0.9580158\n","Validation loss:  6 0.21483664\n","Validation acc:  6 0.9585992\n","epoch 6: loss = 0.21900782\n","Start of epoch 7\n","Training loss over epoch:  7 0.21877275\n","Training acc over epoch:  7 0.9579588\n","Validation loss:  7 0.21447933\n","Validation acc:  7 0.9585992\n","epoch 7: loss = 0.21877274\n","Start of epoch 8\n","Training loss over epoch:  8 0.21828376\n","Training acc over epoch:  8 0.9580099\n","Validation loss:  8 0.2144922\n","Validation acc:  8 0.9585992\n","epoch 8: loss = 0.21828374\n","Start of epoch 9\n","Training loss over epoch:  9 0.21784656\n","Training acc over epoch:  9 0.95804197\n","Validation loss:  9 0.21430643\n","Validation acc:  9 0.9585992\n","epoch 9: loss = 0.21784656\n","Start of epoch 10\n","Training loss over epoch:  10 0.21766557\n","Training acc over epoch:  10 0.95802057\n","Validation loss:  10 0.21411271\n","Validation acc:  10 0.9585992\n","epoch 10: loss = 0.21766555\n","Start of epoch 11\n","Training loss over epoch:  11 0.21759015\n","Training acc over epoch:  11 0.9580265\n","Validation loss:  11 0.2140331\n","Validation acc:  11 0.9585992\n","epoch 11: loss = 0.21759014\n","Start of epoch 12\n","Training loss over epoch:  12 0.21729913\n","Training acc over epoch:  12 0.95804197\n","Validation loss:  12 0.21390997\n","Validation acc:  12 0.9585992\n","epoch 12: loss = 0.21729909\n","Start of epoch 13\n","Training loss over epoch:  13 0.21735996\n","Training acc over epoch:  13 0.95800513\n","Validation loss:  13 0.2139784\n","Validation acc:  13 0.9585992\n","epoch 13: loss = 0.21735996\n","Start of epoch 14\n","Training loss over epoch:  14 0.21719378\n","Training acc over epoch:  14 0.95800036\n","Validation loss:  14 0.21380854\n","Validation acc:  14 0.9585992\n","epoch 14: loss = 0.2171938\n","Start of epoch 15\n","Training loss over epoch:  15 0.21688966\n","Training acc over epoch:  15 0.95802414\n","Validation loss:  15 0.21358809\n","Validation acc:  15 0.9585992\n","epoch 15: loss = 0.21688965\n","Start of epoch 16\n","Training loss over epoch:  16 0.21706136\n","Training acc over epoch:  16 0.9579707\n","Validation loss:  16 0.21347432\n","Validation acc:  16 0.9585992\n","epoch 16: loss = 0.21706136\n","Start of epoch 17\n","Training loss over epoch:  17 0.21670863\n","Training acc over epoch:  17 0.9580099\n","Validation loss:  17 0.21333852\n","Validation acc:  17 0.9585992\n","epoch 17: loss = 0.21670866\n","Start of epoch 18\n","Training loss over epoch:  18 0.21639809\n","Training acc over epoch:  18 0.9580348\n","Validation loss:  18 0.21317771\n","Validation acc:  18 0.9585992\n","epoch 18: loss = 0.21639815\n","Start of epoch 19\n","Training loss over epoch:  19 0.21635152\n","Training acc over epoch:  19 0.9580063\n","Validation loss:  19 0.21287902\n","Validation acc:  19 0.9585992\n","epoch 19: loss = 0.21635155\n","Start of epoch 20\n","Training loss over epoch:  20 0.21154559\n","Training acc over epoch:  20 0.958017\n","Validation loss:  20 0.19889642\n","Validation acc:  20 0.9585992\n","epoch 20: loss = 0.21154559\n","Start of epoch 21\n","Training loss over epoch:  21 0.20282668\n","Training acc over epoch:  21 0.95802176\n","Validation loss:  21 0.19167714\n","Validation acc:  21 0.9585992\n","epoch 21: loss = 0.2028266\n","Start of epoch 22\n","Training loss over epoch:  22 0.19200456\n","Training acc over epoch:  22 0.9580384\n","Validation loss:  22 0.17684747\n","Validation acc:  22 0.9585992\n","epoch 22: loss = 0.19200453\n","Start of epoch 23\n","Training loss over epoch:  23 0.1746467\n","Training acc over epoch:  23 0.958327\n","Validation loss:  23 0.15869282\n","Validation acc:  23 0.95951706\n","epoch 23: loss = 0.1746467\n","Start of epoch 24\n","Training loss over epoch:  24 0.15787172\n","Training acc over epoch:  24 0.9591073\n","Validation loss:  24 0.13726753\n","Validation acc:  24 0.9617242\n","epoch 24: loss = 0.15787174\n","Start of epoch 25\n","Training loss over epoch:  25 0.13998036\n","Training acc over epoch:  25 0.9607712\n","Validation loss:  25 0.12017733\n","Validation acc:  25 0.9635162\n","epoch 25: loss = 0.13998036\n","Start of epoch 26\n","Training loss over epoch:  26 0.124802984\n","Training acc over epoch:  26 0.9627665\n","Validation loss:  26 0.10577449\n","Validation acc:  26 0.9677229\n","epoch 26: loss = 0.12480299\n","Start of epoch 27\n","Training loss over epoch:  27 0.11055659\n","Training acc over epoch:  27 0.9655765\n","Validation loss:  27 0.09425573\n","Validation acc:  27 0.9711757\n","epoch 27: loss = 0.110556595\n","Start of epoch 28\n","Training loss over epoch:  28 0.09820465\n","Training acc over epoch:  28 0.96921676\n","Validation loss:  28 0.08108125\n","Validation acc:  28 0.9759288\n","epoch 28: loss = 0.098204635\n","Start of epoch 29\n","Training loss over epoch:  29 0.08874148\n","Training acc over epoch:  29 0.9728855\n","Validation loss:  29 0.071956776\n","Validation acc:  29 0.97896636\n","epoch 29: loss = 0.08874148\n","Start of epoch 30\n","Training loss over epoch:  30 0.08226461\n","Training acc over epoch:  30 0.97534275\n","Validation loss:  30 0.066738546\n","Validation acc:  30 0.9811298\n","epoch 30: loss = 0.0822646\n","Start of epoch 31\n","Training loss over epoch:  31 0.0766106\n","Training acc over epoch:  31 0.9774782\n","Validation loss:  31 0.06321978\n","Validation acc:  31 0.9824082\n","epoch 31: loss = 0.0766106\n","Start of epoch 32\n","Training loss over epoch:  32 0.073391154\n","Training acc over epoch:  32 0.9784117\n","Validation loss:  32 0.0604158\n","Validation acc:  32 0.982889\n","epoch 32: loss = 0.07339114\n","Start of epoch 33\n","Training loss over epoch:  33 0.07038258\n","Training acc over epoch:  33 0.9794818\n","Validation loss:  33 0.058534827\n","Validation acc:  33 0.9835774\n","epoch 33: loss = 0.07038257\n","Start of epoch 34\n","Training loss over epoch:  34 0.06775592\n","Training acc over epoch:  34 0.9801208\n","Validation loss:  34 0.056723718\n","Validation acc:  34 0.98423296\n","epoch 34: loss = 0.06775593\n","Start of epoch 35\n","Training loss over epoch:  35 0.06564106\n","Training acc over epoch:  35 0.9810531\n","Validation loss:  35 0.05480029\n","Validation acc:  35 0.98467004\n","epoch 35: loss = 0.06564105\n","Start of epoch 36\n","Training loss over epoch:  36 0.06363247\n","Training acc over epoch:  36 0.9816588\n","Validation loss:  36 0.053840898\n","Validation acc:  36 0.9854895\n","epoch 36: loss = 0.06363247\n","Start of epoch 37\n","Training loss over epoch:  37 0.061513998\n","Training acc over epoch:  37 0.9823191\n","Validation loss:  37 0.052887343\n","Validation acc:  37 0.98600304\n","epoch 37: loss = 0.061514005\n","Start of epoch 38\n","Training loss over epoch:  38 0.06006354\n","Training acc over epoch:  38 0.9828215\n","Validation loss:  38 0.051703185\n","Validation acc:  38 0.9860577\n","epoch 38: loss = 0.060063537\n","Start of epoch 39\n","Training loss over epoch:  39 0.05818817\n","Training acc over epoch:  39 0.98339754\n","Validation loss:  39 0.05013126\n","Validation acc:  39 0.9867461\n","epoch 39: loss = 0.058188163\n","Start of epoch 40\n","Training loss over epoch:  40 0.05591508\n","Training acc over epoch:  40 0.9840472\n","Validation loss:  40 0.04890249\n","Validation acc:  40 0.9870957\n","epoch 40: loss = 0.055915087\n","Start of epoch 41\n","Training loss over epoch:  41 0.054229893\n","Training acc over epoch:  41 0.9845864\n","Validation loss:  41 0.04848201\n","Validation acc:  41 0.98727053\n","epoch 41: loss = 0.054229885\n","Start of epoch 42\n","Training loss over epoch:  42 0.053302422\n","Training acc over epoch:  42 0.9849332\n","Validation loss:  42 0.047313146\n","Validation acc:  42 0.98789334\n","epoch 42: loss = 0.053302422\n","Start of epoch 43\n","Training loss over epoch:  43 0.051538024\n","Training acc over epoch:  43 0.98549616\n","Validation loss:  43 0.04632818\n","Validation acc:  43 0.988243\n","epoch 43: loss = 0.05153802\n","Start of epoch 44\n","Training loss over epoch:  44 0.049949788\n","Training acc over epoch:  44 0.9860021\n","Validation loss:  44 0.045629974\n","Validation acc:  44 0.9881884\n","epoch 44: loss = 0.04994978\n","Start of epoch 45\n","Training loss over epoch:  45 0.04901624\n","Training acc over epoch:  45 0.98627883\n","Validation loss:  45 0.045346007\n","Validation acc:  45 0.9882321\n","epoch 45: loss = 0.04901625\n","Start of epoch 46\n","Training loss over epoch:  46 0.047650527\n","Training acc over epoch:  46 0.9865603\n","Validation loss:  46 0.0443287\n","Validation acc:  46 0.9885271\n","epoch 46: loss = 0.047650523\n","Start of epoch 47\n","Training loss over epoch:  47 0.046675917\n","Training acc over epoch:  47 0.98697716\n","Validation loss:  47 0.04355097\n","Validation acc:  47 0.98890954\n","epoch 47: loss = 0.046675924\n","Start of epoch 48\n","Training loss over epoch:  48 0.045162804\n","Training acc over epoch:  48 0.9872563\n","Validation loss:  48 0.043222584\n","Validation acc:  48 0.9890953\n","epoch 48: loss = 0.045162797\n","Start of epoch 49\n","Training loss over epoch:  49 0.0445496\n","Training acc over epoch:  49 0.9873525\n","Validation loss:  49 0.042949505\n","Validation acc:  49 0.9889969\n","epoch 49: loss = 0.044549607\n","Start of epoch 50\n","Training loss over epoch:  50 0.04347147\n","Training acc over epoch:  50 0.9877955\n","Validation loss:  50 0.04283072\n","Validation acc:  50 0.9889751\n","epoch 50: loss = 0.043471467\n","Start of epoch 51\n","Training loss over epoch:  51 0.042413406\n","Training acc over epoch:  51 0.9880746\n","Validation loss:  51 0.042076223\n","Validation acc:  51 0.9892592\n","epoch 51: loss = 0.04241341\n","Start of epoch 52\n","Training loss over epoch:  52 0.041561835\n","Training acc over epoch:  52 0.98820406\n","Validation loss:  52 0.042909473\n","Validation acc:  52 0.9888112\n","epoch 52: loss = 0.04156184\n","Start of epoch 53\n","Training loss over epoch:  53 0.040530574\n","Training acc over epoch:  53 0.9884368\n","Validation loss:  53 0.04218074\n","Validation acc:  53 0.9891936\n","epoch 53: loss = 0.04053057\n","Start of epoch 54\n","Training loss over epoch:  54 0.039550573\n","Training acc over epoch:  54 0.9886993\n","Validation loss:  54 0.041426983\n","Validation acc:  54 0.98931384\n","epoch 54: loss = 0.039550573\n","Start of epoch 55\n","Training loss over epoch:  55 0.038917206\n","Training acc over epoch:  55 0.98871\n","Validation loss:  55 0.041157853\n","Validation acc:  55 0.9896088\n","epoch 55: loss = 0.038917206\n","Start of epoch 56\n","Training loss over epoch:  56 0.037655517\n","Training acc over epoch:  56 0.98905796\n","Validation loss:  56 0.04085057\n","Validation acc:  56 0.98958695\n","epoch 56: loss = 0.03765552\n","Start of epoch 57\n","Training loss over epoch:  57 0.036682494\n","Training acc over epoch:  57 0.9893656\n","Validation loss:  57 0.04050209\n","Validation acc:  57 0.98977274\n","epoch 57: loss = 0.036682494\n","Start of epoch 58\n","Training loss over epoch:  58 0.03614451\n","Training acc over epoch:  58 0.989476\n","Validation loss:  58 0.040486854\n","Validation acc:  58 0.98998034\n","epoch 58: loss = 0.0361445\n","Start of epoch 59\n","Training loss over epoch:  59 0.035566956\n","Training acc over epoch:  59 0.9895509\n","Validation loss:  59 0.039984778\n","Validation acc:  59 0.9898055\n","epoch 59: loss = 0.035566963\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:09<00:00, 90.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9616: \n","Sensitivity: 0.38251349252440003\n","Specificity: 0.8084850054847503\n","F1-score macro: 0.43175155435107926\n","F1-score micro: 0.9615675880348353\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ifLke-oWc1Ok"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1lo9cmptjRRESsVlVECvlJFHbLeaygq2s","timestamp":1640128751823},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb","timestamp":1621552889682}],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}