{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18280,
     "status": "ok",
     "timestamp": 1686884255842,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "doSuiA3dyTyg",
    "outputId": "3355d0ab-f4d3-480d-decc-14d05943dee7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10664,
     "status": "ok",
     "timestamp": 1686884266502,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "liJJGzQp4qzO",
    "outputId": "cdfb244f-707f-4443-bf03-603ad440bc19"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m591.0/591.0 kB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Collecting scikit-allel\n",
      "  Downloading scikit_allel-1.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.4/7.4 MB\u001B[0m \u001B[31m55.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-allel) (1.22.4)\n",
      "Requirement already satisfied: dask[array] in /usr/local/lib/python3.10/dist-packages (from scikit-allel) (2022.12.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (2023.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (23.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (6.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask[array]->scikit-allel) (1.0.0)\n",
      "Installing collected packages: scikit-allel\n",
      "Successfully installed scikit-allel-1.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n",
    "!pip install toolz scikit-allel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3118,
     "status": "ok",
     "timestamp": 1686884269616,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "ZWzi3Z3L5RO2",
    "outputId": "9eb264df-3898-4265-b36d-53dc9e6ac250"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjGOO5PdFPf7"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4506,
     "status": "ok",
     "timestamp": 1686884274119,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "odmhCqSVFPf8",
    "outputId": "431ad049-c8f4-48f8-d60c-21c87bcd24bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"MODIN_CPUS\"] = \"8\"\n",
    "# from distributed import Client\n",
    "# client = Client()\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import efficientnet as efn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "# import allel\n",
    "from scipy.spatial.distance import squareform\n",
    "%matplotlib inline\n",
    "from toolz import interleave\n",
    "from tqdm import tqdm\n",
    "import allel\n",
    "from scipy.spatial.distance import squareform\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLd26RspFhaS"
   },
   "source": [
    "## Hardware Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1686884274119,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "SLd7mAFgFUnR",
    "outputId": "44261fc9-8de1-4af7-92d4-95bc151b2451"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on GPU\n",
      "N_REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', TPU.master())\n",
    "except ValueError:\n",
    "    print('Running on GPU')\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "# Number of computing cores, is 8 for a TPU V3-8\n",
    "print(f'N_REPLICAS: {N_REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A77GFE3xFPf8"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3zy8i_8FPf_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686884275210,
     "user_tz": 240,
     "elapsed": 1093,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "88cd1a1e-6d89-4601-d6b2-01e30903bd57"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8    9    ...  838  839  840  \\\n",
       "HG00096  3|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "HG00097  0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "HG00099  0|0  0|0  0|0  0|0  0|0  0|1  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "HG00100  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "HG00101  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "NA21137  3|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "NA21141  3|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "NA21142  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "NA21143  0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "NA21144  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n",
       "\n",
       "         841  842  843  844  845  846  847  \n",
       "HG00096  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "HG00097  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "HG00099  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "HG00100  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "HG00101  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "NA21137  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "NA21141  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "NA21142  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "NA21143  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "NA21144  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n",
       "\n",
       "[2504 rows x 848 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-5788a17e-dec3-4397-b48b-2c94569eb1b5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "      <th>847</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG00096</th>\n",
       "      <td>3|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00097</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>1|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00099</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00100</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00101</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21137</th>\n",
       "      <td>3|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21141</th>\n",
       "      <td>3|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>1|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21142</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21143</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>1|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA21144</th>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>...</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "      <td>0|0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2504 rows × 848 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5788a17e-dec3-4397-b48b-2c94569eb1b5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5788a17e-dec3-4397-b48b-2c94569eb1b5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5788a17e-dec3-4397-b48b-2c94569eb1b5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "root_dir = '[data_path]'\n",
    "new_data_header = \"\"\n",
    "# get header\n",
    "with open(root_dir + \"ALL.chr22.mergedSV.v8.20130502.svs.genotypes.vcf\", 'r') as f_in:\n",
    "    # skip info\n",
    "    for line_num in range(70):\n",
    "        f_in.readline()\n",
    "\n",
    "    new_data_header = f_in.readline()\n",
    "# load data\n",
    "\n",
    "# load genotype\n",
    "genotypes = pd.read_csv(root_dir + \"ALL.chr22.mergedSV.v8.20130502.svs.genotypes.vcf\", comment='#', sep='\\t', names=new_data_header.strip().split('\\t'), header=None)\n",
    "info = genotypes.iloc[:, :9]\n",
    "genotypes = genotypes.iloc[:, 9:].T\n",
    "headers = genotypes.columns[:]\n",
    "genotypes"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ped_file = '[data_path]/integrated_call_samples.20130502.ALL.ped'\n",
    "pedigree = pd.read_csv(ped_file, sep='\\t', index_col='Individual ID')"
   ],
   "metadata": {
    "id": "gp5LsBCPWjdF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Y_train = pedigree.loc[genotypes.index]['Population']\n",
    "Y_train.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sc53l7EXWmuk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686884276351,
     "user_tz": 240,
     "elapsed": 5,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "d854c883-993e-43c0-a5f9-60ced6e28261"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2504,)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1686884276352,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "hiP0EEwsEMNP",
    "outputId": "55852df9-10cc-477f-ed85-a7a4a7b6e60f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2504, 848)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X = genotypes[:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_max_genotype(g):\n",
    "  v1, v2 = g.split(\"|\")\n",
    "  return max(int(v1), int(v2)) + 1\n",
    "\n",
    "def key_gen(v1, v2):\n",
    "  return f\"{v1}|{v2}\""
   ],
   "metadata": {
    "id": "q2Q455LrQEc7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "genotype_keys = np.unique(X.values)\n",
    "max_gen = max(map(get_max_genotype, genotype_keys))\n",
    "onehot_encoding_depth = max_gen**2 + 1\n",
    "MISSING_VALUE = [0 for _ in range(onehot_encoding_depth)]\n",
    "MISSING_VALUE[-1] = 1\n",
    "genotype_keys = [key_gen(i,j) for i in range(max_gen) for j in range(max_gen)]\n",
    "replacement_dict = {k:i for i,k in enumerate(genotype_keys)}\n",
    "reverse_replacement_dict = {v:k for k,v in replacement_dict.items()}\n",
    "# replacement_dict"
   ],
   "metadata": {
    "id": "0tZbhQKILnvZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8402,
     "status": "ok",
     "timestamp": 1686884286653,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "DNU6deVHXIdE",
    "outputId": "a510c25d-a375-4a44-c071-3fb74b287b9e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2504, 848)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "X = X.replace(replacement_dict)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MAF"
   ],
   "metadata": {
    "id": "Utkz6Di_pMRJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def maf_mapper(txt):\n",
    "  af = min(list(map(lambda x: float(x), txt.split(\";\")[1].split(\"=\")[1].split(\",\"))))\n",
    "  return min(af, 1-af)"
   ],
   "metadata": {
    "id": "zkgK-B2vnSqg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mafs = info.iloc[:, 7:8].applymap(maf_mapper)\n",
    "mafs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ff9pAePPGi1G",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885322369,
     "user_tz": 240,
     "elapsed": 178,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "ef48ca45-508d-4ea6-854c-ae7a28f21238"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         INFO\n",
       "0    0.001797\n",
       "1    0.024960\n",
       "2    0.005791\n",
       "3    0.037141\n",
       "4    0.000399\n",
       "..        ...\n",
       "843  0.000200\n",
       "844  0.001198\n",
       "845  0.000599\n",
       "846  0.000200\n",
       "847  0.000200\n",
       "\n",
       "[848 rows x 1 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-e64089d6-2f42-4dec-b38d-b34936b8f7e8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0.001198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0.000599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848 rows × 1 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e64089d6-2f42-4dec-b38d-b34936b8f7e8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e64089d6-2f42-4dec-b38d-b34936b8f7e8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e64089d6-2f42-4dec-b38d-b34936b8f7e8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "max(mafs.values)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUULbATzL7HR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885323851,
     "user_tz": 240,
     "elapsed": 155,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "47963377-ad22-4f9e-bedc-c03a0f5b3fcb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.496605])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.hist(mafs, bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5]);"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "fjWLZ4nWLKc3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885334210,
     "user_tz": 240,
     "elapsed": 326,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "2d64f9dd-b6ae-4988-fb39-2e3137291447"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bin_labels = np.digitize(mafs, bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5], right=False)\n",
    "bin_general_labels, bin_counts = np.unique(bin_labels, return_counts=True)\n",
    "bin_general_labels, bin_counts"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWiEzZuzeABb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885345298,
     "user_tz": 240,
     "elapsed": 432,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "d9ec6e18-e465-43a2-de45-fe4790caacf3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5]), array([783,  22,  17,  16,  10]))"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pZoO-FvKdr3"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "feature_size = X.shape[1]\n",
    "inChannel = onehot_encoding_depth\n",
    "# optimizer learning rate\n",
    "learning_rate = 0.001\n",
    "epochs = 60\n",
    "#epochs = 100   # chr20 LOS 5K\n",
    "\n",
    "\n",
    "# training batch size\n",
    "#batch_size = 32   # u19, 4984 samples\n",
    "bs = 32\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "# l1 regulalization\n",
    "kr = 1e-4\n",
    "k_initial = 'glorot_uniform'\n",
    "\n",
    "\n",
    "# channel = inChannel\n",
    "channel = inChannel\n",
    "\n",
    "ndf_num = 128\n",
    "kernel_len = 32\n",
    "num_latent = ndf_num*4\n",
    "p_size = 2\n",
    "\n",
    "\n",
    "\n",
    "#dr_rate = drop_prec\n",
    "dr_rate = 0.2  # avoid overfitting for missing ratio of 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCkow4q5MJk_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgGz9_6u8u-O"
   },
   "outputs": [],
   "source": [
    "# AE model in one cell\n",
    "# # V1: verify our own ae model with yeast genotype data\n",
    "\n",
    "# # 2.2.8 build variatial autoencoder for snp with subclassing function.\n",
    "\n",
    "class SNP_ENCODER(tf.keras.Model):\n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):\n",
    "        super(SNP_ENCODER, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate\n",
    "\n",
    "        # object, can be saved in tf mode\n",
    "        self.stride=1\n",
    "        self.kl = kernel_len\n",
    "\n",
    "\n",
    "    def build(self, inputs):\n",
    "        #encoder\n",
    "        # dense layer 1\n",
    "        self.c1 = layers.Conv1D(filters=self.ndf, kernel_size=self.kl, strides=self.stride, padding=\"same\",\n",
    "                      activation='relu', use_bias=True,\n",
    "                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr),\n",
    "                      input_shape=(self.feature_size, self.channel))\n",
    "        self.p1 = layers.MaxPooling1D(pool_size=p_size)\n",
    "        self.drop1 = layers.Dropout(rate=self.dr)\n",
    "\n",
    "        # dense layer 2\n",
    "        self.c2 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\",\n",
    "                      activation='relu', use_bias=True, kernel_initializer=k_initial,\n",
    "                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n",
    "        self.p2 = layers.MaxPooling1D(pool_size=p_size)\n",
    "        self.drop2 = layers.Dropout(rate=self.dr)\n",
    "\n",
    "        # dense layer 3\n",
    "        self.c3 = layers.Conv1D(filters=(4*self.ndf), kernel_size=self.kl, strides=1, padding=\"same\",\n",
    "                      activation='relu', use_bias=True, kernel_initializer=k_initial,\n",
    "                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n",
    "        self.p3 = layers.MaxPooling1D(pool_size=p_size)\n",
    "        self.drop3 = layers.Dropout(rate=self.dr)\n",
    "\n",
    "        super(SNP_ENCODER, self).build(inputs)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print('SNP_ENCODER training flag: ', training)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.p1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "\n",
    "        x = self.c3(x)\n",
    "        return x\n",
    "\n",
    "    # AFAIK: The most convenient method to print model.summary()\n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.feature_size, self.channel))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "\n",
    "\n",
    "# SNP_DECODER(keras.Model):\n",
    "class SNP_DECODER(tf.keras.Model):\n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):\n",
    "        super(SNP_DECODER, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate\n",
    "\n",
    "        # object, can be saved in tf mode\n",
    "        self.stride=1\n",
    "        self.kl=kernel_len\n",
    "\n",
    "    def build(self, inputs):\n",
    "        #decoder\n",
    "        self.c1 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\",\n",
    "                      activation='relu', use_bias=True,\n",
    "                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr),\n",
    "                      input_shape=((self.feature_size>>2), self.n_latent))\n",
    "\n",
    "\n",
    "        self.s1 = layers.UpSampling1D(size=p_size)\n",
    "        self.drop1 = layers.Dropout(rate=self.dr)\n",
    "\n",
    "        # dense layer 2\n",
    "        self.c2 = layers.Conv1D(filters=(1*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\",\n",
    "                      activation='relu', use_bias=True,\n",
    "                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr))\n",
    "\n",
    "        self.s2 = layers.UpSampling1D(size=p_size)\n",
    "        self.drop2 = layers.Dropout(rate=self.dr)\n",
    "\n",
    "\n",
    "        # dense layer6\n",
    "        self.c3 = layers.Conv1D(filters=self.channel, kernel_size=self.kl, strides=1, padding=\"same\",\n",
    "                      activation='softmax', use_bias=True)\n",
    "\n",
    "        super(SNP_DECODER, self).build(inputs)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        #print('SNP_DECODER training flag: ', training)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.s1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "\n",
    "\n",
    "        d_out = self.c3(x)\n",
    "        return d_out\n",
    "\n",
    "    # AFAIK: The most convenient method to print model.summary()\n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.feature_size>>2, self.n_latent))\n",
    "\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "\n",
    "\n",
    "#class SNP_AE(keras.Model):\n",
    "class SNP_AE(tf.keras.Model):\n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):\n",
    "        super(SNP_AE, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate\n",
    "\n",
    "        self.encoder = SNP_ENCODER(self.feature_size)\n",
    "        self.decoder = SNP_DECODER(self.feature_size)\n",
    "\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        latent = self.encoder(x, training)\n",
    "        res = self.decoder(latent, training)\n",
    "\n",
    "        return res, latent\n",
    "\n",
    "    # AFAIK: The most convenient method to print model.summary()\n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.feature_size, self.channel))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plot snp vae encoder model\n",
    "\n",
    "SNP_encoder = SNP_ENCODER(feature_size)\n",
    "SNP_encoder.build((None, feature_size, channel))\n",
    "SNP_encoder.build_graph().summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaAkHl0XKyIM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885360251,
     "user_tz": 240,
     "elapsed": 3798,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "cc8fc6e6-7988-443e-e908-18a9729b7ca3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 848, 37)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 848, 128)          151680    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 424, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 424, 128)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 424, 256)          1048832   \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 212, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 212, 256)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 212, 512)          4194816   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,395,328\n",
      "Trainable params: 5,395,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plot snp vae decoder model\n",
    "\n",
    "SNP_decoder = SNP_DECODER(feature_size)\n",
    "\n",
    "SNP_decoder.build((None, feature_size>>2, num_latent))\n",
    "SNP_decoder.build_graph().summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SEoV7YaLzwv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885361751,
     "user_tz": 240,
     "elapsed": 443,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "4d78d229-fe23-4590-8a60-36f1a3054f6a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 212, 512)]        0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 212, 256)          4194560   \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1D  (None, 424, 256)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 424, 256)          0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 424, 128)          1048704   \n",
      "                                                                 \n",
      " up_sampling1d_1 (UpSampling  (None, 848, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 848, 128)          0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 848, 37)           151589    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,394,853\n",
      "Trainable params: 5,394,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plot snp vae model\n",
    "\n",
    "SNP_ae = SNP_AE(feature_size)\n",
    "SNP_ae.build((None, feature_size, channel))\n",
    "SNP_ae.build_graph().summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmP5dBy-L2Ep",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686885365084,
     "user_tz": 240,
     "elapsed": 722,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "91f2ebf4-5e67-4605-fb73-5adc8d8fad1b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 848, 37)]         0         \n",
      "                                                                 \n",
      " snp_encoder_1 (SNP_ENCODER)  (None, 212, 512)         5395328   \n",
      "                                                                 \n",
      " snp_decoder_1 (SNP_DECODER)  (None, 848, 37)          5394853   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,790,181\n",
      "Trainable params: 10,790,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# AE model sub-functions\n",
    "\n",
    "def generate_fake_missing(x_in, missing_ratio=0.5):\n",
    "\n",
    "        # Generates missing genotypes\n",
    "        # different missing loci for each individuals\n",
    "        x_fake = x_in.copy()   # with .copy() to not overwrite the original data\n",
    "\n",
    "        for i in range(x_in.shape[0]):\n",
    "            missing_size = int(missing_ratio * x_in.shape[1])\n",
    "\n",
    "            # without repeat random numbers: set replace with false\n",
    "            missing_index = np.random.choice(x_in.shape[1], size=missing_size, replace=False)\n",
    "\n",
    "            # missing loci are encoded as [0, 0]\n",
    "            # x_fake[i, missing_index, :] = [0, 0, 1]  # yeast\n",
    "            x_fake[i, missing_index, :] = MISSING_VALUE  # human\n",
    "\n",
    "        return x_fake\n",
    "        #return x_fake, x_in\n",
    "\n",
    "\n",
    "\n",
    "def loss_function_cce(recon_x, x):\n",
    "    # orders: y_true, y_pred\n",
    "    cce = tf.keras.losses.categorical_crossentropy(x, recon_x)\n",
    "\n",
    "    #cce = np.double(cce)\n",
    "    cce = K.cast(cce, dtype='float32')\n",
    "\n",
    "    lamb1 = 1.0\n",
    "    loss = lamb1*cce\n",
    "    #print('loss:', loss)\n",
    "\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    #print('ave loss:', loss)\n",
    "\n",
    "    return loss\n"
   ],
   "metadata": {
    "id": "-EXc-erqK16d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOA_NexzN5Qq"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "save_dir = \"save_path\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "  # shutil.rmtree(save_dir)\n",
    "  os.makedirs(save_dir)"
   ],
   "metadata": {
    "id": "ufNuxe7q93pL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# With constraint\n",
    "N_SPLITS=3\n",
    "results = None\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=2022)\n",
    "fold = 0\n",
    "_x = tf.keras.utils.to_categorical(X.to_numpy(), inChannel)\n",
    "print(_x.shape)\n",
    "_y = Y_train.to_numpy()\n",
    "for train_index, test_index in kf.split(_x):\n",
    "  fold += 1\n",
    "  accuracies = []\n",
    "  print(f\"Training using fold {fold}\")\n",
    "  print(\"*******************************************\")\n",
    "  print(\"*******************************************\")\n",
    "\n",
    "  x_train, y_train, test_dataset, test_indices = _x[train_index], _y[train_index], (_x[test_index], _y[test_index]),Y_train.index[test_index]\n",
    "  x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10,\n",
    "                                      random_state=fold,\n",
    "                                      shuffle=True,\n",
    "                                      stratify=y_train)\n",
    "\n",
    "\n",
    "  for missing_ratio in [\n",
    "                        0.05,\n",
    "                        0.1,\n",
    "                        0.2\n",
    "                        ]:\n",
    "    train_X = np.copy(x_train)\n",
    "    valid_X = np.copy(x_valid)\n",
    "    print(f\"Missing rate {missing_ratio}\")\n",
    "    print(\"=====================================================\")\n",
    "    train_X_fake = generate_fake_missing(train_X, missing_ratio)\n",
    "\n",
    "    diff = np.absolute(np.array(train_X) - np.array(train_X_fake))\n",
    "    print('train_X_fake diff:', np.sum(diff))\n",
    "\n",
    "\n",
    "    valid_X_fake = generate_fake_missing(valid_X, missing_ratio)\n",
    "    diff = np.absolute(np.array(valid_X) - np.array(valid_X_fake))\n",
    "    print('valid_X_fake diff:', np.sum(diff))\n",
    "\n",
    "\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "      model = SNP_AE(feature_size)\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "      train_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n",
    "      val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n",
    "\n",
    "      train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "      val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "        #----shuffle train data and lablel for each epoch-----------------------------------------------------------------------------------#\n",
    "        # shuffle data and labels at the same time\n",
    "        idx = train_X.shape[0]\n",
    "\n",
    "\n",
    "        indices = tf.range(start=0, limit=idx, dtype=tf.int32)\n",
    "\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "        train_X_fake = tf.gather(train_X_fake, shuffled_indices)\n",
    "        train_X = tf.gather(train_X, shuffled_indices)\n",
    "\n",
    "        snp_x = tf.data.Dataset.from_tensor_slices(train_X_fake).batch(bs, drop_remainder=True)\n",
    "        snp_y = tf.data.Dataset.from_tensor_slices(train_X).batch(bs, drop_remainder=True)\n",
    "\n",
    "        snp_x_v = tf.data.Dataset.from_tensor_slices(valid_X_fake).batch(bs, drop_remainder=True)\n",
    "        snp_y_v = tf.data.Dataset.from_tensor_slices(valid_X).batch(bs, drop_remainder=True)\n",
    "\n",
    "        loss_batch = []\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (snp_fake_batch, snp_label_batch) in enumerate(zip(snp_x, snp_y)):\n",
    "          with tf.GradientTape() as tape:\n",
    "              recon_inputs, latents= model(snp_fake_batch, training=True)\n",
    "              loss = loss_function_cce(recon_inputs, snp_label_batch)\n",
    "\n",
    "\n",
    "          grads = tape.gradient(loss, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "          loss_batch.append(loss.numpy())\n",
    "\n",
    "\n",
    "          # Update training metric.\n",
    "          train_acc_metric.update_state(snp_label_batch, recon_inputs)\n",
    "          train_loss_metric.update_state(snp_label_batch, recon_inputs)\n",
    "\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_loss = train_loss_metric.result()\n",
    "        print(\"Training loss over epoch: \", epoch, train_loss.numpy())\n",
    "\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: \", epoch, train_acc.numpy())\n",
    "\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_loss_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in zip(snp_x_v, snp_y_v):\n",
    "\n",
    "            val_recons, latents = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_loss_metric.update_state(y_batch_val, val_recons)\n",
    "            val_acc_metric.update_state(y_batch_val, val_recons)\n",
    "\n",
    "        val_loss = val_loss_metric.result()\n",
    "        val_acc = val_acc_metric.result()\n",
    "\n",
    "        val_loss_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        #print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Validation loss: \", epoch, val_loss.numpy())\n",
    "        print(\"Validation acc: \", epoch, val_acc.numpy())\n",
    "\n",
    "\n",
    "        #print('epoch %s: batch loss = %s' % (epoch, loss_batch))\n",
    "        loss_epoch = np.mean(loss_batch)\n",
    "        print('epoch %s: loss = %s' % (epoch, loss_epoch))\n",
    "\n",
    "    save_name = save_dir + f\"preds_mixed_mr_{missing_ratio}_fold_{fold}_probs\"\n",
    "    avg_accuracy = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1], inChannel-1), dtype='float32')\n",
    "    test_X_missing = np.copy(test_dataset[0])\n",
    "    for i in tqdm(list(range(test_dataset[0].shape[0]))):\n",
    "      missing_index, _ = train_test_split(np.arange(x_train.shape[1]), train_size=missing_ratio,\n",
    "                                    random_state=i + fold,\n",
    "                                    shuffle=True,\n",
    "                                    stratify=bin_labels\n",
    "                                    )\n",
    "      test_X_missing[i:i+1, missing_index, :] = MISSING_VALUE\n",
    "      # predict\n",
    "      predict_onehot, _ = model(test_X_missing[i:i+1, :, :], training=False)\n",
    "      predict_onehot = predict_onehot.numpy()\n",
    "      # only care the missing position\n",
    "      predict_missing_onehot = predict_onehot[0:1, missing_index, :]\n",
    "      # predict label\n",
    "      predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
    "\n",
    "      preds.extend(predict_missing.ravel().tolist())\n",
    "\n",
    "      # predict_haplotypes = np.argmax(predict_onehot, axis=2)\n",
    "      # Only for haploids\n",
    "      to_save_array[i] = predict_onehot[:, :, :-1]\n",
    "      # real label\n",
    "      label_missing_onehot = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n",
    "      label_missing = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n",
    "      true_labels.extend(label_missing.ravel().tolist())\n",
    "      # accuracy\n",
    "      correct_prediction = np.equal(predict_missing, label_missing)\n",
    "      accuracy = np.mean(correct_prediction)\n",
    "\n",
    "      avg_accuracy.append(accuracy)\n",
    "\n",
    "    # df = pd.DataFrame(to_save_array, columns= headers[:], index = Y_train.index[test_index])\n",
    "    # df.to_csv(save_name)\n",
    "    np.save(save_name, to_save_array)\n",
    "    print('The average imputation accuracy' \\\n",
    "          'on test data with {} missing genotypes is {:.4f}: '\n",
    "        .format(missing_ratio, np.mean(avg_accuracy)))\n",
    "    cnf_matrix = confusion_matrix(true_labels, preds)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP)\n",
    "    print(f\"Sensitivity: {np.mean(TPR)}\")\n",
    "    print(f\"Specificity: {np.mean(TNR)}\")\n",
    "    print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n",
    "    print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n",
    "    accuracies.append(np.mean(avg_accuracy))\n",
    "\n"
   ],
   "metadata": {
    "id": "Gq5VWuKWVZVx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1cee6cf7-f8d9-47fd-88ec-0f2a5c2ced64",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686888877507,
     "user_tz": 240,
     "elapsed": 3417464,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2504, 848, 37)\n",
      "Training using fold 1\n",
      "*******************************************\n",
      "*******************************************\n",
      "Missing rate 0.05\n",
      "=====================================================\n",
      "train_X_fake diff: 126168.0\n",
      "valid_X_fake diff: 14028.0\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f33ec57cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f33ec57cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss over epoch:  0 1.173883\n",
      "Training acc over epoch:  0 0.7759306\n",
      "Validation loss:  0 0.28635374\n",
      "Validation acc:  0 0.94966096\n",
      "epoch 0: loss = 1.1738831\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.27326295\n",
      "Training acc over epoch:  1 0.9494174\n",
      "Validation loss:  1 0.2572011\n",
      "Validation acc:  1 0.94966096\n",
      "epoch 1: loss = 0.27326298\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.25644988\n",
      "Training acc over epoch:  2 0.9494014\n",
      "Validation loss:  2 0.24477701\n",
      "Validation acc:  2 0.94966096\n",
      "epoch 2: loss = 0.25644988\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.24415697\n",
      "Training acc over epoch:  3 0.94948953\n",
      "Validation loss:  3 0.23408246\n",
      "Validation acc:  3 0.94966096\n",
      "epoch 3: loss = 0.24415699\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.2335581\n",
      "Training acc over epoch:  4 0.9494471\n",
      "Validation loss:  4 0.22211283\n",
      "Validation acc:  4 0.94966096\n",
      "epoch 4: loss = 0.23355812\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.22405703\n",
      "Training acc over epoch:  5 0.9494439\n",
      "Validation loss:  5 0.21143882\n",
      "Validation acc:  5 0.9497126\n",
      "epoch 5: loss = 0.22405703\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.21153021\n",
      "Training acc over epoch:  6 0.94956726\n",
      "Validation loss:  6 0.19492707\n",
      "Validation acc:  6 0.9504201\n",
      "epoch 6: loss = 0.21153021\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.19690572\n",
      "Training acc over epoch:  7 0.9502458\n",
      "Validation loss:  7 0.17842576\n",
      "Validation acc:  7 0.9514962\n",
      "epoch 7: loss = 0.1969057\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.18358837\n",
      "Training acc over epoch:  8 0.9509067\n",
      "Validation loss:  8 0.16423029\n",
      "Validation acc:  8 0.9524322\n",
      "epoch 8: loss = 0.18358836\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.17271149\n",
      "Training acc over epoch:  9 0.9514675\n",
      "Validation loss:  9 0.15383016\n",
      "Validation acc:  9 0.9530071\n",
      "epoch 9: loss = 0.17271152\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.16394715\n",
      "Training acc over epoch:  10 0.95201546\n",
      "Validation loss:  10 0.14418088\n",
      "Validation acc:  10 0.9538473\n",
      "epoch 10: loss = 0.16394714\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.15458937\n",
      "Training acc over epoch:  11 0.9529007\n",
      "Validation loss:  11 0.13537619\n",
      "Validation acc:  11 0.9556162\n",
      "epoch 11: loss = 0.1545894\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.14727135\n",
      "Training acc over epoch:  12 0.95381397\n",
      "Validation loss:  12 0.12595032\n",
      "Validation acc:  12 0.95963293\n",
      "epoch 12: loss = 0.14727136\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.13969816\n",
      "Training acc over epoch:  13 0.9549363\n",
      "Validation loss:  13 0.117518544\n",
      "Validation acc:  13 0.9612397\n",
      "epoch 13: loss = 0.13969813\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.13256443\n",
      "Training acc over epoch:  14 0.956678\n",
      "Validation loss:  14 0.1104146\n",
      "Validation acc:  14 0.9650796\n",
      "epoch 14: loss = 0.13256446\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.12516548\n",
      "Training acc over epoch:  15 0.9585101\n",
      "Validation loss:  15 0.101899646\n",
      "Validation acc:  15 0.9680056\n",
      "epoch 15: loss = 0.12516548\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.11754867\n",
      "Training acc over epoch:  16 0.9612467\n",
      "Validation loss:  16 0.09390217\n",
      "Validation acc:  16 0.97043043\n",
      "epoch 16: loss = 0.11754866\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.11103503\n",
      "Training acc over epoch:  17 0.963457\n",
      "Validation loss:  17 0.0871314\n",
      "Validation acc:  17 0.9724351\n",
      "epoch 17: loss = 0.11103501\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.104850166\n",
      "Training acc over epoch:  18 0.96567774\n",
      "Validation loss:  18 0.08160951\n",
      "Validation acc:  18 0.97603923\n",
      "epoch 18: loss = 0.10485016\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.098152995\n",
      "Training acc over epoch:  19 0.9678872\n",
      "Validation loss:  19 0.07485816\n",
      "Validation acc:  19 0.9776312\n",
      "epoch 19: loss = 0.09815299\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.09199132\n",
      "Training acc over epoch:  20 0.96994686\n",
      "Validation loss:  20 0.06988428\n",
      "Validation acc:  20 0.9789505\n",
      "epoch 20: loss = 0.09199133\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.08620974\n",
      "Training acc over epoch:  21 0.9719673\n",
      "Validation loss:  21 0.06538196\n",
      "Validation acc:  21 0.9804319\n",
      "epoch 21: loss = 0.08620975\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.081923656\n",
      "Training acc over epoch:  22 0.9734918\n",
      "Validation loss:  22 0.060554866\n",
      "Validation acc:  22 0.9827167\n",
      "epoch 22: loss = 0.08192366\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.075854436\n",
      "Training acc over epoch:  23 0.97568846\n",
      "Validation loss:  23 0.054100454\n",
      "Validation acc:  23 0.9846993\n",
      "epoch 23: loss = 0.07585444\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.070083484\n",
      "Training acc over epoch:  24 0.97784907\n",
      "Validation loss:  24 0.04902473\n",
      "Validation acc:  24 0.98594487\n",
      "epoch 24: loss = 0.07008349\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.06405587\n",
      "Training acc over epoch:  25 0.98003775\n",
      "Validation loss:  25 0.045867663\n",
      "Validation acc:  25 0.9877211\n",
      "epoch 25: loss = 0.064055875\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.059311014\n",
      "Training acc over epoch:  26 0.98165923\n",
      "Validation loss:  26 0.04104093\n",
      "Validation acc:  26 0.98872346\n",
      "epoch 26: loss = 0.05931101\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.055580866\n",
      "Training acc over epoch:  27 0.9830227\n",
      "Validation loss:  27 0.038756266\n",
      "Validation acc:  27 0.9897111\n",
      "epoch 27: loss = 0.05558087\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.05240372\n",
      "Training acc over epoch:  28 0.9841042\n",
      "Validation loss:  28 0.036566414\n",
      "Validation acc:  28 0.9904113\n",
      "epoch 28: loss = 0.05240373\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.05009129\n",
      "Training acc over epoch:  29 0.9848509\n",
      "Validation loss:  29 0.034525838\n",
      "Validation acc:  29 0.9909935\n",
      "epoch 29: loss = 0.05009129\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.04731531\n",
      "Training acc over epoch:  30 0.98583865\n",
      "Validation loss:  30 0.032928694\n",
      "Validation acc:  30 0.9914652\n",
      "epoch 30: loss = 0.04731531\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.04493566\n",
      "Training acc over epoch:  31 0.98650277\n",
      "Validation loss:  31 0.0324307\n",
      "Validation acc:  31 0.99153894\n",
      "epoch 31: loss = 0.04493565\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.0436325\n",
      "Training acc over epoch:  32 0.98691934\n",
      "Validation loss:  32 0.03211413\n",
      "Validation acc:  32 0.99177474\n",
      "epoch 32: loss = 0.043632504\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.04187006\n",
      "Training acc over epoch:  33 0.98743445\n",
      "Validation loss:  33 0.02982219\n",
      "Validation acc:  33 0.9924381\n",
      "epoch 33: loss = 0.04187007\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.040315054\n",
      "Training acc over epoch:  34 0.9879872\n",
      "Validation loss:  34 0.028471509\n",
      "Validation acc:  34 0.9927992\n",
      "epoch 34: loss = 0.04031506\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.038574267\n",
      "Training acc over epoch:  35 0.9884375\n",
      "Validation loss:  35 0.027333925\n",
      "Validation acc:  35 0.9928803\n",
      "epoch 35: loss = 0.038574267\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.0374409\n",
      "Training acc over epoch:  36 0.98880917\n",
      "Validation loss:  36 0.02671449\n",
      "Validation acc:  36 0.9932783\n",
      "epoch 36: loss = 0.037440903\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.03641423\n",
      "Training acc over epoch:  37 0.9892298\n",
      "Validation loss:  37 0.026292887\n",
      "Validation acc:  37 0.99330044\n",
      "epoch 37: loss = 0.03641423\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.035304513\n",
      "Training acc over epoch:  38 0.9896015\n",
      "Validation loss:  38 0.024909338\n",
      "Validation acc:  38 0.99375737\n",
      "epoch 38: loss = 0.035304517\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.034302484\n",
      "Training acc over epoch:  39 0.989837\n",
      "Validation loss:  39 0.024200331\n",
      "Validation acc:  39 0.99376476\n",
      "epoch 39: loss = 0.03430248\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.033094894\n",
      "Training acc over epoch:  40 0.99015987\n",
      "Validation loss:  40 0.023913145\n",
      "Validation acc:  40 0.99388266\n",
      "epoch 40: loss = 0.033094894\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.032447353\n",
      "Training acc over epoch:  41 0.99045306\n",
      "Validation loss:  41 0.023431761\n",
      "Validation acc:  41 0.99404484\n",
      "epoch 41: loss = 0.032447346\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.032016963\n",
      "Training acc over epoch:  42 0.9905284\n",
      "Validation loss:  42 0.023040563\n",
      "Validation acc:  42 0.99436176\n",
      "epoch 42: loss = 0.032016966\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.030528829\n",
      "Training acc over epoch:  43 0.99101067\n",
      "Validation loss:  43 0.022790201\n",
      "Validation acc:  43 0.99428064\n",
      "epoch 43: loss = 0.030528829\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.030132487\n",
      "Training acc over epoch:  44 0.9910411\n",
      "Validation loss:  44 0.022836477\n",
      "Validation acc:  44 0.9942217\n",
      "epoch 44: loss = 0.030132486\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.029001841\n",
      "Training acc over epoch:  45 0.99132067\n",
      "Validation loss:  45 0.021695787\n",
      "Validation acc:  45 0.994546\n",
      "epoch 45: loss = 0.029001845\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.02849808\n",
      "Training acc over epoch:  46 0.9915923\n",
      "Validation loss:  46 0.02171487\n",
      "Validation acc:  46 0.99465656\n",
      "epoch 46: loss = 0.02849808\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.02815853\n",
      "Training acc over epoch:  47 0.9915907\n",
      "Validation loss:  47 0.02114686\n",
      "Validation acc:  47 0.994686\n",
      "epoch 47: loss = 0.02815853\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.027213948\n",
      "Training acc over epoch:  48 0.99186385\n",
      "Validation loss:  48 0.02059567\n",
      "Validation acc:  48 0.994745\n",
      "epoch 48: loss = 0.027213944\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.02695589\n",
      "Training acc over epoch:  49 0.9919015\n",
      "Validation loss:  49 0.021585926\n",
      "Validation acc:  49 0.9945313\n",
      "epoch 49: loss = 0.02695589\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.026661072\n",
      "Training acc over epoch:  50 0.99205774\n",
      "Validation loss:  50 0.020833146\n",
      "Validation acc:  50 0.9947376\n",
      "epoch 50: loss = 0.026661083\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.025548646\n",
      "Training acc over epoch:  51 0.9922364\n",
      "Validation loss:  51 0.020323893\n",
      "Validation acc:  51 0.99489975\n",
      "epoch 51: loss = 0.02554865\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.024977239\n",
      "Training acc over epoch:  52 0.9924679\n",
      "Validation loss:  52 0.019967481\n",
      "Validation acc:  52 0.9948334\n",
      "epoch 52: loss = 0.024977243\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.024670694\n",
      "Training acc over epoch:  53 0.9926097\n",
      "Validation loss:  53 0.022071935\n",
      "Validation acc:  53 0.9951577\n",
      "epoch 53: loss = 0.024670692\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.024152512\n",
      "Training acc over epoch:  54 0.992721\n",
      "Validation loss:  54 0.019788716\n",
      "Validation acc:  54 0.99475235\n",
      "epoch 54: loss = 0.024152506\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.02362993\n",
      "Training acc over epoch:  55 0.9928725\n",
      "Validation loss:  55 0.019794974\n",
      "Validation acc:  55 0.99495137\n",
      "epoch 55: loss = 0.02362993\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.022977356\n",
      "Training acc over epoch:  56 0.9929878\n",
      "Validation loss:  56 0.019672498\n",
      "Validation acc:  56 0.9950029\n",
      "epoch 56: loss = 0.022977352\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.022740843\n",
      "Training acc over epoch:  57 0.9930575\n",
      "Validation loss:  57 0.020146262\n",
      "Validation acc:  57 0.9951356\n",
      "epoch 57: loss = 0.022740843\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.021971775\n",
      "Training acc over epoch:  58 0.99322414\n",
      "Validation loss:  58 0.019193267\n",
      "Validation acc:  58 0.9951356\n",
      "epoch 58: loss = 0.021971773\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.021860847\n",
      "Training acc over epoch:  59 0.99328345\n",
      "Validation loss:  59 0.019817395\n",
      "Validation acc:  59 0.9952683\n",
      "epoch 59: loss = 0.02186085\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:20<00:00, 39.96it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.05 missing genotypes is 0.9553: \n",
      "Sensitivity: nan\n",
      "Specificity: 0.947241560536452\n",
      "F1-score macro: 0.17828707171583336\n",
      "F1-score micro: 0.9553179355574565\n",
      "Missing rate 0.1\n",
      "=====================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-53-ab2fe261998b>:183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  TPR = TP/(TP+FN)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_X_fake diff: 252336.0\n",
      "valid_X_fake diff: 28056.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.3512841\n",
      "Training acc over epoch:  0 0.7541161\n",
      "Validation loss:  0 0.29541236\n",
      "Validation acc:  0 0.94966096\n",
      "epoch 0: loss = 1.351284\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.2878604\n",
      "Training acc over epoch:  1 0.9494495\n",
      "Validation loss:  1 0.2690759\n",
      "Validation acc:  1 0.94966096\n",
      "epoch 1: loss = 0.28786036\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.26598486\n",
      "Training acc over epoch:  2 0.9494543\n",
      "Validation loss:  2 0.2497121\n",
      "Validation acc:  2 0.94966096\n",
      "epoch 2: loss = 0.26598483\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.25318\n",
      "Training acc over epoch:  3 0.94942623\n",
      "Validation loss:  3 0.23992822\n",
      "Validation acc:  3 0.94966096\n",
      "epoch 3: loss = 0.25318\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.2439232\n",
      "Training acc over epoch:  4 0.9494831\n",
      "Validation loss:  4 0.23364913\n",
      "Validation acc:  4 0.94966096\n",
      "epoch 4: loss = 0.2439232\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.23845427\n",
      "Training acc over epoch:  5 0.9494078\n",
      "Validation loss:  5 0.22789748\n",
      "Validation acc:  5 0.94966096\n",
      "epoch 5: loss = 0.2384543\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.2331246\n",
      "Training acc over epoch:  6 0.9494751\n",
      "Validation loss:  6 0.22244625\n",
      "Validation acc:  6 0.94966096\n",
      "epoch 6: loss = 0.23312461\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.22793543\n",
      "Training acc over epoch:  7 0.9493934\n",
      "Validation loss:  7 0.21583585\n",
      "Validation acc:  7 0.94966096\n",
      "epoch 7: loss = 0.22793542\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.22267671\n",
      "Training acc over epoch:  8 0.94953036\n",
      "Validation loss:  8 0.21068555\n",
      "Validation acc:  8 0.94993365\n",
      "epoch 8: loss = 0.22267671\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.21623051\n",
      "Training acc over epoch:  9 0.9498653\n",
      "Validation loss:  9 0.20131958\n",
      "Validation acc:  9 0.9510613\n",
      "epoch 9: loss = 0.21623051\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.20419696\n",
      "Training acc over epoch:  10 0.9503283\n",
      "Validation loss:  10 0.18436068\n",
      "Validation acc:  10 0.95177627\n",
      "epoch 10: loss = 0.20419697\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.18853486\n",
      "Training acc over epoch:  11 0.950962\n",
      "Validation loss:  11 0.16628312\n",
      "Validation acc:  11 0.9527049\n",
      "epoch 11: loss = 0.18853483\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.17276257\n",
      "Training acc over epoch:  12 0.95200026\n",
      "Validation loss:  12 0.14809492\n",
      "Validation acc:  12 0.95484966\n",
      "epoch 12: loss = 0.17276259\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.15461399\n",
      "Training acc over epoch:  13 0.953697\n",
      "Validation loss:  13 0.13110477\n",
      "Validation acc:  13 0.95794517\n",
      "epoch 13: loss = 0.15461399\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.13909939\n",
      "Training acc over epoch:  14 0.9561284\n",
      "Validation loss:  14 0.1134534\n",
      "Validation acc:  14 0.96141654\n",
      "epoch 14: loss = 0.13909937\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.12391595\n",
      "Training acc over epoch:  15 0.9603743\n",
      "Validation loss:  15 0.09655288\n",
      "Validation acc:  15 0.9701135\n",
      "epoch 15: loss = 0.12391594\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.110778\n",
      "Training acc over epoch:  16 0.9653372\n",
      "Validation loss:  16 0.08545806\n",
      "Validation acc:  16 0.974661\n",
      "epoch 16: loss = 0.11077797\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.10113277\n",
      "Training acc over epoch:  17 0.9686082\n",
      "Validation loss:  17 0.07685335\n",
      "Validation acc:  17 0.97709316\n",
      "epoch 17: loss = 0.10113279\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.09489962\n",
      "Training acc over epoch:  18 0.9707688\n",
      "Validation loss:  18 0.07153333\n",
      "Validation acc:  18 0.9792305\n",
      "epoch 18: loss = 0.09489961\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.088039\n",
      "Training acc over epoch:  19 0.9730696\n",
      "Validation loss:  19 0.065721385\n",
      "Validation acc:  19 0.98125\n",
      "epoch 19: loss = 0.088039\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.08305406\n",
      "Training acc over epoch:  20 0.9749659\n",
      "Validation loss:  20 0.061421447\n",
      "Validation acc:  20 0.98224497\n",
      "epoch 20: loss = 0.083054066\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.07960072\n",
      "Training acc over epoch:  21 0.9762068\n",
      "Validation loss:  21 0.058354843\n",
      "Validation acc:  21 0.9833653\n",
      "epoch 21: loss = 0.079600714\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.074748315\n",
      "Training acc over epoch:  22 0.97784346\n",
      "Validation loss:  22 0.05531871\n",
      "Validation acc:  22 0.9846845\n",
      "epoch 22: loss = 0.07474832\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.07201642\n",
      "Training acc over epoch:  23 0.9788649\n",
      "Validation loss:  23 0.054298718\n",
      "Validation acc:  23 0.9856132\n",
      "epoch 23: loss = 0.07201642\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.06827643\n",
      "Training acc over epoch:  24 0.97997046\n",
      "Validation loss:  24 0.050967198\n",
      "Validation acc:  24 0.9864755\n",
      "epoch 24: loss = 0.068276435\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.06630836\n",
      "Training acc over epoch:  25 0.9807403\n",
      "Validation loss:  25 0.049096294\n",
      "Validation acc:  25 0.9871315\n",
      "epoch 25: loss = 0.066308364\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.063651115\n",
      "Training acc over epoch:  26 0.98159593\n",
      "Validation loss:  26 0.047500256\n",
      "Validation acc:  26 0.9874337\n",
      "epoch 26: loss = 0.0636511\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.06180067\n",
      "Training acc over epoch:  27 0.98218954\n",
      "Validation loss:  27 0.04552094\n",
      "Validation acc:  27 0.98815596\n",
      "epoch 27: loss = 0.061800677\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.06015943\n",
      "Training acc over epoch:  28 0.9827896\n",
      "Validation loss:  28 0.044263747\n",
      "Validation acc:  28 0.9884876\n",
      "epoch 28: loss = 0.060159422\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.058323197\n",
      "Training acc over epoch:  29 0.98331034\n",
      "Validation loss:  29 0.043047253\n",
      "Validation acc:  29 0.98903304\n",
      "epoch 29: loss = 0.058323193\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.05643105\n",
      "Training acc over epoch:  30 0.98405296\n",
      "Validation loss:  30 0.04186146\n",
      "Validation acc:  30 0.9893868\n",
      "epoch 30: loss = 0.05643105\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.054610047\n",
      "Training acc over epoch:  31 0.9846041\n",
      "Validation loss:  31 0.040164236\n",
      "Validation acc:  31 0.989969\n",
      "epoch 31: loss = 0.054610044\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.053263593\n",
      "Training acc over epoch:  32 0.9850311\n",
      "Validation loss:  32 0.039755635\n",
      "Validation acc:  32 0.99005747\n",
      "epoch 32: loss = 0.053263597\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.05175051\n",
      "Training acc over epoch:  33 0.9855254\n",
      "Validation loss:  33 0.038061198\n",
      "Validation acc:  33 0.99048495\n",
      "epoch 33: loss = 0.05175051\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.050149214\n",
      "Training acc over epoch:  34 0.9860333\n",
      "Validation loss:  34 0.036897432\n",
      "Validation acc:  34 0.990684\n",
      "epoch 34: loss = 0.050149214\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.048538998\n",
      "Training acc over epoch:  35 0.98649156\n",
      "Validation loss:  35 0.03648064\n",
      "Validation acc:  35 0.99098617\n",
      "epoch 35: loss = 0.048539\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.047793314\n",
      "Training acc over epoch:  36 0.986679\n",
      "Validation loss:  36 0.03508819\n",
      "Validation acc:  36 0.99120724\n",
      "epoch 36: loss = 0.047793306\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.0461084\n",
      "Training acc over epoch:  37 0.9870948\n",
      "Validation loss:  37 0.034426212\n",
      "Validation acc:  37 0.99137676\n",
      "epoch 37: loss = 0.046108395\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.04495726\n",
      "Training acc over epoch:  38 0.98750174\n",
      "Validation loss:  38 0.033801377\n",
      "Validation acc:  38 0.9915831\n",
      "epoch 38: loss = 0.044957265\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.043649517\n",
      "Training acc over epoch:  39 0.9879111\n",
      "Validation loss:  39 0.032870233\n",
      "Validation acc:  39 0.9919148\n",
      "epoch 39: loss = 0.043649506\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.04300643\n",
      "Training acc over epoch:  40 0.9880938\n",
      "Validation loss:  40 0.032572016\n",
      "Validation acc:  40 0.9921728\n",
      "epoch 40: loss = 0.04300643\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.041528568\n",
      "Training acc over epoch:  41 0.9886041\n",
      "Validation loss:  41 0.031792067\n",
      "Validation acc:  41 0.9922391\n",
      "epoch 41: loss = 0.04152857\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.040487584\n",
      "Training acc over epoch:  42 0.9888084\n",
      "Validation loss:  42 0.031575534\n",
      "Validation acc:  42 0.9923275\n",
      "epoch 42: loss = 0.04048759\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.040006187\n",
      "Training acc over epoch:  43 0.98901266\n",
      "Validation loss:  43 0.030858122\n",
      "Validation acc:  43 0.99240124\n",
      "epoch 43: loss = 0.040006187\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.03846551\n",
      "Training acc over epoch:  44 0.9893379\n",
      "Validation loss:  44 0.030447578\n",
      "Validation acc:  44 0.9925118\n",
      "epoch 44: loss = 0.038465507\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.0377201\n",
      "Training acc over epoch:  45 0.9895358\n",
      "Validation loss:  45 0.030185036\n",
      "Validation acc:  45 0.9927624\n",
      "epoch 45: loss = 0.037720095\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.037242547\n",
      "Training acc over epoch:  46 0.98974407\n",
      "Validation loss:  46 0.029512316\n",
      "Validation acc:  46 0.9925855\n",
      "epoch 46: loss = 0.03724254\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.036040395\n",
      "Training acc over epoch:  47 0.989962\n",
      "Validation loss:  47 0.029499969\n",
      "Validation acc:  47 0.9929098\n",
      "epoch 47: loss = 0.036040388\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.03511168\n",
      "Training acc over epoch:  48 0.9901831\n",
      "Validation loss:  48 0.028720241\n",
      "Validation acc:  48 0.9932341\n",
      "epoch 48: loss = 0.035111677\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.034240622\n",
      "Training acc over epoch:  49 0.99048835\n",
      "Validation loss:  49 0.028189616\n",
      "Validation acc:  49 0.9931751\n",
      "epoch 49: loss = 0.03424062\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.033576336\n",
      "Training acc over epoch:  50 0.99059886\n",
      "Validation loss:  50 0.027963527\n",
      "Validation acc:  50 0.99315304\n",
      "epoch 50: loss = 0.033576343\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.032657906\n",
      "Training acc over epoch:  51 0.9908945\n",
      "Validation loss:  51 0.027374454\n",
      "Validation acc:  51 0.9932267\n",
      "epoch 51: loss = 0.03265791\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.031889115\n",
      "Training acc over epoch:  52 0.99101305\n",
      "Validation loss:  52 0.0273416\n",
      "Validation acc:  52 0.9932267\n",
      "epoch 52: loss = 0.03188912\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.031170962\n",
      "Training acc over epoch:  53 0.9913223\n",
      "Validation loss:  53 0.026716622\n",
      "Validation acc:  53 0.99338883\n",
      "epoch 53: loss = 0.031170957\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.030533988\n",
      "Training acc over epoch:  54 0.99138635\n",
      "Validation loss:  54 0.026695099\n",
      "Validation acc:  54 0.99341094\n",
      "epoch 54: loss = 0.030533994\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.029872056\n",
      "Training acc over epoch:  55 0.99149054\n",
      "Validation loss:  55 0.026399693\n",
      "Validation acc:  55 0.99346995\n",
      "epoch 55: loss = 0.029872058\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.02920068\n",
      "Training acc over epoch:  56 0.9916403\n",
      "Validation loss:  56 0.026335133\n",
      "Validation acc:  56 0.9934036\n",
      "epoch 56: loss = 0.029200688\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.028783515\n",
      "Training acc over epoch:  57 0.9917789\n",
      "Validation loss:  57 0.02583279\n",
      "Validation acc:  57 0.99358785\n",
      "epoch 57: loss = 0.028783511\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.028015183\n",
      "Training acc over epoch:  58 0.9919864\n",
      "Validation loss:  58 0.026042989\n",
      "Validation acc:  58 0.9936542\n",
      "epoch 58: loss = 0.028015178\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.02724442\n",
      "Training acc over epoch:  59 0.9921234\n",
      "Validation loss:  59 0.025859859\n",
      "Validation acc:  59 0.9936616\n",
      "epoch 59: loss = 0.027244423\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:22<00:00, 37.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9482: \n",
      "Sensitivity: 0.14617296151402703\n",
      "Specificity: 0.9547322168306585\n",
      "F1-score macro: 0.1791509666315712\n",
      "F1-score micro: 0.9482321072141432\n",
      "Missing rate 0.2\n",
      "=====================================================\n",
      "train_X_fake diff: 507676.0\n",
      "valid_X_fake diff: 56446.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.7619594\n",
      "Training acc over epoch:  0 0.7556615\n",
      "Validation loss:  0 0.29795578\n",
      "Validation acc:  0 0.94966096\n",
      "epoch 0: loss = 1.7619599\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.28419948\n",
      "Training acc over epoch:  1 0.949415\n",
      "Validation loss:  1 0.26374367\n",
      "Validation acc:  1 0.94966096\n",
      "epoch 1: loss = 0.28419945\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.26278943\n",
      "Training acc over epoch:  2 0.94937575\n",
      "Validation loss:  2 0.25175717\n",
      "Validation acc:  2 0.94966096\n",
      "epoch 2: loss = 0.26278946\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.25614962\n",
      "Training acc over epoch:  3 0.9494246\n",
      "Validation loss:  3 0.24744372\n",
      "Validation acc:  3 0.94966096\n",
      "epoch 3: loss = 0.25614962\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.2525989\n",
      "Training acc over epoch:  4 0.94943506\n",
      "Validation loss:  4 0.24452247\n",
      "Validation acc:  4 0.94966096\n",
      "epoch 4: loss = 0.2525989\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.24870898\n",
      "Training acc over epoch:  5 0.94946307\n",
      "Validation loss:  5 0.23861067\n",
      "Validation acc:  5 0.94966096\n",
      "epoch 5: loss = 0.24870901\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.24338745\n",
      "Training acc over epoch:  6 0.94943345\n",
      "Validation loss:  6 0.23425114\n",
      "Validation acc:  6 0.94966096\n",
      "epoch 6: loss = 0.24338745\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.24003293\n",
      "Training acc over epoch:  7 0.949395\n",
      "Validation loss:  7 0.23085514\n",
      "Validation acc:  7 0.94966096\n",
      "epoch 7: loss = 0.24003293\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.23635894\n",
      "Training acc over epoch:  8 0.94942546\n",
      "Validation loss:  8 0.22681262\n",
      "Validation acc:  8 0.94966096\n",
      "epoch 8: loss = 0.23635897\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.23272958\n",
      "Training acc over epoch:  9 0.94944227\n",
      "Validation loss:  9 0.2225115\n",
      "Validation acc:  9 0.94966096\n",
      "epoch 9: loss = 0.23272955\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.22905467\n",
      "Training acc over epoch:  10 0.94945186\n",
      "Validation loss:  10 0.21727614\n",
      "Validation acc:  10 0.94966096\n",
      "epoch 10: loss = 0.22905466\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.22517109\n",
      "Training acc over epoch:  11 0.94944066\n",
      "Validation loss:  11 0.21222003\n",
      "Validation acc:  11 0.94966096\n",
      "epoch 11: loss = 0.22517104\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.21946244\n",
      "Training acc over epoch:  12 0.9496041\n",
      "Validation loss:  12 0.20282993\n",
      "Validation acc:  12 0.9502801\n",
      "epoch 12: loss = 0.21946241\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.21278763\n",
      "Training acc over epoch:  13 0.9499622\n",
      "Validation loss:  13 0.19703476\n",
      "Validation acc:  13 0.95083284\n",
      "epoch 13: loss = 0.21278763\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.20563106\n",
      "Training acc over epoch:  14 0.95028025\n",
      "Validation loss:  14 0.18867074\n",
      "Validation acc:  14 0.9516141\n",
      "epoch 14: loss = 0.20563106\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.194487\n",
      "Training acc over epoch:  15 0.95098925\n",
      "Validation loss:  15 0.17512232\n",
      "Validation acc:  15 0.9522479\n",
      "epoch 15: loss = 0.19448704\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.17772356\n",
      "Training acc over epoch:  16 0.9520723\n",
      "Validation loss:  16 0.15520976\n",
      "Validation acc:  16 0.9547907\n",
      "epoch 16: loss = 0.17772357\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.15381189\n",
      "Training acc over epoch:  17 0.9549828\n",
      "Validation loss:  17 0.12563623\n",
      "Validation acc:  17 0.96010464\n",
      "epoch 17: loss = 0.15381189\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.13510074\n",
      "Training acc over epoch:  18 0.9581344\n",
      "Validation loss:  18 0.11068802\n",
      "Validation acc:  18 0.9641657\n",
      "epoch 18: loss = 0.13510075\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.1220104\n",
      "Training acc over epoch:  19 0.9613613\n",
      "Validation loss:  19 0.100720085\n",
      "Validation acc:  19 0.9683151\n",
      "epoch 19: loss = 0.12201039\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.11130808\n",
      "Training acc over epoch:  20 0.9657306\n",
      "Validation loss:  20 0.09003788\n",
      "Validation acc:  20 0.9730027\n",
      "epoch 20: loss = 0.11130809\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.10271578\n",
      "Training acc over epoch:  21 0.9692988\n",
      "Validation loss:  21 0.081807375\n",
      "Validation acc:  21 0.9759729\n",
      "epoch 21: loss = 0.102715775\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.09511182\n",
      "Training acc over epoch:  22 0.97171974\n",
      "Validation loss:  22 0.07623607\n",
      "Validation acc:  22 0.9777344\n",
      "epoch 22: loss = 0.09511182\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.08912674\n",
      "Training acc over epoch:  23 0.9739757\n",
      "Validation loss:  23 0.07037381\n",
      "Validation acc:  23 0.98034346\n",
      "epoch 23: loss = 0.08912673\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.083419874\n",
      "Training acc over epoch:  24 0.975932\n",
      "Validation loss:  24 0.066391945\n",
      "Validation acc:  24 0.9815301\n",
      "epoch 24: loss = 0.08341989\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.07919239\n",
      "Training acc over epoch:  25 0.97737885\n",
      "Validation loss:  25 0.063792914\n",
      "Validation acc:  25 0.9823187\n",
      "epoch 25: loss = 0.07919239\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.07755577\n",
      "Training acc over epoch:  26 0.9781976\n",
      "Validation loss:  26 0.061974175\n",
      "Validation acc:  26 0.98323995\n",
      "epoch 26: loss = 0.07755577\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.07239255\n",
      "Training acc over epoch:  27 0.9796692\n",
      "Validation loss:  27 0.059090473\n",
      "Validation acc:  27 0.9842497\n",
      "epoch 27: loss = 0.07239257\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.06924221\n",
      "Training acc over epoch:  28 0.98062897\n",
      "Validation loss:  28 0.057888027\n",
      "Validation acc:  28 0.98474354\n",
      "epoch 28: loss = 0.06924222\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.066619724\n",
      "Training acc over epoch:  29 0.9813131\n",
      "Validation loss:  29 0.056283448\n",
      "Validation acc:  29 0.9850973\n",
      "epoch 29: loss = 0.06661971\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.0643764\n",
      "Training acc over epoch:  30 0.98205256\n",
      "Validation loss:  30 0.053744722\n",
      "Validation acc:  30 0.9859596\n",
      "epoch 30: loss = 0.0643764\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.062041286\n",
      "Training acc over epoch:  31 0.9828737\n",
      "Validation loss:  31 0.05303403\n",
      "Validation acc:  31 0.98620284\n",
      "epoch 31: loss = 0.062041283\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.060231447\n",
      "Training acc over epoch:  32 0.98327106\n",
      "Validation loss:  32 0.05253807\n",
      "Validation acc:  32 0.9863134\n",
      "epoch 32: loss = 0.060231447\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.05875477\n",
      "Training acc over epoch:  33 0.98362356\n",
      "Validation loss:  33 0.051156837\n",
      "Validation acc:  33 0.986903\n",
      "epoch 33: loss = 0.05875476\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.056949116\n",
      "Training acc over epoch:  34 0.98420113\n",
      "Validation loss:  34 0.050576795\n",
      "Validation acc:  34 0.98682195\n",
      "epoch 34: loss = 0.056949116\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.05544371\n",
      "Training acc over epoch:  35 0.9845256\n",
      "Validation loss:  35 0.049421858\n",
      "Validation acc:  35 0.98725677\n",
      "epoch 35: loss = 0.05544371\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.05429461\n",
      "Training acc over epoch:  36 0.98499185\n",
      "Validation loss:  36 0.049207386\n",
      "Validation acc:  36 0.987161\n",
      "epoch 36: loss = 0.054294612\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.05279645\n",
      "Training acc over epoch:  37 0.9853051\n",
      "Validation loss:  37 0.048425272\n",
      "Validation acc:  37 0.9875811\n",
      "epoch 37: loss = 0.052796442\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.05176983\n",
      "Training acc over epoch:  38 0.9854597\n",
      "Validation loss:  38 0.047773585\n",
      "Validation acc:  38 0.98740417\n",
      "epoch 38: loss = 0.051769838\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.049822513\n",
      "Training acc over epoch:  39 0.9859652\n",
      "Validation loss:  39 0.047571745\n",
      "Validation acc:  39 0.9877874\n",
      "epoch 39: loss = 0.04982252\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.04906764\n",
      "Training acc over epoch:  40 0.986107\n",
      "Validation loss:  40 0.047449287\n",
      "Validation acc:  40 0.98755896\n",
      "epoch 40: loss = 0.04906763\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.047963895\n",
      "Training acc over epoch:  41 0.9864475\n",
      "Validation loss:  41 0.047091704\n",
      "Validation acc:  41 0.9875737\n",
      "epoch 41: loss = 0.047963895\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.04703869\n",
      "Training acc over epoch:  42 0.9866742\n",
      "Validation loss:  42 0.047228105\n",
      "Validation acc:  42 0.98758847\n",
      "epoch 42: loss = 0.047038697\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.0458077\n",
      "Training acc over epoch:  43 0.9868232\n",
      "Validation loss:  43 0.04665913\n",
      "Validation acc:  43 0.98811173\n",
      "epoch 43: loss = 0.045807704\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.04490515\n",
      "Training acc over epoch:  44 0.98709\n",
      "Validation loss:  44 0.046017278\n",
      "Validation acc:  44 0.9880896\n",
      "epoch 44: loss = 0.044905156\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.043952197\n",
      "Training acc over epoch:  45 0.9873071\n",
      "Validation loss:  45 0.046189047\n",
      "Validation acc:  45 0.9877432\n",
      "epoch 45: loss = 0.043952204\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.042783722\n",
      "Training acc over epoch:  46 0.98748654\n",
      "Validation loss:  46 0.04594541\n",
      "Validation acc:  46 0.98786116\n",
      "epoch 46: loss = 0.042783722\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.042074073\n",
      "Training acc over epoch:  47 0.98770523\n",
      "Validation loss:  47 0.04557015\n",
      "Validation acc:  47 0.988097\n",
      "epoch 47: loss = 0.042074073\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.041196827\n",
      "Training acc over epoch:  48 0.9878262\n",
      "Validation loss:  48 0.045535933\n",
      "Validation acc:  48 0.988237\n",
      "epoch 48: loss = 0.04119682\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.039846715\n",
      "Training acc over epoch:  49 0.9881234\n",
      "Validation loss:  49 0.04563751\n",
      "Validation acc:  49 0.9880307\n",
      "epoch 49: loss = 0.039846722\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.038711186\n",
      "Training acc over epoch:  50 0.9884086\n",
      "Validation loss:  50 0.046158742\n",
      "Validation acc:  50 0.9885318\n",
      "epoch 50: loss = 0.038711186\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.038503155\n",
      "Training acc over epoch:  51 0.9883774\n",
      "Validation loss:  51 0.046154786\n",
      "Validation acc:  51 0.9880233\n",
      "epoch 51: loss = 0.03850316\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.03721917\n",
      "Training acc over epoch:  52 0.98869544\n",
      "Validation loss:  52 0.04586312\n",
      "Validation acc:  52 0.98795694\n",
      "epoch 52: loss = 0.037219167\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.036765553\n",
      "Training acc over epoch:  53 0.9888124\n",
      "Validation loss:  53 0.046564706\n",
      "Validation acc:  53 0.98766214\n",
      "epoch 53: loss = 0.03676556\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.03579212\n",
      "Training acc over epoch:  54 0.988943\n",
      "Validation loss:  54 0.04596883\n",
      "Validation acc:  54 0.98815596\n",
      "epoch 54: loss = 0.03579212\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.035200503\n",
      "Training acc over epoch:  55 0.98900867\n",
      "Validation loss:  55 0.046370685\n",
      "Validation acc:  55 0.98822963\n",
      "epoch 55: loss = 0.035200506\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.03412801\n",
      "Training acc over epoch:  56 0.9893099\n",
      "Validation loss:  56 0.047071375\n",
      "Validation acc:  56 0.9878685\n",
      "epoch 56: loss = 0.034128007\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.03339782\n",
      "Training acc over epoch:  57 0.9894805\n",
      "Validation loss:  57 0.047145963\n",
      "Validation acc:  57 0.9882665\n",
      "epoch 57: loss = 0.033397824\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.032849684\n",
      "Training acc over epoch:  58 0.9896039\n",
      "Validation loss:  58 0.047495894\n",
      "Validation acc:  58 0.9880528\n",
      "epoch 58: loss = 0.03284968\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.032499623\n",
      "Training acc over epoch:  59 0.9897641\n",
      "Validation loss:  59 0.047337968\n",
      "Validation acc:  59 0.9879201\n",
      "epoch 59: loss = 0.032499623\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:21<00:00, 38.90it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.2 missing genotypes is 0.9515: \n",
      "Sensitivity: nan\n",
      "Specificity: 0.966913904386967\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-53-ab2fe261998b>:183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  TPR = TP/(TP+FN)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1-score macro: 0.12778213003467592\n",
      "F1-score micro: 0.9514863763597066\n",
      "Training using fold 2\n",
      "*******************************************\n",
      "*******************************************\n",
      "Missing rate 0.05\n",
      "=====================================================\n",
      "train_X_fake diff: 126168.0\n",
      "valid_X_fake diff: 14028.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 0.8638097\n",
      "Training acc over epoch:  0 0.8482202\n",
      "Validation loss:  0 0.28270435\n",
      "Validation acc:  0 0.94892395\n",
      "epoch 0: loss = 0.86380965\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.2757894\n",
      "Training acc over epoch:  1 0.94929326\n",
      "Validation loss:  1 0.2665817\n",
      "Validation acc:  1 0.94892395\n",
      "epoch 1: loss = 0.27578935\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.2648531\n",
      "Training acc over epoch:  2 0.94923156\n",
      "Validation loss:  2 0.26128888\n",
      "Validation acc:  2 0.94892395\n",
      "epoch 2: loss = 0.2648531\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.25511244\n",
      "Training acc over epoch:  3 0.9492909\n",
      "Validation loss:  3 0.25156724\n",
      "Validation acc:  3 0.94892395\n",
      "epoch 3: loss = 0.2551124\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.24829072\n",
      "Training acc over epoch:  4 0.9492532\n",
      "Validation loss:  4 0.24466601\n",
      "Validation acc:  4 0.94892395\n",
      "epoch 4: loss = 0.24829075\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.24169955\n",
      "Training acc over epoch:  5 0.9492716\n",
      "Validation loss:  5 0.23792186\n",
      "Validation acc:  5 0.94892395\n",
      "epoch 5: loss = 0.24169953\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.23607302\n",
      "Training acc over epoch:  6 0.9492812\n",
      "Validation loss:  6 0.23179932\n",
      "Validation acc:  6 0.94892395\n",
      "epoch 6: loss = 0.23607302\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.23074658\n",
      "Training acc over epoch:  7 0.94928527\n",
      "Validation loss:  7 0.22597207\n",
      "Validation acc:  7 0.94892395\n",
      "epoch 7: loss = 0.23074658\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.22621197\n",
      "Training acc over epoch:  8 0.94927\n",
      "Validation loss:  8 0.22336026\n",
      "Validation acc:  8 0.94892395\n",
      "epoch 8: loss = 0.22621192\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.2190568\n",
      "Training acc over epoch:  9 0.9492508\n",
      "Validation loss:  9 0.2077053\n",
      "Validation acc:  9 0.94892395\n",
      "epoch 9: loss = 0.2190568\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.20894764\n",
      "Training acc over epoch:  10 0.94940543\n",
      "Validation loss:  10 0.1998189\n",
      "Validation acc:  10 0.94938827\n",
      "epoch 10: loss = 0.20894767\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.20041433\n",
      "Training acc over epoch:  11 0.9499117\n",
      "Validation loss:  11 0.18751667\n",
      "Validation acc:  11 0.9507444\n",
      "epoch 11: loss = 0.20041433\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.19144902\n",
      "Training acc over epoch:  12 0.95072484\n",
      "Validation loss:  12 0.17741808\n",
      "Validation acc:  12 0.9517689\n",
      "epoch 12: loss = 0.19144908\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.18184161\n",
      "Training acc over epoch:  13 0.95129764\n",
      "Validation loss:  13 0.16873951\n",
      "Validation acc:  13 0.95244694\n",
      "epoch 13: loss = 0.1818416\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.17422755\n",
      "Training acc over epoch:  14 0.95172626\n",
      "Validation loss:  14 0.1618068\n",
      "Validation acc:  14 0.9530292\n",
      "epoch 14: loss = 0.17422757\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.16552472\n",
      "Training acc over epoch:  15 0.95236874\n",
      "Validation loss:  15 0.15327305\n",
      "Validation acc:  15 0.9540389\n",
      "epoch 15: loss = 0.1655247\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.1572636\n",
      "Training acc over epoch:  16 0.95327\n",
      "Validation loss:  16 0.1403111\n",
      "Validation acc:  16 0.95538765\n",
      "epoch 16: loss = 0.1572636\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.14801121\n",
      "Training acc over epoch:  17 0.9545318\n",
      "Validation loss:  17 0.13075063\n",
      "Validation acc:  17 0.957245\n",
      "epoch 17: loss = 0.14801121\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.1404281\n",
      "Training acc over epoch:  18 0.95616126\n",
      "Validation loss:  18 0.1301578\n",
      "Validation acc:  18 0.9616524\n",
      "epoch 18: loss = 0.14042813\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.13239425\n",
      "Training acc over epoch:  19 0.95825696\n",
      "Validation loss:  19 0.11626657\n",
      "Validation acc:  19 0.96248525\n",
      "epoch 19: loss = 0.13239424\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.12586804\n",
      "Training acc over epoch:  20 0.959715\n",
      "Validation loss:  20 0.10934836\n",
      "Validation acc:  20 0.96502066\n",
      "epoch 20: loss = 0.12586805\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.120336905\n",
      "Training acc over epoch:  21 0.9613461\n",
      "Validation loss:  21 0.10615961\n",
      "Validation acc:  21 0.9678582\n",
      "epoch 21: loss = 0.12033691\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.1138093\n",
      "Training acc over epoch:  22 0.96320546\n",
      "Validation loss:  22 0.09758303\n",
      "Validation acc:  22 0.9697892\n",
      "epoch 22: loss = 0.11380934\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.10757654\n",
      "Training acc over epoch:  23 0.965197\n",
      "Validation loss:  23 0.09032006\n",
      "Validation acc:  23 0.97118956\n",
      "epoch 23: loss = 0.10757653\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.10256758\n",
      "Training acc over epoch:  24 0.9666847\n",
      "Validation loss:  24 0.085284375\n",
      "Validation acc:  24 0.9729142\n",
      "epoch 24: loss = 0.1025676\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.09757188\n",
      "Training acc over epoch:  25 0.96830857\n",
      "Validation loss:  25 0.081553794\n",
      "Validation acc:  25 0.9748305\n",
      "epoch 25: loss = 0.09757188\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.092824645\n",
      "Training acc over epoch:  26 0.969866\n",
      "Validation loss:  26 0.077312216\n",
      "Validation acc:  26 0.97632664\n",
      "epoch 26: loss = 0.09282464\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.089224055\n",
      "Training acc over epoch:  27 0.97113895\n",
      "Validation loss:  27 0.073602974\n",
      "Validation acc:  27 0.9772922\n",
      "epoch 27: loss = 0.089224055\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.08579322\n",
      "Training acc over epoch:  28 0.972202\n",
      "Validation loss:  28 0.070427775\n",
      "Validation acc:  28 0.9786114\n",
      "epoch 28: loss = 0.08579322\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.08257109\n",
      "Training acc over epoch:  29 0.97322744\n",
      "Validation loss:  29 0.0720911\n",
      "Validation acc:  29 0.9792453\n",
      "epoch 29: loss = 0.08257109\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.081022814\n",
      "Training acc over epoch:  30 0.9738371\n",
      "Validation loss:  30 0.06791117\n",
      "Validation acc:  30 0.97957695\n",
      "epoch 30: loss = 0.08102279\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.077925205\n",
      "Training acc over epoch:  31 0.9748257\n",
      "Validation loss:  31 0.06358163\n",
      "Validation acc:  31 0.98103625\n",
      "epoch 31: loss = 0.0779252\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.07562642\n",
      "Training acc over epoch:  32 0.9755867\n",
      "Validation loss:  32 0.063312314\n",
      "Validation acc:  32 0.98122054\n",
      "epoch 32: loss = 0.07562642\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.07337874\n",
      "Training acc over epoch:  33 0.97659534\n",
      "Validation loss:  33 0.060469054\n",
      "Validation acc:  33 0.98223025\n",
      "epoch 33: loss = 0.073378734\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.071098156\n",
      "Training acc over epoch:  34 0.9771994\n",
      "Validation loss:  34 0.05879734\n",
      "Validation acc:  34 0.98265773\n",
      "epoch 34: loss = 0.07109814\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.06949936\n",
      "Training acc over epoch:  35 0.9777225\n",
      "Validation loss:  35 0.057977423\n",
      "Validation acc:  35 0.9833726\n",
      "epoch 35: loss = 0.069499366\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.06795379\n",
      "Training acc over epoch:  36 0.97831213\n",
      "Validation loss:  36 0.05644291\n",
      "Validation acc:  36 0.983579\n",
      "epoch 36: loss = 0.067953795\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.06627518\n",
      "Training acc over epoch:  37 0.9787864\n",
      "Validation loss:  37 0.055548\n",
      "Validation acc:  37 0.98385906\n",
      "epoch 37: loss = 0.06627518\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.065033615\n",
      "Training acc over epoch:  38 0.97938323\n",
      "Validation loss:  38 0.055260684\n",
      "Validation acc:  38 0.9841981\n",
      "epoch 38: loss = 0.06503361\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.06309932\n",
      "Training acc over epoch:  39 0.98009944\n",
      "Validation loss:  39 0.05140642\n",
      "Validation acc:  39 0.9849646\n",
      "epoch 39: loss = 0.063099325\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.0608604\n",
      "Training acc over epoch:  40 0.9807844\n",
      "Validation loss:  40 0.05167602\n",
      "Validation acc:  40 0.98562795\n",
      "epoch 40: loss = 0.060860395\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.059963875\n",
      "Training acc over epoch:  41 0.981068\n",
      "Validation loss:  41 0.050081715\n",
      "Validation acc:  41 0.98607016\n",
      "epoch 41: loss = 0.059963875\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.05816551\n",
      "Training acc over epoch:  42 0.9817497\n",
      "Validation loss:  42 0.04835423\n",
      "Validation acc:  42 0.986763\n",
      "epoch 42: loss = 0.05816551\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.056169048\n",
      "Training acc over epoch:  43 0.9823209\n",
      "Validation loss:  43 0.04531874\n",
      "Validation acc:  43 0.9870357\n",
      "epoch 43: loss = 0.056169037\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.05522052\n",
      "Training acc over epoch:  44 0.9827087\n",
      "Validation loss:  44 0.04589189\n",
      "Validation acc:  44 0.9873305\n",
      "epoch 44: loss = 0.055220522\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.0532919\n",
      "Training acc over epoch:  45 0.9835466\n",
      "Validation loss:  45 0.04301585\n",
      "Validation acc:  45 0.98800856\n",
      "epoch 45: loss = 0.053291902\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.051355347\n",
      "Training acc over epoch:  46 0.9841379\n",
      "Validation loss:  46 0.041192435\n",
      "Validation acc:  46 0.9885466\n",
      "epoch 46: loss = 0.05135534\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.05063151\n",
      "Training acc over epoch:  47 0.98452723\n",
      "Validation loss:  47 0.04048792\n",
      "Validation acc:  47 0.988893\n",
      "epoch 47: loss = 0.050631512\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.048320867\n",
      "Training acc over epoch:  48 0.98531073\n",
      "Validation loss:  48 0.03863076\n",
      "Validation acc:  48 0.98944575\n",
      "epoch 48: loss = 0.048320867\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.046961747\n",
      "Training acc over epoch:  49 0.98580337\n",
      "Validation loss:  49 0.03720283\n",
      "Validation acc:  49 0.98991007\n",
      "epoch 49: loss = 0.046961743\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.045404278\n",
      "Training acc over epoch:  50 0.98620796\n",
      "Validation loss:  50 0.036630057\n",
      "Validation acc:  50 0.99021965\n",
      "epoch 50: loss = 0.045404267\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.04441235\n",
      "Training acc over epoch:  51 0.98662776\n",
      "Validation loss:  51 0.035484515\n",
      "Validation acc:  51 0.9905587\n",
      "epoch 51: loss = 0.04441235\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.042499572\n",
      "Training acc over epoch:  52 0.9873287\n",
      "Validation loss:  52 0.033960868\n",
      "Validation acc:  52 0.99106723\n",
      "epoch 52: loss = 0.042499572\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.041412685\n",
      "Training acc over epoch:  53 0.9876251\n",
      "Validation loss:  53 0.03261464\n",
      "Validation acc:  53 0.9914431\n",
      "epoch 53: loss = 0.041412685\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.039955776\n",
      "Training acc over epoch:  54 0.9881779\n",
      "Validation loss:  54 0.031172138\n",
      "Validation acc:  54 0.9917674\n",
      "epoch 54: loss = 0.039955776\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.03855186\n",
      "Training acc over epoch:  55 0.98850876\n",
      "Validation loss:  55 0.029932642\n",
      "Validation acc:  55 0.9921654\n",
      "epoch 55: loss = 0.038551856\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.03719089\n",
      "Training acc over epoch:  56 0.9889614\n",
      "Validation loss:  56 0.029675243\n",
      "Validation acc:  56 0.99227595\n",
      "epoch 56: loss = 0.03719089\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.03605191\n",
      "Training acc over epoch:  57 0.98934996\n",
      "Validation loss:  57 0.029086854\n",
      "Validation acc:  57 0.9926444\n",
      "epoch 57: loss = 0.036051907\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.03504337\n",
      "Training acc over epoch:  58 0.98976094\n",
      "Validation loss:  58 0.027268251\n",
      "Validation acc:  58 0.9928066\n",
      "epoch 58: loss = 0.035043377\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.03443352\n",
      "Training acc over epoch:  59 0.98993635\n",
      "Validation loss:  59 0.026731418\n",
      "Validation acc:  59 0.9931309\n",
      "epoch 59: loss = 0.03443352\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:22<00:00, 37.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.05 missing genotypes is 0.9561: \n",
      "Sensitivity: 0.19739795931139065\n",
      "Specificity: 0.9185293203079288\n",
      "F1-score macro: 0.2418233109384375\n",
      "F1-score micro: 0.9561163387510693\n",
      "Missing rate 0.1\n",
      "=====================================================\n",
      "train_X_fake diff: 252336.0\n",
      "valid_X_fake diff: 28056.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.2120343\n",
      "Training acc over epoch:  0 0.79038197\n",
      "Validation loss:  0 0.29489923\n",
      "Validation acc:  0 0.94892395\n",
      "epoch 0: loss = 1.2120343\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.28435197\n",
      "Training acc over epoch:  1 0.949246\n",
      "Validation loss:  1 0.27274945\n",
      "Validation acc:  1 0.94892395\n",
      "epoch 1: loss = 0.28435192\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.27042615\n",
      "Training acc over epoch:  2 0.9492676\n",
      "Validation loss:  2 0.26408282\n",
      "Validation acc:  2 0.94892395\n",
      "epoch 2: loss = 0.27042618\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.2608886\n",
      "Training acc over epoch:  3 0.9492796\n",
      "Validation loss:  3 0.2538816\n",
      "Validation acc:  3 0.94892395\n",
      "epoch 3: loss = 0.2608886\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.25253448\n",
      "Training acc over epoch:  4 0.9492348\n",
      "Validation loss:  4 0.24780153\n",
      "Validation acc:  4 0.94892395\n",
      "epoch 4: loss = 0.25253448\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.24643798\n",
      "Training acc over epoch:  5 0.949262\n",
      "Validation loss:  5 0.24120457\n",
      "Validation acc:  5 0.94892395\n",
      "epoch 5: loss = 0.24643797\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.24071279\n",
      "Training acc over epoch:  6 0.94926524\n",
      "Validation loss:  6 0.23673314\n",
      "Validation acc:  6 0.94892395\n",
      "epoch 6: loss = 0.24071279\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.2372135\n",
      "Training acc over epoch:  7 0.94926924\n",
      "Validation loss:  7 0.23416963\n",
      "Validation acc:  7 0.94892395\n",
      "epoch 7: loss = 0.23721355\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.23451158\n",
      "Training acc over epoch:  8 0.949266\n",
      "Validation loss:  8 0.2310262\n",
      "Validation acc:  8 0.94892395\n",
      "epoch 8: loss = 0.23451164\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.23196459\n",
      "Training acc over epoch:  9 0.949258\n",
      "Validation loss:  9 0.22828443\n",
      "Validation acc:  9 0.94892395\n",
      "epoch 9: loss = 0.23196459\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.22937895\n",
      "Training acc over epoch:  10 0.94930047\n",
      "Validation loss:  10 0.22625287\n",
      "Validation acc:  10 0.94892395\n",
      "epoch 10: loss = 0.22937897\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.22637756\n",
      "Training acc over epoch:  11 0.949266\n",
      "Validation loss:  11 0.22148535\n",
      "Validation acc:  11 0.94892395\n",
      "epoch 11: loss = 0.22637758\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.2218646\n",
      "Training acc over epoch:  12 0.94931406\n",
      "Validation loss:  12 0.21430497\n",
      "Validation acc:  12 0.94893867\n",
      "epoch 12: loss = 0.2218646\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.21390386\n",
      "Training acc over epoch:  13 0.9493998\n",
      "Validation loss:  13 0.20339228\n",
      "Validation acc:  13 0.94901973\n",
      "epoch 13: loss = 0.21390386\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.20530988\n",
      "Training acc over epoch:  14 0.9498332\n",
      "Validation loss:  14 0.19400688\n",
      "Validation acc:  14 0.9503243\n",
      "epoch 14: loss = 0.20530991\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.19452986\n",
      "Training acc over epoch:  15 0.95045966\n",
      "Validation loss:  15 0.17942813\n",
      "Validation acc:  15 0.9517689\n",
      "epoch 15: loss = 0.19452989\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.18233019\n",
      "Training acc over epoch:  16 0.9511911\n",
      "Validation loss:  16 0.16576828\n",
      "Validation acc:  16 0.95247644\n",
      "epoch 16: loss = 0.18233019\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.17129515\n",
      "Training acc over epoch:  17 0.95209396\n",
      "Validation loss:  17 0.15489048\n",
      "Validation acc:  17 0.953921\n",
      "epoch 17: loss = 0.17129518\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.1616694\n",
      "Training acc over epoch:  18 0.9527781\n",
      "Validation loss:  18 0.14355987\n",
      "Validation acc:  18 0.9545622\n",
      "epoch 18: loss = 0.16166939\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.15255484\n",
      "Training acc over epoch:  19 0.9539349\n",
      "Validation loss:  19 0.13487269\n",
      "Validation acc:  19 0.9557341\n",
      "epoch 19: loss = 0.15255484\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.14388062\n",
      "Training acc over epoch:  20 0.95558846\n",
      "Validation loss:  20 0.12511596\n",
      "Validation acc:  20 0.9608196\n",
      "epoch 20: loss = 0.14388062\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.13635485\n",
      "Training acc over epoch:  21 0.9573525\n",
      "Validation loss:  21 0.11784973\n",
      "Validation acc:  21 0.96350974\n",
      "epoch 21: loss = 0.13635483\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.13048624\n",
      "Training acc over epoch:  22 0.95896035\n",
      "Validation loss:  22 0.11001911\n",
      "Validation acc:  22 0.96603036\n",
      "epoch 22: loss = 0.13048624\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.12505165\n",
      "Training acc over epoch:  23 0.9605658\n",
      "Validation loss:  23 0.10577759\n",
      "Validation acc:  23 0.9668706\n",
      "epoch 23: loss = 0.12505165\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.12059288\n",
      "Training acc over epoch:  24 0.9620166\n",
      "Validation loss:  24 0.101269364\n",
      "Validation acc:  24 0.9692954\n",
      "epoch 24: loss = 0.12059288\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.11622758\n",
      "Training acc over epoch:  25 0.9634979\n",
      "Validation loss:  25 0.097667485\n",
      "Validation acc:  25 0.97149915\n",
      "epoch 25: loss = 0.116227575\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.1130118\n",
      "Training acc over epoch:  26 0.9647436\n",
      "Validation loss:  26 0.09452966\n",
      "Validation acc:  26 0.97368807\n",
      "epoch 26: loss = 0.11301181\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.10841047\n",
      "Training acc over epoch:  27 0.9663394\n",
      "Validation loss:  27 0.088906534\n",
      "Validation acc:  27 0.97503686\n",
      "epoch 27: loss = 0.10841047\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.10270791\n",
      "Training acc over epoch:  28 0.9682549\n",
      "Validation loss:  28 0.08423139\n",
      "Validation acc:  28 0.97676885\n",
      "epoch 28: loss = 0.102707915\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.09863567\n",
      "Training acc over epoch:  29 0.96980745\n",
      "Validation loss:  29 0.078918844\n",
      "Validation acc:  29 0.9777712\n",
      "epoch 29: loss = 0.09863568\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.09403007\n",
      "Training acc over epoch:  30 0.9714025\n",
      "Validation loss:  30 0.07566059\n",
      "Validation acc:  30 0.9786409\n",
      "epoch 30: loss = 0.09403006\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.09100101\n",
      "Training acc over epoch:  31 0.9725345\n",
      "Validation loss:  31 0.07320919\n",
      "Validation acc:  31 0.97943693\n",
      "epoch 31: loss = 0.091001004\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.0870654\n",
      "Training acc over epoch:  32 0.9738139\n",
      "Validation loss:  32 0.069358364\n",
      "Validation acc:  32 0.9808373\n",
      "epoch 32: loss = 0.087065406\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.08414571\n",
      "Training acc over epoch:  33 0.9747992\n",
      "Validation loss:  33 0.06705756\n",
      "Validation acc:  33 0.98178804\n",
      "epoch 33: loss = 0.08414571\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.08088647\n",
      "Training acc over epoch:  34 0.9759168\n",
      "Validation loss:  34 0.064685464\n",
      "Validation acc:  34 0.98251766\n",
      "epoch 34: loss = 0.08088646\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.078336045\n",
      "Training acc over epoch:  35 0.9766819\n",
      "Validation loss:  35 0.063977115\n",
      "Validation acc:  35 0.98326945\n",
      "epoch 35: loss = 0.078336045\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.075734705\n",
      "Training acc over epoch:  36 0.9776328\n",
      "Validation loss:  36 0.060701232\n",
      "Validation acc:  36 0.9839401\n",
      "epoch 36: loss = 0.07573472\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.07383898\n",
      "Training acc over epoch:  37 0.9784267\n",
      "Validation loss:  37 0.0587112\n",
      "Validation acc:  37 0.9845445\n",
      "epoch 37: loss = 0.07383897\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.070739776\n",
      "Training acc over epoch:  38 0.9794385\n",
      "Validation loss:  38 0.057625685\n",
      "Validation acc:  38 0.9851341\n",
      "epoch 38: loss = 0.07073978\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.06842604\n",
      "Training acc over epoch:  39 0.9801171\n",
      "Validation loss:  39 0.05550165\n",
      "Validation acc:  39 0.9856722\n",
      "epoch 39: loss = 0.06842606\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.06632382\n",
      "Training acc over epoch:  40 0.9808004\n",
      "Validation loss:  40 0.05464828\n",
      "Validation acc:  40 0.9858859\n",
      "epoch 40: loss = 0.06632382\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.064652756\n",
      "Training acc over epoch:  41 0.9814758\n",
      "Validation loss:  41 0.05275309\n",
      "Validation acc:  41 0.9866819\n",
      "epoch 41: loss = 0.06465277\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.062350474\n",
      "Training acc over epoch:  42 0.98220396\n",
      "Validation loss:  42 0.05072727\n",
      "Validation acc:  42 0.9871978\n",
      "epoch 42: loss = 0.062350463\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.060435243\n",
      "Training acc over epoch:  43 0.9830091\n",
      "Validation loss:  43 0.049276486\n",
      "Validation acc:  43 0.9874558\n",
      "epoch 43: loss = 0.060435243\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.058316305\n",
      "Training acc over epoch:  44 0.9835162\n",
      "Validation loss:  44 0.04898454\n",
      "Validation acc:  44 0.9879275\n",
      "epoch 44: loss = 0.058316313\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.0570233\n",
      "Training acc over epoch:  45 0.98393357\n",
      "Validation loss:  45 0.0468448\n",
      "Validation acc:  45 0.988495\n",
      "epoch 45: loss = 0.0570233\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.05534466\n",
      "Training acc over epoch:  46 0.9845729\n",
      "Validation loss:  46 0.045239076\n",
      "Validation acc:  46 0.98914355\n",
      "epoch 46: loss = 0.055344664\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.053723924\n",
      "Training acc over epoch:  47 0.9850856\n",
      "Validation loss:  47 0.044234782\n",
      "Validation acc:  47 0.9893942\n",
      "epoch 47: loss = 0.05372392\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.052231252\n",
      "Training acc over epoch:  48 0.98563915\n",
      "Validation loss:  48 0.043297727\n",
      "Validation acc:  48 0.98971844\n",
      "epoch 48: loss = 0.052231256\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.05072539\n",
      "Training acc over epoch:  49 0.9861134\n",
      "Validation loss:  49 0.04205158\n",
      "Validation acc:  49 0.98991746\n",
      "epoch 49: loss = 0.050725393\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.04938767\n",
      "Training acc over epoch:  50 0.98642343\n",
      "Validation loss:  50 0.042116575\n",
      "Validation acc:  50 0.99019015\n",
      "epoch 50: loss = 0.04938767\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.04786282\n",
      "Training acc over epoch:  51 0.9868825\n",
      "Validation loss:  51 0.040307302\n",
      "Validation acc:  51 0.99064714\n",
      "epoch 51: loss = 0.04786282\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.046781298\n",
      "Training acc over epoch:  52 0.9871837\n",
      "Validation loss:  52 0.039585494\n",
      "Validation acc:  52 0.9908608\n",
      "epoch 52: loss = 0.0467813\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.04546108\n",
      "Training acc over epoch:  53 0.98770124\n",
      "Validation loss:  53 0.03874282\n",
      "Validation acc:  53 0.99095666\n",
      "epoch 53: loss = 0.04546107\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.04416437\n",
      "Training acc over epoch:  54 0.9878855\n",
      "Validation loss:  54 0.038299464\n",
      "Validation acc:  54 0.9908903\n",
      "epoch 54: loss = 0.04416437\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.04308897\n",
      "Training acc over epoch:  55 0.98828363\n",
      "Validation loss:  55 0.037239738\n",
      "Validation acc:  55 0.9914136\n",
      "epoch 55: loss = 0.04308897\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.04241306\n",
      "Training acc over epoch:  56 0.9884447\n",
      "Validation loss:  56 0.036764476\n",
      "Validation acc:  56 0.9913178\n",
      "epoch 56: loss = 0.04241306\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.041538287\n",
      "Training acc over epoch:  57 0.9886722\n",
      "Validation loss:  57 0.036693353\n",
      "Validation acc:  57 0.9914652\n",
      "epoch 57: loss = 0.041538287\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.040726833\n",
      "Training acc over epoch:  58 0.9889606\n",
      "Validation loss:  58 0.03561139\n",
      "Validation acc:  58 0.99182636\n",
      "epoch 58: loss = 0.04072683\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.039289963\n",
      "Training acc over epoch:  59 0.989378\n",
      "Validation loss:  59 0.0348532\n",
      "Validation acc:  59 0.99185586\n",
      "epoch 59: loss = 0.039289955\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:21<00:00, 38.90it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9487: \n",
      "Sensitivity: 0.1476110833146169\n",
      "Specificity: 0.9426710653412395\n",
      "F1-score macro: 0.17977881142261007\n",
      "F1-score micro: 0.9486740804106074\n",
      "Missing rate 0.2\n",
      "=====================================================\n",
      "train_X_fake diff: 507676.0\n",
      "valid_X_fake diff: 56446.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.2024674\n",
      "Training acc over epoch:  0 0.79395014\n",
      "Validation loss:  0 0.2936563\n",
      "Validation acc:  0 0.94892395\n",
      "epoch 0: loss = 1.2024677\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.2903837\n",
      "Training acc over epoch:  1 0.9492756\n",
      "Validation loss:  1 0.28079984\n",
      "Validation acc:  1 0.94892395\n",
      "epoch 1: loss = 0.29038373\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.28168255\n",
      "Training acc over epoch:  2 0.94923717\n",
      "Validation loss:  2 0.2743265\n",
      "Validation acc:  2 0.94892395\n",
      "epoch 2: loss = 0.28168252\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.27376425\n",
      "Training acc over epoch:  3 0.94926524\n",
      "Validation loss:  3 0.26622128\n",
      "Validation acc:  3 0.94892395\n",
      "epoch 3: loss = 0.27376422\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.2639506\n",
      "Training acc over epoch:  4 0.9492644\n",
      "Validation loss:  4 0.253617\n",
      "Validation acc:  4 0.94892395\n",
      "epoch 4: loss = 0.2639506\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.25121891\n",
      "Training acc over epoch:  5 0.9492588\n",
      "Validation loss:  5 0.24351723\n",
      "Validation acc:  5 0.94892395\n",
      "epoch 5: loss = 0.25121891\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.2442154\n",
      "Training acc over epoch:  6 0.9492884\n",
      "Validation loss:  6 0.2398901\n",
      "Validation acc:  6 0.94892395\n",
      "epoch 6: loss = 0.24421538\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.24144739\n",
      "Training acc over epoch:  7 0.94929963\n",
      "Validation loss:  7 0.2377713\n",
      "Validation acc:  7 0.94892395\n",
      "epoch 7: loss = 0.24144736\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.23964931\n",
      "Training acc over epoch:  8 0.9492868\n",
      "Validation loss:  8 0.23621197\n",
      "Validation acc:  8 0.94892395\n",
      "epoch 8: loss = 0.23964931\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.23839824\n",
      "Training acc over epoch:  9 0.94922596\n",
      "Validation loss:  9 0.23471047\n",
      "Validation acc:  9 0.94892395\n",
      "epoch 9: loss = 0.23839827\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.23650257\n",
      "Training acc over epoch:  10 0.9492588\n",
      "Validation loss:  10 0.23262364\n",
      "Validation acc:  10 0.94892395\n",
      "epoch 10: loss = 0.23650248\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.23426084\n",
      "Training acc over epoch:  11 0.94928443\n",
      "Validation loss:  11 0.22981587\n",
      "Validation acc:  11 0.94892395\n",
      "epoch 11: loss = 0.23426084\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.23121347\n",
      "Training acc over epoch:  12 0.9492588\n",
      "Validation loss:  12 0.22637482\n",
      "Validation acc:  12 0.94892395\n",
      "epoch 12: loss = 0.23121352\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.22804707\n",
      "Training acc over epoch:  13 0.9492516\n",
      "Validation loss:  13 0.22308603\n",
      "Validation acc:  13 0.94892395\n",
      "epoch 13: loss = 0.22804706\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.22441217\n",
      "Training acc over epoch:  14 0.94930047\n",
      "Validation loss:  14 0.21716802\n",
      "Validation acc:  14 0.9489829\n",
      "epoch 14: loss = 0.22441219\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.21875162\n",
      "Training acc over epoch:  15 0.9495328\n",
      "Validation loss:  15 0.21151832\n",
      "Validation acc:  15 0.9497789\n",
      "epoch 15: loss = 0.21875165\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.21300115\n",
      "Training acc over epoch:  16 0.9497435\n",
      "Validation loss:  16 0.20490952\n",
      "Validation acc:  16 0.94993365\n",
      "epoch 16: loss = 0.21300119\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.20726967\n",
      "Training acc over epoch:  17 0.9500014\n",
      "Validation loss:  17 0.19866267\n",
      "Validation acc:  17 0.9503317\n",
      "epoch 17: loss = 0.20726968\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.20093426\n",
      "Training acc over epoch:  18 0.95043004\n",
      "Validation loss:  18 0.19338168\n",
      "Validation acc:  18 0.95038325\n",
      "epoch 18: loss = 0.20093429\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.19563551\n",
      "Training acc over epoch:  19 0.95079935\n",
      "Validation loss:  19 0.18455431\n",
      "Validation acc:  19 0.9513856\n",
      "epoch 19: loss = 0.19563553\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.18938588\n",
      "Training acc over epoch:  20 0.95164937\n",
      "Validation loss:  20 0.17659432\n",
      "Validation acc:  20 0.95274174\n",
      "epoch 20: loss = 0.18938588\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.18235251\n",
      "Training acc over epoch:  21 0.9521845\n",
      "Validation loss:  21 0.17018478\n",
      "Validation acc:  21 0.9532282\n",
      "epoch 21: loss = 0.18235248\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.17668873\n",
      "Training acc over epoch:  22 0.9525594\n",
      "Validation loss:  22 0.16512483\n",
      "Validation acc:  22 0.95405364\n",
      "epoch 22: loss = 0.17668872\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.17033938\n",
      "Training acc over epoch:  23 0.95316106\n",
      "Validation loss:  23 0.15694511\n",
      "Validation acc:  23 0.9547907\n",
      "epoch 23: loss = 0.17033938\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.16487364\n",
      "Training acc over epoch:  24 0.9537859\n",
      "Validation loss:  24 0.15145655\n",
      "Validation acc:  24 0.95538765\n",
      "epoch 24: loss = 0.16487364\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.15952288\n",
      "Training acc over epoch:  25 0.95430267\n",
      "Validation loss:  25 0.14613591\n",
      "Validation acc:  25 0.95660377\n",
      "epoch 25: loss = 0.15952289\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.15481265\n",
      "Training acc over epoch:  26 0.9550781\n",
      "Validation loss:  26 0.14030232\n",
      "Validation acc:  26 0.95824736\n",
      "epoch 26: loss = 0.15481266\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.14914827\n",
      "Training acc over epoch:  27 0.95607394\n",
      "Validation loss:  27 0.13341655\n",
      "Validation acc:  27 0.9596403\n",
      "epoch 27: loss = 0.1491483\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.14424984\n",
      "Training acc over epoch:  28 0.9572884\n",
      "Validation loss:  28 0.12862587\n",
      "Validation acc:  28 0.96338445\n",
      "epoch 28: loss = 0.14424983\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.13778782\n",
      "Training acc over epoch:  29 0.9592656\n",
      "Validation loss:  29 0.12075132\n",
      "Validation acc:  29 0.9644384\n",
      "epoch 29: loss = 0.13778782\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.13292228\n",
      "Training acc over epoch:  30 0.96024454\n",
      "Validation loss:  30 0.11541141\n",
      "Validation acc:  30 0.9665684\n",
      "epoch 30: loss = 0.13292226\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.12824558\n",
      "Training acc over epoch:  31 0.9619469\n",
      "Validation loss:  31 0.11238502\n",
      "Validation acc:  31 0.96880895\n",
      "epoch 31: loss = 0.1282456\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.12361617\n",
      "Training acc over epoch:  32 0.9633112\n",
      "Validation loss:  32 0.10912503\n",
      "Validation acc:  32 0.97103477\n",
      "epoch 32: loss = 0.123616174\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.12007394\n",
      "Training acc over epoch:  33 0.96455455\n",
      "Validation loss:  33 0.10288799\n",
      "Validation acc:  33 0.97233933\n",
      "epoch 33: loss = 0.12007393\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.11606621\n",
      "Training acc over epoch:  34 0.96608067\n",
      "Validation loss:  34 0.099905305\n",
      "Validation acc:  34 0.973069\n",
      "epoch 34: loss = 0.11606622\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.11192618\n",
      "Training acc over epoch:  35 0.96734726\n",
      "Validation loss:  35 0.09449005\n",
      "Validation acc:  35 0.9746315\n",
      "epoch 35: loss = 0.111926205\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.10889815\n",
      "Training acc over epoch:  36 0.96841913\n",
      "Validation loss:  36 0.09075666\n",
      "Validation acc:  36 0.9752506\n",
      "epoch 36: loss = 0.10889814\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.10577265\n",
      "Training acc over epoch:  37 0.9693957\n",
      "Validation loss:  37 0.089019224\n",
      "Validation acc:  37 0.97623086\n",
      "epoch 37: loss = 0.10577263\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.10296406\n",
      "Training acc over epoch:  38 0.9705806\n",
      "Validation loss:  38 0.08839296\n",
      "Validation acc:  38 0.9769826\n",
      "epoch 38: loss = 0.10296406\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.09999761\n",
      "Training acc over epoch:  39 0.97157794\n",
      "Validation loss:  39 0.08307264\n",
      "Validation acc:  39 0.97810286\n",
      "epoch 39: loss = 0.09999763\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.09649826\n",
      "Training acc over epoch:  40 0.97283167\n",
      "Validation loss:  40 0.080155\n",
      "Validation acc:  40 0.978523\n",
      "epoch 40: loss = 0.09649825\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.09369043\n",
      "Training acc over epoch:  41 0.973753\n",
      "Validation loss:  41 0.077476054\n",
      "Validation acc:  41 0.9795106\n",
      "epoch 41: loss = 0.09369044\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.0907434\n",
      "Training acc over epoch:  42 0.9747856\n",
      "Validation loss:  42 0.0756727\n",
      "Validation acc:  42 0.98041713\n",
      "epoch 42: loss = 0.0907434\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.08792433\n",
      "Training acc over epoch:  43 0.97593844\n",
      "Validation loss:  43 0.07281834\n",
      "Validation acc:  43 0.9812942\n",
      "epoch 43: loss = 0.08792434\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.084383324\n",
      "Training acc over epoch:  44 0.9770744\n",
      "Validation loss:  44 0.06894917\n",
      "Validation acc:  44 0.9825398\n",
      "epoch 44: loss = 0.08438333\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.08307092\n",
      "Training acc over epoch:  45 0.977648\n",
      "Validation loss:  45 0.06863793\n",
      "Validation acc:  45 0.9824882\n",
      "epoch 45: loss = 0.08307091\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.079990014\n",
      "Training acc over epoch:  46 0.97860056\n",
      "Validation loss:  46 0.06602435\n",
      "Validation acc:  46 0.9833063\n",
      "epoch 46: loss = 0.07999001\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.07799469\n",
      "Training acc over epoch:  47 0.97910684\n",
      "Validation loss:  47 0.06436156\n",
      "Validation acc:  47 0.9837338\n",
      "epoch 47: loss = 0.0779947\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.07682223\n",
      "Training acc over epoch:  48 0.9794882\n",
      "Validation loss:  48 0.06396368\n",
      "Validation acc:  48 0.98404336\n",
      "epoch 48: loss = 0.076822214\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.07464132\n",
      "Training acc over epoch:  49 0.98009944\n",
      "Validation loss:  49 0.061771717\n",
      "Validation acc:  49 0.98459613\n",
      "epoch 49: loss = 0.07464134\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.07382532\n",
      "Training acc over epoch:  50 0.98049676\n",
      "Validation loss:  50 0.061191197\n",
      "Validation acc:  50 0.98475087\n",
      "epoch 50: loss = 0.073825315\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.072003186\n",
      "Training acc over epoch:  51 0.9809326\n",
      "Validation loss:  51 0.060009934\n",
      "Validation acc:  51 0.98502356\n",
      "epoch 51: loss = 0.07200318\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.07137471\n",
      "Training acc over epoch:  52 0.98118657\n",
      "Validation loss:  52 0.059471913\n",
      "Validation acc:  52 0.985112\n",
      "epoch 52: loss = 0.071374714\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.070460044\n",
      "Training acc over epoch:  53 0.98134196\n",
      "Validation loss:  53 0.05874115\n",
      "Validation acc:  53 0.9854142\n",
      "epoch 53: loss = 0.07046005\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.068896964\n",
      "Training acc over epoch:  54 0.9817065\n",
      "Validation loss:  54 0.05750121\n",
      "Validation acc:  54 0.9855395\n",
      "epoch 54: loss = 0.06889697\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.067112274\n",
      "Training acc over epoch:  55 0.98220795\n",
      "Validation loss:  55 0.056957625\n",
      "Validation acc:  55 0.9859596\n",
      "epoch 55: loss = 0.067112274\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.06659292\n",
      "Training acc over epoch:  56 0.9824715\n",
      "Validation loss:  56 0.056370083\n",
      "Validation acc:  56 0.9860407\n",
      "epoch 56: loss = 0.06659292\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.06601427\n",
      "Training acc over epoch:  57 0.98255163\n",
      "Validation loss:  57 0.05604513\n",
      "Validation acc:  57 0.986166\n",
      "epoch 57: loss = 0.066014275\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.06458525\n",
      "Training acc over epoch:  58 0.9829602\n",
      "Validation loss:  58 0.054838087\n",
      "Validation acc:  58 0.98637974\n",
      "epoch 58: loss = 0.06458523\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.06405337\n",
      "Training acc over epoch:  59 0.98311883\n",
      "Validation loss:  59 0.05506009\n",
      "Validation acc:  59 0.9864313\n",
      "epoch 59: loss = 0.064053364\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 835/835 [00:21<00:00, 38.20it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.2 missing genotypes is 0.9524: \n",
      "Sensitivity: 0.1299551900761628\n",
      "Specificity: 0.9451571809396299\n",
      "F1-score macro: 0.16195241474285893\n",
      "F1-score micro: 0.9524288700705099\n",
      "Training using fold 3\n",
      "*******************************************\n",
      "*******************************************\n",
      "Missing rate 0.05\n",
      "=====================================================\n",
      "train_X_fake diff: 126252.0\n",
      "valid_X_fake diff: 14028.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.6455864\n",
      "Training acc over epoch:  0 0.7170837\n",
      "Validation loss:  0 0.28274798\n",
      "Validation acc:  0 0.9494767\n",
      "epoch 0: loss = 1.6455864\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.27602068\n",
      "Training acc over epoch:  1 0.9492011\n",
      "Validation loss:  1 0.2644132\n",
      "Validation acc:  1 0.9494767\n",
      "epoch 1: loss = 0.27602065\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.26134044\n",
      "Training acc over epoch:  2 0.9491891\n",
      "Validation loss:  2 0.25347322\n",
      "Validation acc:  2 0.9494767\n",
      "epoch 2: loss = 0.26134044\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.25303715\n",
      "Training acc over epoch:  3 0.9491827\n",
      "Validation loss:  3 0.24483484\n",
      "Validation acc:  3 0.9494767\n",
      "epoch 3: loss = 0.25303718\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.24587439\n",
      "Training acc over epoch:  4 0.94921553\n",
      "Validation loss:  4 0.2381273\n",
      "Validation acc:  4 0.9494767\n",
      "epoch 4: loss = 0.24587435\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.24028617\n",
      "Training acc over epoch:  5 0.9491947\n",
      "Validation loss:  5 0.23298717\n",
      "Validation acc:  5 0.9494767\n",
      "epoch 5: loss = 0.24028616\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.2362259\n",
      "Training acc over epoch:  6 0.9491931\n",
      "Validation loss:  6 0.22970337\n",
      "Validation acc:  6 0.9494767\n",
      "epoch 6: loss = 0.23622589\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.23245585\n",
      "Training acc over epoch:  7 0.9491819\n",
      "Validation loss:  7 0.22365294\n",
      "Validation acc:  7 0.9494767\n",
      "epoch 7: loss = 0.23245588\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.225\n",
      "Training acc over epoch:  8 0.949145\n",
      "Validation loss:  8 0.21457133\n",
      "Validation acc:  8 0.9494767\n",
      "epoch 8: loss = 0.22499995\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.2168112\n",
      "Training acc over epoch:  9 0.94923395\n",
      "Validation loss:  9 0.20485765\n",
      "Validation acc:  9 0.9494767\n",
      "epoch 9: loss = 0.21681118\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.20831764\n",
      "Training acc over epoch:  10 0.9493333\n",
      "Validation loss:  10 0.196789\n",
      "Validation acc:  10 0.94950616\n",
      "epoch 10: loss = 0.20831765\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.20065138\n",
      "Training acc over epoch:  11 0.9496818\n",
      "Validation loss:  11 0.18758188\n",
      "Validation acc:  11 0.95056015\n",
      "epoch 11: loss = 0.20065136\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.1950655\n",
      "Training acc over epoch:  12 0.9500463\n",
      "Validation loss:  12 0.18140422\n",
      "Validation acc:  12 0.95100975\n",
      "epoch 12: loss = 0.19506548\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.18950564\n",
      "Training acc over epoch:  13 0.9504637\n",
      "Validation loss:  13 0.17308609\n",
      "Validation acc:  13 0.9516436\n",
      "epoch 13: loss = 0.18950564\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.18355905\n",
      "Training acc over epoch:  14 0.9508114\n",
      "Validation loss:  14 0.16724497\n",
      "Validation acc:  14 0.95234376\n",
      "epoch 14: loss = 0.18355903\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.17764321\n",
      "Training acc over epoch:  15 0.95120955\n",
      "Validation loss:  15 0.15988989\n",
      "Validation acc:  15 0.95286703\n",
      "epoch 15: loss = 0.1776432\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.1712629\n",
      "Training acc over epoch:  16 0.951695\n",
      "Validation loss:  16 0.151221\n",
      "Validation acc:  16 0.9535672\n",
      "epoch 16: loss = 0.17126292\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.16458236\n",
      "Training acc over epoch:  17 0.9521773\n",
      "Validation loss:  17 0.14265174\n",
      "Validation acc:  17 0.95465803\n",
      "epoch 17: loss = 0.16458234\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.15750815\n",
      "Training acc over epoch:  18 0.95279896\n",
      "Validation loss:  18 0.13452688\n",
      "Validation acc:  18 0.9568101\n",
      "epoch 18: loss = 0.15750818\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.15047587\n",
      "Training acc over epoch:  19 0.95382756\n",
      "Validation loss:  19 0.12619627\n",
      "Validation acc:  19 0.95718604\n",
      "epoch 19: loss = 0.15047583\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.1423378\n",
      "Training acc over epoch:  20 0.95509255\n",
      "Validation loss:  20 0.11714321\n",
      "Validation acc:  20 0.96082693\n",
      "epoch 20: loss = 0.14233781\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.13493462\n",
      "Training acc over epoch:  21 0.9568518\n",
      "Validation loss:  21 0.10818221\n",
      "Validation acc:  21 0.9650943\n",
      "epoch 21: loss = 0.13493462\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.124704786\n",
      "Training acc over epoch:  22 0.9597302\n",
      "Validation loss:  22 0.094579026\n",
      "Validation acc:  22 0.9707326\n",
      "epoch 22: loss = 0.12470481\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.11249067\n",
      "Training acc over epoch:  23 0.9635099\n",
      "Validation loss:  23 0.08235813\n",
      "Validation acc:  23 0.97530216\n",
      "epoch 23: loss = 0.11249066\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.1001579\n",
      "Training acc over epoch:  24 0.9677855\n",
      "Validation loss:  24 0.06751358\n",
      "Validation acc:  24 0.9795622\n",
      "epoch 24: loss = 0.10015788\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.08846742\n",
      "Training acc over epoch:  25 0.97189677\n",
      "Validation loss:  25 0.05791244\n",
      "Validation acc:  25 0.983041\n",
      "epoch 25: loss = 0.08846741\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.07897921\n",
      "Training acc over epoch:  26 0.9750596\n",
      "Validation loss:  26 0.052891575\n",
      "Validation acc:  26 0.9854216\n",
      "epoch 26: loss = 0.07897921\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.07138909\n",
      "Training acc over epoch:  27 0.9775799\n",
      "Validation loss:  27 0.048618406\n",
      "Validation acc:  27 0.98706514\n",
      "epoch 27: loss = 0.071389094\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.06543163\n",
      "Training acc over epoch:  28 0.979642\n",
      "Validation loss:  28 0.041874982\n",
      "Validation acc:  28 0.9885392\n",
      "epoch 28: loss = 0.06543162\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.060770113\n",
      "Training acc over epoch:  29 0.9812979\n",
      "Validation loss:  29 0.039950497\n",
      "Validation acc:  29 0.98959315\n",
      "epoch 29: loss = 0.060770113\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.056625478\n",
      "Training acc over epoch:  30 0.9827175\n",
      "Validation loss:  30 0.0359174\n",
      "Validation acc:  30 0.9905881\n",
      "epoch 30: loss = 0.056625474\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.053494047\n",
      "Training acc over epoch:  31 0.98377335\n",
      "Validation loss:  31 0.033998933\n",
      "Validation acc:  31 0.9912441\n",
      "epoch 31: loss = 0.053494047\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.050748665\n",
      "Training acc over epoch:  32 0.98468983\n",
      "Validation loss:  32 0.03287542\n",
      "Validation acc:  32 0.9916347\n",
      "epoch 32: loss = 0.050748676\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.048275404\n",
      "Training acc over epoch:  33 0.98562557\n",
      "Validation loss:  33 0.030810388\n",
      "Validation acc:  33 0.9923128\n",
      "epoch 33: loss = 0.048275404\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.04558452\n",
      "Training acc over epoch:  34 0.9865132\n",
      "Validation loss:  34 0.02942332\n",
      "Validation acc:  34 0.99255604\n",
      "epoch 34: loss = 0.04558452\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.043451563\n",
      "Training acc over epoch:  35 0.987094\n",
      "Validation loss:  35 0.027633121\n",
      "Validation acc:  35 0.9932562\n",
      "epoch 35: loss = 0.04345156\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.04207882\n",
      "Training acc over epoch:  36 0.9875418\n",
      "Validation loss:  36 0.026593264\n",
      "Validation acc:  36 0.9935068\n",
      "epoch 36: loss = 0.042078823\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.040527217\n",
      "Training acc over epoch:  37 0.9881274\n",
      "Validation loss:  37 0.026002904\n",
      "Validation acc:  37 0.9936837\n",
      "epoch 37: loss = 0.04052722\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.039413854\n",
      "Training acc over epoch:  38 0.9884335\n",
      "Validation loss:  38 0.025462741\n",
      "Validation acc:  38 0.9939564\n",
      "epoch 38: loss = 0.039413854\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.038399495\n",
      "Training acc over epoch:  39 0.98886204\n",
      "Validation loss:  39 0.02468143\n",
      "Validation acc:  39 0.9940817\n",
      "epoch 39: loss = 0.0383995\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.037160497\n",
      "Training acc over epoch:  40 0.989112\n",
      "Validation loss:  40 0.023557138\n",
      "Validation acc:  40 0.99421436\n",
      "epoch 40: loss = 0.0371605\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.03698565\n",
      "Training acc over epoch:  41 0.9893107\n",
      "Validation loss:  41 0.024316892\n",
      "Validation acc:  41 0.9941996\n",
      "epoch 41: loss = 0.03698565\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.035026807\n",
      "Training acc over epoch:  42 0.9898066\n",
      "Validation loss:  42 0.02239577\n",
      "Validation acc:  42 0.9948924\n",
      "epoch 42: loss = 0.03502681\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.03395664\n",
      "Training acc over epoch:  43 0.9901422\n",
      "Validation loss:  43 0.021803958\n",
      "Validation acc:  43 0.99491453\n",
      "epoch 43: loss = 0.03395664\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.03306649\n",
      "Training acc over epoch:  44 0.9903065\n",
      "Validation loss:  44 0.021655984\n",
      "Validation acc:  44 0.9948629\n",
      "epoch 44: loss = 0.03306649\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.03253959\n",
      "Training acc over epoch:  45 0.99060607\n",
      "Validation loss:  45 0.021576785\n",
      "Validation acc:  45 0.9948924\n",
      "epoch 45: loss = 0.032539595\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.03165071\n",
      "Training acc over epoch:  46 0.99084\n",
      "Validation loss:  46 0.020458119\n",
      "Validation acc:  46 0.9951209\n",
      "epoch 46: loss = 0.031650703\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.030962043\n",
      "Training acc over epoch:  47 0.9909081\n",
      "Validation loss:  47 0.020198425\n",
      "Validation acc:  47 0.9952609\n",
      "epoch 47: loss = 0.030962039\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.030485373\n",
      "Training acc over epoch:  48 0.991098\n",
      "Validation loss:  48 0.02034187\n",
      "Validation acc:  48 0.99522406\n",
      "epoch 48: loss = 0.030485366\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.029397685\n",
      "Training acc over epoch:  49 0.9914593\n",
      "Validation loss:  49 0.019642746\n",
      "Validation acc:  49 0.99536407\n",
      "epoch 49: loss = 0.029397687\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.02937464\n",
      "Training acc over epoch:  50 0.99151856\n",
      "Validation loss:  50 0.019220766\n",
      "Validation acc:  50 0.995482\n",
      "epoch 50: loss = 0.02937464\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.028507724\n",
      "Training acc over epoch:  51 0.99172205\n",
      "Validation loss:  51 0.019141788\n",
      "Validation acc:  51 0.9953936\n",
      "epoch 51: loss = 0.028507728\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.028065307\n",
      "Training acc over epoch:  52 0.9918206\n",
      "Validation loss:  52 0.01898752\n",
      "Validation acc:  52 0.9957105\n",
      "epoch 52: loss = 0.028065303\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.027271869\n",
      "Training acc over epoch:  53 0.99195355\n",
      "Validation loss:  53 0.019391868\n",
      "Validation acc:  53 0.99537885\n",
      "epoch 53: loss = 0.027271867\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.027997006\n",
      "Training acc over epoch:  54 0.99186546\n",
      "Validation loss:  54 0.018849075\n",
      "Validation acc:  54 0.99550414\n",
      "epoch 54: loss = 0.027997004\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.026485024\n",
      "Training acc over epoch:  55 0.9921843\n",
      "Validation loss:  55 0.018603519\n",
      "Validation acc:  55 0.9955778\n",
      "epoch 55: loss = 0.026485015\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.026484849\n",
      "Training acc over epoch:  56 0.99222517\n",
      "Validation loss:  56 0.01909872\n",
      "Validation acc:  56 0.9954673\n",
      "epoch 56: loss = 0.026484849\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.0258602\n",
      "Training acc over epoch:  57 0.99244463\n",
      "Validation loss:  57 0.018508097\n",
      "Validation acc:  57 0.9957473\n",
      "epoch 57: loss = 0.025860203\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.025368894\n",
      "Training acc over epoch:  58 0.9925344\n",
      "Validation loss:  58 0.017720211\n",
      "Validation acc:  58 0.9959832\n",
      "epoch 58: loss = 0.025368894\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.02516635\n",
      "Training acc over epoch:  59 0.99259126\n",
      "Validation loss:  59 0.017874902\n",
      "Validation acc:  59 0.9957473\n",
      "epoch 59: loss = 0.025166353\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 834/834 [00:21<00:00, 38.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.05 missing genotypes is 0.9534: \n",
      "Sensitivity: 0.22960220031975437\n",
      "Specificity: 0.9313304350634175\n",
      "F1-score macro: 0.2797737062948416\n",
      "F1-score micro: 0.9534087016101405\n",
      "Missing rate 0.1\n",
      "=====================================================\n",
      "train_X_fake diff: 252504.0\n",
      "valid_X_fake diff: 28056.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 0.9238315\n",
      "Training acc over epoch:  0 0.91855276\n",
      "Validation loss:  0 0.30075756\n",
      "Validation acc:  0 0.9494767\n",
      "epoch 0: loss = 0.9238316\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.27860776\n",
      "Training acc over epoch:  1 0.94923156\n",
      "Validation loss:  1 0.2581945\n",
      "Validation acc:  1 0.9494767\n",
      "epoch 1: loss = 0.27860776\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.2570372\n",
      "Training acc over epoch:  2 0.94922835\n",
      "Validation loss:  2 0.24812138\n",
      "Validation acc:  2 0.9494767\n",
      "epoch 2: loss = 0.2570372\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.2493707\n",
      "Training acc over epoch:  3 0.94921315\n",
      "Validation loss:  3 0.24037722\n",
      "Validation acc:  3 0.9494767\n",
      "epoch 3: loss = 0.24937066\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.23900616\n",
      "Training acc over epoch:  4 0.9492043\n",
      "Validation loss:  4 0.22842203\n",
      "Validation acc:  4 0.9494767\n",
      "epoch 4: loss = 0.23900613\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.23100007\n",
      "Training acc over epoch:  5 0.9491282\n",
      "Validation loss:  5 0.2210399\n",
      "Validation acc:  5 0.9494767\n",
      "epoch 5: loss = 0.2310001\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.22368966\n",
      "Training acc over epoch:  6 0.94908255\n",
      "Validation loss:  6 0.21196039\n",
      "Validation acc:  6 0.9494693\n",
      "epoch 6: loss = 0.22368966\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.21452937\n",
      "Training acc over epoch:  7 0.949371\n",
      "Validation loss:  7 0.19841473\n",
      "Validation acc:  7 0.9501769\n",
      "epoch 7: loss = 0.21452937\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.20200817\n",
      "Training acc over epoch:  8 0.95015526\n",
      "Validation loss:  8 0.18494281\n",
      "Validation acc:  8 0.95247644\n",
      "epoch 8: loss = 0.2020082\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.18617462\n",
      "Training acc over epoch:  9 0.9514763\n",
      "Validation loss:  9 0.16198258\n",
      "Validation acc:  9 0.95397997\n",
      "epoch 9: loss = 0.18617459\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.16757518\n",
      "Training acc over epoch:  10 0.9533862\n",
      "Validation loss:  10 0.13915265\n",
      "Validation acc:  10 0.9577388\n",
      "epoch 10: loss = 0.16757517\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.14623311\n",
      "Training acc over epoch:  11 0.95690626\n",
      "Validation loss:  11 0.119164586\n",
      "Validation acc:  11 0.9647553\n",
      "epoch 11: loss = 0.14623311\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.12868798\n",
      "Training acc over epoch:  12 0.96082133\n",
      "Validation loss:  12 0.10366336\n",
      "Validation acc:  12 0.9690006\n",
      "epoch 12: loss = 0.128688\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.11518537\n",
      "Training acc over epoch:  13 0.9643879\n",
      "Validation loss:  13 0.09181457\n",
      "Validation acc:  13 0.9723909\n",
      "epoch 13: loss = 0.115185395\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.10484802\n",
      "Training acc over epoch:  14 0.9674402\n",
      "Validation loss:  14 0.0836222\n",
      "Validation acc:  14 0.9756854\n",
      "epoch 14: loss = 0.10484802\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.09616444\n",
      "Training acc over epoch:  15 0.96994126\n",
      "Validation loss:  15 0.076736644\n",
      "Validation acc:  15 0.9775722\n",
      "epoch 15: loss = 0.096164435\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.0902725\n",
      "Training acc over epoch:  16 0.9717622\n",
      "Validation loss:  16 0.072183765\n",
      "Validation acc:  16 0.9790168\n",
      "epoch 16: loss = 0.09027251\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.083724156\n",
      "Training acc over epoch:  17 0.974047\n",
      "Validation loss:  17 0.06567932\n",
      "Validation acc:  17 0.980712\n",
      "epoch 17: loss = 0.083724156\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.078810215\n",
      "Training acc over epoch:  18 0.9755731\n",
      "Validation loss:  18 0.062071502\n",
      "Validation acc:  18 0.9819944\n",
      "epoch 18: loss = 0.07881022\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.073885344\n",
      "Training acc over epoch:  19 0.9773035\n",
      "Validation loss:  19 0.059965406\n",
      "Validation acc:  19 0.98302627\n",
      "epoch 19: loss = 0.07388533\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.06982835\n",
      "Training acc over epoch:  20 0.9786398\n",
      "Validation loss:  20 0.054839578\n",
      "Validation acc:  20 0.98470664\n",
      "epoch 20: loss = 0.069828354\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.06498297\n",
      "Training acc over epoch:  21 0.98051363\n",
      "Validation loss:  21 0.05169906\n",
      "Validation acc:  21 0.9856943\n",
      "epoch 21: loss = 0.06498297\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.061206847\n",
      "Training acc over epoch:  22 0.9817073\n",
      "Validation loss:  22 0.048133094\n",
      "Validation acc:  22 0.9868662\n",
      "epoch 22: loss = 0.061206844\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.058103785\n",
      "Training acc over epoch:  23 0.98291534\n",
      "Validation loss:  23 0.045136463\n",
      "Validation acc:  23 0.98781693\n",
      "epoch 23: loss = 0.058103774\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.055406474\n",
      "Training acc over epoch:  24 0.98404974\n",
      "Validation loss:  24 0.043452762\n",
      "Validation acc:  24 0.988495\n",
      "epoch 24: loss = 0.05540649\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.052392405\n",
      "Training acc over epoch:  25 0.98491657\n",
      "Validation loss:  25 0.04122679\n",
      "Validation acc:  25 0.98949\n",
      "epoch 25: loss = 0.052392412\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.049662326\n",
      "Training acc over epoch:  26 0.9856792\n",
      "Validation loss:  26 0.039945558\n",
      "Validation acc:  26 0.9897332\n",
      "epoch 26: loss = 0.049662314\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.047934186\n",
      "Training acc over epoch:  27 0.9862576\n",
      "Validation loss:  27 0.039265666\n",
      "Validation acc:  27 0.99004275\n",
      "epoch 27: loss = 0.04793418\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.04612193\n",
      "Training acc over epoch:  28 0.98693216\n",
      "Validation loss:  28 0.037605394\n",
      "Validation acc:  28 0.9907134\n",
      "epoch 28: loss = 0.04612193\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.04425076\n",
      "Training acc over epoch:  29 0.98742163\n",
      "Validation loss:  29 0.03596978\n",
      "Validation acc:  29 0.9910377\n",
      "epoch 29: loss = 0.044250764\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.042796534\n",
      "Training acc over epoch:  30 0.9878903\n",
      "Validation loss:  30 0.036030278\n",
      "Validation acc:  30 0.99122196\n",
      "epoch 30: loss = 0.042796522\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.041027337\n",
      "Training acc over epoch:  31 0.9884767\n",
      "Validation loss:  31 0.034502972\n",
      "Validation acc:  31 0.9913031\n",
      "epoch 31: loss = 0.04102734\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.039795034\n",
      "Training acc over epoch:  32 0.98879635\n",
      "Validation loss:  32 0.034789853\n",
      "Validation acc:  32 0.99156106\n",
      "epoch 32: loss = 0.039795034\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.0382013\n",
      "Training acc over epoch:  33 0.9892474\n",
      "Validation loss:  33 0.033306614\n",
      "Validation acc:  33 0.99167895\n",
      "epoch 33: loss = 0.03820129\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.03744967\n",
      "Training acc over epoch:  34 0.9894317\n",
      "Validation loss:  34 0.032943144\n",
      "Validation acc:  34 0.99152416\n",
      "epoch 34: loss = 0.037449673\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.036154423\n",
      "Training acc over epoch:  35 0.9896712\n",
      "Validation loss:  35 0.032192484\n",
      "Validation acc:  35 0.9920475\n",
      "epoch 35: loss = 0.03615442\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.03520628\n",
      "Training acc over epoch:  36 0.9900085\n",
      "Validation loss:  36 0.031807728\n",
      "Validation acc:  36 0.99227595\n",
      "epoch 36: loss = 0.035206277\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.034044493\n",
      "Training acc over epoch:  37 0.99036336\n",
      "Validation loss:  37 0.03158288\n",
      "Validation acc:  37 0.99240124\n",
      "epoch 37: loss = 0.0340445\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.032730557\n",
      "Training acc over epoch:  38 0.9906798\n",
      "Validation loss:  38 0.030518701\n",
      "Validation acc:  38 0.9924749\n",
      "epoch 38: loss = 0.03273055\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.032135792\n",
      "Training acc over epoch:  39 0.99077195\n",
      "Validation loss:  39 0.030322446\n",
      "Validation acc:  39 0.9926076\n",
      "epoch 39: loss = 0.032135792\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.031200413\n",
      "Training acc over epoch:  40 0.9910291\n",
      "Validation loss:  40 0.029545566\n",
      "Validation acc:  40 0.99240124\n",
      "epoch 40: loss = 0.031200415\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.030280488\n",
      "Training acc over epoch:  41 0.9911765\n",
      "Validation loss:  41 0.029551275\n",
      "Validation acc:  41 0.9926224\n",
      "epoch 41: loss = 0.030280491\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.029476404\n",
      "Training acc over epoch:  42 0.991428\n",
      "Validation loss:  42 0.028706765\n",
      "Validation acc:  42 0.9929245\n",
      "epoch 42: loss = 0.029476404\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.028440515\n",
      "Training acc over epoch:  43 0.9916243\n",
      "Validation loss:  43 0.028434176\n",
      "Validation acc:  43 0.9927919\n",
      "epoch 43: loss = 0.028440515\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.027747799\n",
      "Training acc over epoch:  44 0.99180615\n",
      "Validation loss:  44 0.028441194\n",
      "Validation acc:  44 0.99299824\n",
      "epoch 44: loss = 0.027747797\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.027209789\n",
      "Training acc over epoch:  45 0.9919135\n",
      "Validation loss:  45 0.028017087\n",
      "Validation acc:  45 0.99301296\n",
      "epoch 45: loss = 0.027209787\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.026601123\n",
      "Training acc over epoch:  46 0.9920946\n",
      "Validation loss:  46 0.028080765\n",
      "Validation acc:  46 0.9931751\n",
      "epoch 46: loss = 0.026601123\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.02591553\n",
      "Training acc over epoch:  47 0.9922652\n",
      "Validation loss:  47 0.028376952\n",
      "Validation acc:  47 0.9930056\n",
      "epoch 47: loss = 0.025915528\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.024984542\n",
      "Training acc over epoch:  48 0.99238217\n",
      "Validation loss:  48 0.028722946\n",
      "Validation acc:  48 0.9932488\n",
      "epoch 48: loss = 0.024984553\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.024858784\n",
      "Training acc over epoch:  49 0.9923854\n",
      "Validation loss:  49 0.02789301\n",
      "Validation acc:  49 0.9932414\n",
      "epoch 49: loss = 0.024858788\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.02380038\n",
      "Training acc over epoch:  50 0.9927515\n",
      "Validation loss:  50 0.029136771\n",
      "Validation acc:  50 0.9933373\n",
      "epoch 50: loss = 0.02380038\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.023365496\n",
      "Training acc over epoch:  51 0.9928612\n",
      "Validation loss:  51 0.028472582\n",
      "Validation acc:  51 0.9932193\n",
      "epoch 51: loss = 0.023365496\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.02303486\n",
      "Training acc over epoch:  52 0.9928853\n",
      "Validation loss:  52 0.02827743\n",
      "Validation acc:  52 0.99330044\n",
      "epoch 52: loss = 0.02303486\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.022820532\n",
      "Training acc over epoch:  53 0.99295336\n",
      "Validation loss:  53 0.028092321\n",
      "Validation acc:  53 0.99363947\n",
      "epoch 53: loss = 0.02282053\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.021817701\n",
      "Training acc over epoch:  54 0.99318486\n",
      "Validation loss:  54 0.027064838\n",
      "Validation acc:  54 0.99344045\n",
      "epoch 54: loss = 0.021817703\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.021439902\n",
      "Training acc over epoch:  55 0.9933708\n",
      "Validation loss:  55 0.028225807\n",
      "Validation acc:  55 0.9934552\n",
      "epoch 55: loss = 0.021439899\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.020731306\n",
      "Training acc over epoch:  56 0.99345565\n",
      "Validation loss:  56 0.027683582\n",
      "Validation acc:  56 0.9933299\n",
      "epoch 56: loss = 0.020731306\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.020591317\n",
      "Training acc over epoch:  57 0.99350613\n",
      "Validation loss:  57 0.0276052\n",
      "Validation acc:  57 0.9934036\n",
      "epoch 57: loss = 0.020591317\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.01980043\n",
      "Training acc over epoch:  58 0.9937537\n",
      "Validation loss:  58 0.028383523\n",
      "Validation acc:  58 0.99355835\n",
      "epoch 58: loss = 0.01980043\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.019365802\n",
      "Training acc over epoch:  59 0.9938106\n",
      "Validation loss:  59 0.028267985\n",
      "Validation acc:  59 0.99358046\n",
      "epoch 59: loss = 0.0193658\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 834/834 [00:21<00:00, 38.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9482: \n",
      "Sensitivity: 0.23307454725705107\n",
      "Specificity: 0.9312017428280894\n",
      "F1-score macro: 0.2865383391601152\n",
      "F1-score micro: 0.948184309695101\n",
      "Missing rate 0.2\n",
      "=====================================================\n",
      "train_X_fake diff: 508014.0\n",
      "valid_X_fake diff: 56446.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 1.0766238\n",
      "Training acc over epoch:  0 0.809475\n",
      "Validation loss:  0 0.28972602\n",
      "Validation acc:  0 0.9494767\n",
      "epoch 0: loss = 1.0766236\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.28406364\n",
      "Training acc over epoch:  1 0.9492099\n",
      "Validation loss:  1 0.26585642\n",
      "Validation acc:  1 0.9494767\n",
      "epoch 1: loss = 0.28406364\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.26243323\n",
      "Training acc over epoch:  2 0.9491955\n",
      "Validation loss:  2 0.25145468\n",
      "Validation acc:  2 0.9494767\n",
      "epoch 2: loss = 0.26243326\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.25544542\n",
      "Training acc over epoch:  3 0.94920754\n",
      "Validation loss:  3 0.24907537\n",
      "Validation acc:  3 0.9494767\n",
      "epoch 3: loss = 0.2554454\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.25123483\n",
      "Training acc over epoch:  4 0.9491851\n",
      "Validation loss:  4 0.2431914\n",
      "Validation acc:  4 0.9494767\n",
      "epoch 4: loss = 0.25123483\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.24499258\n",
      "Training acc over epoch:  5 0.94915706\n",
      "Validation loss:  5 0.2390904\n",
      "Validation acc:  5 0.9494767\n",
      "epoch 5: loss = 0.24499257\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.24179089\n",
      "Training acc over epoch:  6 0.9491843\n",
      "Validation loss:  6 0.23669752\n",
      "Validation acc:  6 0.9494767\n",
      "epoch 6: loss = 0.24179089\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.23998243\n",
      "Training acc over epoch:  7 0.94920033\n",
      "Validation loss:  7 0.23493733\n",
      "Validation acc:  7 0.9494767\n",
      "epoch 7: loss = 0.23998249\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.23852825\n",
      "Training acc over epoch:  8 0.9491795\n",
      "Validation loss:  8 0.23379542\n",
      "Validation acc:  8 0.9494767\n",
      "epoch 8: loss = 0.23852824\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.23695394\n",
      "Training acc over epoch:  9 0.9492099\n",
      "Validation loss:  9 0.23210981\n",
      "Validation acc:  9 0.9494767\n",
      "epoch 9: loss = 0.23695393\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.23487234\n",
      "Training acc over epoch:  10 0.94921154\n",
      "Validation loss:  10 0.23029017\n",
      "Validation acc:  10 0.9494767\n",
      "epoch 10: loss = 0.23487233\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.2317806\n",
      "Training acc over epoch:  11 0.9492195\n",
      "Validation loss:  11 0.22509198\n",
      "Validation acc:  11 0.9494767\n",
      "epoch 11: loss = 0.23178062\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.22751671\n",
      "Training acc over epoch:  12 0.9491699\n",
      "Validation loss:  12 0.21916687\n",
      "Validation acc:  12 0.9494767\n",
      "epoch 12: loss = 0.22751674\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.22122075\n",
      "Training acc over epoch:  13 0.9492292\n",
      "Validation loss:  13 0.21042117\n",
      "Validation acc:  13 0.9494767\n",
      "epoch 13: loss = 0.22122076\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.21492927\n",
      "Training acc over epoch:  14 0.94934213\n",
      "Validation loss:  14 0.20490663\n",
      "Validation acc:  14 0.94953567\n",
      "epoch 14: loss = 0.21492922\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.2113977\n",
      "Training acc over epoch:  15 0.9495776\n",
      "Validation loss:  15 0.20393486\n",
      "Validation acc:  15 0.9507223\n",
      "epoch 15: loss = 0.2113977\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.20629507\n",
      "Training acc over epoch:  16 0.94991815\n",
      "Validation loss:  16 0.19583687\n",
      "Validation acc:  16 0.9511129\n",
      "epoch 16: loss = 0.20629507\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.20309994\n",
      "Training acc over epoch:  17 0.9501745\n",
      "Validation loss:  17 0.19100507\n",
      "Validation acc:  17 0.95145196\n",
      "epoch 17: loss = 0.20309995\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.19875431\n",
      "Training acc over epoch:  18 0.950555\n",
      "Validation loss:  18 0.18743232\n",
      "Validation acc:  18 0.9521153\n",
      "epoch 18: loss = 0.19875431\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.19481267\n",
      "Training acc over epoch:  19 0.95087147\n",
      "Validation loss:  19 0.18269947\n",
      "Validation acc:  19 0.9523069\n",
      "epoch 19: loss = 0.19481267\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.19147576\n",
      "Training acc over epoch:  20 0.9511038\n",
      "Validation loss:  20 0.17982195\n",
      "Validation acc:  20 0.9528228\n",
      "epoch 20: loss = 0.19147575\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.1881377\n",
      "Training acc over epoch:  21 0.9513433\n",
      "Validation loss:  21 0.17895718\n",
      "Validation acc:  21 0.9532798\n",
      "epoch 21: loss = 0.1881377\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.18476228\n",
      "Training acc over epoch:  22 0.95172465\n",
      "Validation loss:  22 0.17295933\n",
      "Validation acc:  22 0.9536704\n",
      "epoch 22: loss = 0.18476231\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.18105783\n",
      "Training acc over epoch:  23 0.9519786\n",
      "Validation loss:  23 0.16994585\n",
      "Validation acc:  23 0.95420104\n",
      "epoch 23: loss = 0.18105784\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.17826842\n",
      "Training acc over epoch:  24 0.95223415\n",
      "Validation loss:  24 0.16428427\n",
      "Validation acc:  24 0.954459\n",
      "epoch 24: loss = 0.17826842\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.17296305\n",
      "Training acc over epoch:  25 0.9528494\n",
      "Validation loss:  25 0.15838757\n",
      "Validation acc:  25 0.9556309\n",
      "epoch 25: loss = 0.17296305\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.1670163\n",
      "Training acc over epoch:  26 0.9536345\n",
      "Validation loss:  26 0.15174797\n",
      "Validation acc:  26 0.956788\n",
      "epoch 26: loss = 0.16701631\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.16280352\n",
      "Training acc over epoch:  27 0.95414644\n",
      "Validation loss:  27 0.14448874\n",
      "Validation acc:  27 0.9579747\n",
      "epoch 27: loss = 0.16280353\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.15771145\n",
      "Training acc over epoch:  28 0.9550557\n",
      "Validation loss:  28 0.14409825\n",
      "Validation acc:  28 0.9596698\n",
      "epoch 28: loss = 0.15771149\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.15398093\n",
      "Training acc over epoch:  29 0.95579594\n",
      "Validation loss:  29 0.1360132\n",
      "Validation acc:  29 0.96073115\n",
      "epoch 29: loss = 0.15398093\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.14976057\n",
      "Training acc over epoch:  30 0.9563944\n",
      "Validation loss:  30 0.13299143\n",
      "Validation acc:  30 0.96231574\n",
      "epoch 30: loss = 0.14976054\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.14582065\n",
      "Training acc over epoch:  31 0.9571851\n",
      "Validation loss:  31 0.12851568\n",
      "Validation acc:  31 0.963635\n",
      "epoch 31: loss = 0.14582063\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.14257145\n",
      "Training acc over epoch:  32 0.95833707\n",
      "Validation loss:  32 0.12514123\n",
      "Validation acc:  32 0.96437204\n",
      "epoch 32: loss = 0.14257145\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.13916236\n",
      "Training acc over epoch:  33 0.95917904\n",
      "Validation loss:  33 0.121204376\n",
      "Validation acc:  33 0.9659714\n",
      "epoch 33: loss = 0.13916235\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.13533914\n",
      "Training acc over epoch:  34 0.96026134\n",
      "Validation loss:  34 0.11691831\n",
      "Validation acc:  34 0.9662588\n",
      "epoch 34: loss = 0.13533914\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.13224304\n",
      "Training acc over epoch:  35 0.96111614\n",
      "Validation loss:  35 0.116011105\n",
      "Validation acc:  35 0.9684626\n",
      "epoch 35: loss = 0.13224304\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.12870479\n",
      "Training acc over epoch:  36 0.96227133\n",
      "Validation loss:  36 0.1100891\n",
      "Validation acc:  36 0.96910375\n",
      "epoch 36: loss = 0.12870479\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.124988236\n",
      "Training acc over epoch:  37 0.96310455\n",
      "Validation loss:  37 0.10738976\n",
      "Validation acc:  37 0.9703641\n",
      "epoch 37: loss = 0.12498822\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.1221346\n",
      "Training acc over epoch:  38 0.96407145\n",
      "Validation loss:  38 0.108264446\n",
      "Validation acc:  38 0.9710053\n",
      "epoch 38: loss = 0.12213459\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.11942312\n",
      "Training acc over epoch:  39 0.96490544\n",
      "Validation loss:  39 0.10178168\n",
      "Validation acc:  39 0.97181606\n",
      "epoch 39: loss = 0.11942313\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.11669013\n",
      "Training acc over epoch:  40 0.96593326\n",
      "Validation loss:  40 0.099191785\n",
      "Validation acc:  40 0.9724425\n",
      "epoch 40: loss = 0.11669014\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.11427427\n",
      "Training acc over epoch:  41 0.96673757\n",
      "Validation loss:  41 0.09705606\n",
      "Validation acc:  41 0.97264886\n",
      "epoch 41: loss = 0.11427426\n",
      "Start of epoch 42\n",
      "Training loss over epoch:  42 0.111682616\n",
      "Training acc over epoch:  42 0.9676661\n",
      "Validation loss:  42 0.096069165\n",
      "Validation acc:  42 0.97413766\n",
      "epoch 42: loss = 0.11168262\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.10915304\n",
      "Training acc over epoch:  43 0.9683695\n",
      "Validation loss:  43 0.09454576\n",
      "Validation acc:  43 0.9745136\n",
      "epoch 43: loss = 0.109153055\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.1076879\n",
      "Training acc over epoch:  44 0.96892625\n",
      "Validation loss:  44 0.09267709\n",
      "Validation acc:  44 0.97525793\n",
      "epoch 44: loss = 0.1076879\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.105386496\n",
      "Training acc over epoch:  45 0.96957356\n",
      "Validation loss:  45 0.09227563\n",
      "Validation acc:  45 0.9756191\n",
      "epoch 45: loss = 0.105386496\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.103000976\n",
      "Training acc over epoch:  46 0.9703643\n",
      "Validation loss:  46 0.087996\n",
      "Validation acc:  46 0.97602445\n",
      "epoch 46: loss = 0.10300096\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.10123908\n",
      "Training acc over epoch:  47 0.9709002\n",
      "Validation loss:  47 0.086326815\n",
      "Validation acc:  47 0.976872\n",
      "epoch 47: loss = 0.10123906\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.09908664\n",
      "Training acc over epoch:  48 0.9716492\n",
      "Validation loss:  48 0.08489725\n",
      "Validation acc:  48 0.9774617\n",
      "epoch 48: loss = 0.09908665\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.097257875\n",
      "Training acc over epoch:  49 0.9723326\n",
      "Validation loss:  49 0.083593406\n",
      "Validation acc:  49 0.97782284\n",
      "epoch 49: loss = 0.09725788\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.09556303\n",
      "Training acc over epoch:  50 0.97286135\n",
      "Validation loss:  50 0.08139502\n",
      "Validation acc:  50 0.978184\n",
      "epoch 50: loss = 0.09556302\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.094004035\n",
      "Training acc over epoch:  51 0.9732547\n",
      "Validation loss:  51 0.08037444\n",
      "Validation acc:  51 0.978722\n",
      "epoch 51: loss = 0.09400402\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.09209848\n",
      "Training acc over epoch:  52 0.9740438\n",
      "Validation loss:  52 0.07939544\n",
      "Validation acc:  52 0.97914946\n",
      "epoch 52: loss = 0.09209847\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.08960659\n",
      "Training acc over epoch:  53 0.97478\n",
      "Validation loss:  53 0.07832383\n",
      "Validation acc:  53 0.9796359\n",
      "epoch 53: loss = 0.08960658\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.088093795\n",
      "Training acc over epoch:  54 0.9752791\n",
      "Validation loss:  54 0.07527566\n",
      "Validation acc:  54 0.98026973\n",
      "epoch 54: loss = 0.08809379\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.08657736\n",
      "Training acc over epoch:  55 0.975775\n",
      "Validation loss:  55 0.07382742\n",
      "Validation acc:  55 0.9805056\n",
      "epoch 55: loss = 0.08657736\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.08477974\n",
      "Training acc over epoch:  56 0.97632056\n",
      "Validation loss:  56 0.073374726\n",
      "Validation acc:  56 0.9811468\n",
      "epoch 56: loss = 0.08477975\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.08295561\n",
      "Training acc over epoch:  57 0.9769751\n",
      "Validation loss:  57 0.07126449\n",
      "Validation acc:  57 0.98153746\n",
      "epoch 57: loss = 0.08295561\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.08168004\n",
      "Training acc over epoch:  58 0.97725546\n",
      "Validation loss:  58 0.07042948\n",
      "Validation acc:  58 0.9817733\n",
      "epoch 58: loss = 0.08168005\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.08052044\n",
      "Training acc over epoch:  59 0.9777049\n",
      "Validation loss:  59 0.07034393\n",
      "Validation acc:  59 0.9818986\n",
      "epoch 59: loss = 0.08052044\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 834/834 [00:22<00:00, 37.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average imputation accuracyon test data with 0.2 missing genotypes is 0.9508: \n",
      "Sensitivity: 0.1394978670851363\n",
      "Specificity: 0.948818787403886\n",
      "F1-score macro: 0.16808917386412775\n",
      "F1-score micro: 0.9507754742951201\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "metadata": {
    "id": "ifLke-oWc1Ok"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "CRjKkWClfjmM"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1lo9cmptjRRESsVlVECvlJFHbLeaygq2s",
     "timestamp": 1640128751823
    },
    {
     "file_id": "https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb",
     "timestamp": 1621552889682
    }
   ],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
