{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20396,
     "status": "ok",
     "timestamp": 1694134282538,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "s-9aa8yrHj9j",
    "outputId": "6f183fe8-82cf-4033-96eb-61c1e0d0f824"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20656,
     "status": "ok",
     "timestamp": 1694134303346,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "liJJGzQp4qzO",
    "outputId": "db4c82ab-dcd9-4e4c-ce06-8c1272321a99"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m612.1/612.1 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Collecting scikit-allel\n",
      "  Downloading scikit_allel-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.1/8.1 MB\u001B[0m \u001B[31m22.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-allel) (1.23.5)\n",
      "Requirement already satisfied: dask[array] in /usr/local/lib/python3.10/dist-packages (from scikit-allel) (2023.8.1)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (6.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[array]->scikit-allel) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[array]->scikit-allel) (3.16.2)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[array]->scikit-allel) (1.0.0)\n",
      "Installing collected packages: scikit-allel\n",
      "Successfully installed scikit-allel-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n",
    "!pip install toolz scikit-allel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9397,
     "status": "ok",
     "timestamp": 1694134312739,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "ZWzi3Z3L5RO2",
    "outputId": "9e8353c2-898f-450b-9ba0-13338e328ad2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
      "Tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjGOO5PdFPf7"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3811,
     "status": "ok",
     "timestamp": 1694134316547,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "odmhCqSVFPf8",
    "outputId": "d5977fd7-b361-4a2a-cef6-19120dedc8c6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"MODIN_CPUS\"] = \"8\"\n",
    "# from distributed import Client\n",
    "# client = Client()\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import efficientnet as efn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "# import allel\n",
    "from scipy.spatial.distance import squareform\n",
    "%matplotlib inline\n",
    "from toolz import interleave\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLd26RspFhaS"
   },
   "source": [
    "## Hardware Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7449,
     "status": "ok",
     "timestamp": 1694134323994,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "SLd7mAFgFUnR",
    "outputId": "bb392a37-bc35-47d2-9e8e-661b01ef9ac1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on TPU  grpc://10.28.108.50:8470\n",
      "N_REPLICAS: 8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', TPU.master())\n",
    "except ValueError:\n",
    "    print('Running on GPU')\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.TPUStrategy(TPU)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "N_REPLICAS = strategy.num_replicas_in_sync\n",
    "# Number of computing cores, is 8 for a TPU V3-8\n",
    "print(f'N_REPLICAS: {N_REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A77GFE3xFPf8"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1694134324183,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "PamDEI0R5xMN"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    If the reference is unphased, cannot handle phased target data, so the valid (ref, target) combinations are:\n",
    "    (phased, phased), (phased, unphased), (unphased, unphased)\n",
    "    Important note: for each case, the model should be trained separately\n",
    "    \"\"\"\n",
    "    def __init__(self, reference_panel_file_path, target_file_path):\n",
    "        self.ref_n_header_lines = []\n",
    "        self.ref_n_data_header = \"\"\n",
    "        self.map_values_1_vec = np.vectorize(self.map_hap_2_ind_parent_1)\n",
    "        self.map_values_2_vec = np.vectorize(self.map_hap_2_ind_parent_2)\n",
    "        print(\"Rading the reference file...\")\n",
    "        # get header\n",
    "        root, ext = os.path.splitext(reference_panel_file_path)\n",
    "        with gzip.open(reference_panel_file_path, 'rt') if ext == '.gz' else open(reference_panel_file_path, 'rt') as f_in:\n",
    "            # skip info\n",
    "            while True:\n",
    "                line = f_in.readline()\n",
    "                if line.startswith(\"##\"):\n",
    "                    self.ref_n_header_lines.append(line)\n",
    "                else:\n",
    "                    self.ref_n_data_header = line\n",
    "                    break\n",
    "        self.reference_panel = pd.read_csv(reference_panel_file_path,\n",
    "                                           comment='#',\n",
    "                                           sep='\\t',\n",
    "                                           names=self.ref_n_data_header.strip().split('\\t'))\n",
    "        self.VARIANT_COUNT = self.reference_panel.shape[0]\n",
    "        print(f\"{self.VARIANT_COUNT} variants found. Done!\")\n",
    "        print(\"Rading the target file...\")\n",
    "        self.target_n_header_lines = []\n",
    "        self.target_n_data_header = \"\"\n",
    "        root, ext = os.path.splitext(target_file_path)\n",
    "        # get header\n",
    "        with gzip.open(target_file_path, 'rt') if ext == '.gz' else open(target_file_path, 'rt') as f_in:\n",
    "            # skip info\n",
    "            while True:\n",
    "                line = f_in.readline()\n",
    "                if line.startswith(\"##\"):\n",
    "                    self.target_n_header_lines.append(line)\n",
    "                else:\n",
    "                    self.target_n_data_header = line\n",
    "                    break\n",
    "        real_target_set = pd.read_csv(target_file_path,\n",
    "                                           comment='#',\n",
    "                                           sep='\\t',\n",
    "                                           names=self.target_n_data_header.strip().split('\\t'),)\n",
    "        print(f\"{real_target_set.shape[0]} variants found. Done!\")\n",
    "        target_is_phased = \"|\" in real_target_set.iloc[0, 10]\n",
    "        ref_is_phased = \"|\" in self.reference_panel.iloc[0, 10]\n",
    "        self.is_phased = target_is_phased and ref_is_phased\n",
    "        print(\"Creating the new target dataframe\")\n",
    "        self.target_set = real_target_set.merge(self.reference_panel[\"ID\"], on='ID', how='right')\n",
    "        self.target_set[self.reference_panel.columns[:9]] = self.reference_panel[self.reference_panel.columns[:9]]\n",
    "        self.target_set.fillna(\".|.\" if self.is_phased else \"./.\", inplace=True)\n",
    "        print(\"Extracting genotype information...\")\n",
    "        SEP = \"|\" if self.is_phased else \"/\"\n",
    "        def get_num_allels(g):\n",
    "            v1, v2 = g.split(SEP)\n",
    "            return max(int(v1), int(v2)) + 1\n",
    "\n",
    "        def key_gen(v1, v2):\n",
    "            return f\"{v1}{SEP}{v2}\"\n",
    "\n",
    "        genotype_vals = np.unique(self.reference_panel.iloc[:, 9:].values)\n",
    "        if target_is_phased != ref_is_phased:\n",
    "            phased_to_unphased_dict = {}\n",
    "            for i in range(genotype_vals.shape[0]):\n",
    "                key = genotype_vals[i]\n",
    "                v1, v2 = [int(s) for s in genotype_vals[i].split(\"|\")]\n",
    "                genotype_vals[i] = f\"{min(v1, v2)}{SEP}{max(v1, v2)}\"\n",
    "                phased_to_unphased_dict[key] = genotype_vals[i]\n",
    "            self.reference_panel.replace(phased_to_unphased_dict, inplace=True)\n",
    "        genotype_vals = np.unique(genotype_vals)\n",
    "        allele_count = max(map(get_num_allels, genotype_vals))\n",
    "        if self.is_phased:\n",
    "            self.hap_map = {str(i): i for i in range(allele_count)}\n",
    "            self.hap_map.update({\".\": allele_count})\n",
    "            self.r_hap_map = {i:k for k, i in self.hap_map.items()}\n",
    "            self.map_preds_2_allele = np.vectorize(lambda x: self.r_hap_map[x])\n",
    "        self.MISSING_VALUE = self.SEQ_DEPTH = allele_count + 1 if self.is_phased else len(genotype_vals) + 1\n",
    "        self.genotype_keys = np.array([key_gen(i,j) for i in range(allele_count) for j in range(allele_count)]) if self.is_phased else genotype_vals\n",
    "        self.genotype_keys = np.hstack([self.genotype_keys, [\".|.\"] if self.is_phased else [\"./.\"]])\n",
    "        self.replacement_dict = {g:i for i,g in enumerate(self.genotype_keys)}\n",
    "        self.reverse_replacement_dict = {i:g for g,i in self.replacement_dict.items()}\n",
    "\n",
    "    def get_max_gap_len(self):\n",
    "      self.target_set_vals = self.target_set.iloc[:, 9].values\n",
    "      gap_meter = 0\n",
    "      max_gap_so_far = 0\n",
    "      for i, val in enumerate(self.target_set_vals):\n",
    "          if val in (\".|.\", \"./.\"):\n",
    "              gap_meter+=1\n",
    "          else:\n",
    "              max_gap_so_far = max(gap_meter, max_gap_so_far)\n",
    "              gap_meter = 0\n",
    "      max_gap_so_far = max(gap_meter, max_gap_so_far)\n",
    "      return max_gap_so_far\n",
    "\n",
    "    def map_hap_2_ind_parent_1(self, x):\n",
    "        return self.hap_map[x.split('|')[0]]\n",
    "\n",
    "    def map_hap_2_ind_parent_2(self, x):\n",
    "        return self.hap_map[x.split('|')[1]]\n",
    "\n",
    "    def __get_forward_data(self, data: pd.DataFrame):\n",
    "        if self.is_phased:\n",
    "            # break it into haplotypes\n",
    "            _x = np.empty((data.shape[1] * 2, data.shape[0]), dtype=np.int32)\n",
    "\n",
    "            _x[0::2] = self.map_values_1_vec(data.values.T)\n",
    "            _x[1::2] = self.map_values_2_vec(data.values.T)\n",
    "            return _x\n",
    "        else:\n",
    "            return data.replace(self.replacement_dict).values.T.astype(np.int32)\n",
    "\n",
    "    def get_ref_set(self, starting_var_index=None, ending_var_index=None):\n",
    "        if starting_var_index>=0 and ending_var_index>=starting_var_index:\n",
    "            return self.__get_forward_data(self.reference_panel.iloc[starting_var_index:ending_var_index, 9:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(self.reference_panel.iloc[:, 9:])\n",
    "\n",
    "    def get_target_set(self, starting_var_index=None, ending_var_index=None):\n",
    "        if starting_var_index>=0 and ending_var_index>=starting_var_index:\n",
    "            return self.__get_forward_data(self.target_set.iloc[starting_var_index:ending_var_index, 9:])\n",
    "        else:\n",
    "            print(\"No variant indices provided or indices not valid, using the whole sequence...\")\n",
    "            return self.__get_forward_data(self.target_set.iloc[:, 9:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1694134324184,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "j3zy8i_8FPf_"
   },
   "outputs": [],
   "source": [
    "root_data_dir = '[data_path]'\n",
    "train_file_name = \"beadchip_reference_all_minaf_05_snps_hwe_1e-2_filtered_train.vcf.gz\"\n",
    "test_file_name = \"test_data_beadchip_hwe_filtered.vcf.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190060,
     "status": "ok",
     "timestamp": 1694134514238,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "H2LdnKzXnW0P",
    "outputId": "c1d169f6-6907-4070-c32d-7b2b581b9695"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rading the reference file...\n",
      "31143 variants found. Done!\n",
      "Rading the target file...\n",
      "7336 variants found. Done!\n",
      "Creating the new target dataframe\n",
      "Extracting genotype information...\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(root_data_dir + train_file_name,\n",
    "                root_data_dir + test_file_name)\n",
    "max_gap_len = dl.get_max_gap_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1694134514240,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "JAbnJ8HtYjpX",
    "outputId": "608bf2a5-effd-4609-bca0-5cf21446d52f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "max_gap_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkXsXvzBnr8H"
   },
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1694134514240,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "_pZoO-FvKdr3",
    "outputId": "2252aa26-ed5b-41c3-92d7-9e643011b9fe"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 31143)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# hyperparameters\n",
    "inChannel = dl.SEQ_DEPTH\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "dropout_rate = 0.25\n",
    "attention_range = 100\n",
    "chunk_size = 2000\n",
    "max_features_len_per_model = dl.VARIANT_COUNT\n",
    "inChannel, max_features_len_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1694134514240,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "ql-vYiwNcLKq",
    "outputId": "3f51be41-399a-46f9-f91f-b56b387b9622"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['0|0', '0|1', '1|0', '1|1', '.|.'], dtype='<U3')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dl.genotype_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1694134514241,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "MTacHErccXVT",
    "outputId": "03214377-a1bf-407c-da10-15fdaa7b7f0c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "dl.MISSING_VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtSxW2eMOOCU"
   },
   "source": [
    "## Convert to tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134514241,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "KDiVoe67JHcy"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def add_attention_mask(X_sample, y_sample):\n",
    "  depth = dl.SEQ_DEPTH\n",
    "  mask_size = tf.cast(X_sample.shape[0]*0.8, dtype=tf.int32)\n",
    "  mask_idx = tf.reshape(tf.random.shuffle(tf.range(X_sample.shape[0]))[:mask_size], (-1, 1))\n",
    "  updates = tf.math.add(tf.zeros(shape=(mask_idx.shape[0]), dtype=tf.int32), depth-1)\n",
    "  X_masked = tf.tensor_scatter_nd_update(X_sample, mask_idx, updates)\n",
    "\n",
    "  return tf.one_hot(X_masked, depth), tf.one_hot(y_sample, depth-1)\n",
    "\n",
    "@tf.function()\n",
    "def onehot_encode(X_sample):\n",
    "  depth = dl.SEQ_DEPTH\n",
    "  return tf.one_hot(X_sample, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134514241,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "SlyxYCy96H7V"
   },
   "outputs": [],
   "source": [
    "def get_dataset(x, batch_size, offset_before=0, offset_after=0, training=True):\n",
    "  AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((x, x[:, offset_before:x.shape[1]-offset_after]))\n",
    "  # # Add Attention Mask\n",
    "\n",
    "  if training:\n",
    "    dataset = dataset.shuffle(x.shape[0], reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "  # Add Attention Mask\n",
    "  dataset = dataset.map(add_attention_mask, num_parallel_calls=AUTO, deterministic=False)\n",
    "\n",
    "  # Prefetech to not map the whole dataset\n",
    "  dataset = dataset.prefetch(AUTO)\n",
    "\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def get_test_dataset(x, batch_size):\n",
    "  AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((x))\n",
    "  # # Add Attention Mask\n",
    "\n",
    "\n",
    "  # Add Attention Mask\n",
    "  dataset = dataset.map(onehot_encode, num_parallel_calls=AUTO, deterministic=True)\n",
    "\n",
    "  # Prefetech to not map the whole dataset\n",
    "  dataset = dataset.prefetch(AUTO)\n",
    "\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=False, num_parallel_calls=AUTO)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpRUjMp_L914"
   },
   "source": [
    "## Custom Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ATTENTION_AXES=(1)"
   ],
   "metadata": {
    "id": "ZVxcLA44DdmD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694134514242,
     "user_tz": 240,
     "elapsed": 10,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134514242,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "sD7WOyyoNYGv"
   },
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(layers.Layer):\n",
    "  def __init__(self, local_dim, global_dim,\n",
    "               start_offset=0, end_offset=0,\n",
    "               activation=tf.nn.gelu, dropout_rate=0.1,\n",
    "               n_heads=8):\n",
    "    super(CrossAttentionLayer, self).__init__()\n",
    "    self.local_dim = local_dim\n",
    "    self.global_dim = global_dim\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.activation = activation\n",
    "    self.start_offset = start_offset\n",
    "    self.end_offset = end_offset\n",
    "    self.num_heads = n_heads\n",
    "    self.layer_norm00 = layers.LayerNormalization()\n",
    "    self.layer_norm01 = layers.LayerNormalization()\n",
    "    self.layer_norm1 = layers.LayerNormalization()\n",
    "    self.ffn = tf.keras.Sequential(\n",
    "          [\n",
    "            layers.Dense(self.local_dim//2, activation=self.activation,\n",
    "                        ),\n",
    "            layers.Dense(self.local_dim,\n",
    "                        activation=self.activation,\n",
    "                        ), ]\n",
    "      )\n",
    "    self.add0 = layers.Add()\n",
    "    self.add1 = layers.Add()\n",
    "    self.attention = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                               key_dim=self.local_dim,\n",
    "                                               attention_axes=ATTENTION_AXES)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    local_repr = self.layer_norm00(inputs[0])\n",
    "    global_repr = self.layer_norm01(inputs[1])\n",
    "    query = local_repr[:, self.start_offset:local_repr.shape[1]-self.end_offset, :]\n",
    "    key = global_repr\n",
    "    value = global_repr\n",
    "\n",
    "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
    "    attention_output = self.attention(\n",
    "        query, key, value\n",
    "    )\n",
    "    # Skip connection 1.\n",
    "    attention_output = self.add0([attention_output, query])\n",
    "\n",
    "    # Apply layer norm.\n",
    "    attention_output = self.layer_norm1(attention_output)\n",
    "    # Apply Feedforward network.\n",
    "    outputs = self.ffn(attention_output)\n",
    "    # Skip connection 2.\n",
    "    outputs = self.add1([outputs, attention_output])\n",
    "    return outputs\n",
    "\n",
    "class MaskedTransformerBlock(layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, attention_range, start_offset=0, end_offset=0, attn_block_repeats=1, activation=tf.nn.gelu, dropout_rate=0.1, use_ffn=True):\n",
    "    super(MaskedTransformerBlock, self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.ff_dim = ff_dim\n",
    "    self.start_offset = start_offset\n",
    "    self.end_offset = end_offset\n",
    "    self.attention_range = attention_range\n",
    "    self.attn_block_repeats = attn_block_repeats\n",
    "    self.activation = activation\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.use_ffn = use_ffn\n",
    "    self.att0 = [layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                           key_dim=self.embed_dim,\n",
    "                                           attention_axes=ATTENTION_AXES) for _ in range(attn_block_repeats)]\n",
    "    if self.use_ffn:\n",
    "      self.ffn = [tf.keras.Sequential(\n",
    "          [\n",
    "            layers.Dense(self.ff_dim, activation=self.activation,\n",
    "                        ),\n",
    "            layers.Dense(self.embed_dim,\n",
    "                        activation=self.activation,\n",
    "                        ), ]\n",
    "      ) for _ in range(attn_block_repeats)]\n",
    "    self.layer_norm0 = [layers.LayerNormalization() for _ in range(attn_block_repeats)]\n",
    "    self.layer_norm1 = [layers.LayerNormalization() for _ in range(attn_block_repeats)]\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    assert(self.end_offset >= 0)\n",
    "    self.feature_size = input_shape[1]\n",
    "    attention_mask = np.zeros((self.feature_size,\n",
    "                               self.feature_size), dtype=bool)\n",
    "    for i in range(self.start_offset, self.feature_size - self.end_offset):\n",
    "      attention_indices = np.arange(max(0, i-self.attention_range), min(self.feature_size, i+self.attention_range))\n",
    "      attention_mask[i, attention_indices] = True\n",
    "    self.attention_mask = tf.constant(attention_mask[self.start_offset:self.feature_size-self.end_offset])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "\n",
    "    x = inputs\n",
    "    for i in range(self.attn_block_repeats-1):\n",
    "      x = self.layer_norm0[i](x)\n",
    "      attn_output = self.att0[i](x, x)\n",
    "      out1 = x + attn_output\n",
    "      out1 = self.layer_norm1[i](out1)\n",
    "      if self.use_ffn:\n",
    "        ffn_output = self.ffn[i](out1)\n",
    "        x = out1 + ffn_output\n",
    "      else:\n",
    "        x = out1\n",
    "\n",
    "    x = self.layer_norm0[-1](inputs)\n",
    "    attn_output = self.att0[-1](x[:, self.start_offset:x.shape[1]-self.end_offset, :], x,\n",
    "                            )\n",
    "    out1 = x[:, self.start_offset:x.shape[1]-self.end_offset, :] + attn_output\n",
    "    out1 = self.layer_norm1[-1](out1)\n",
    "    if self.use_ffn:\n",
    "      ffn_output = self.ffn[-1](out1)\n",
    "      x = out1 + ffn_output\n",
    "    else:\n",
    "      x = out1\n",
    "    return x\n",
    "\n",
    "class GenoEmbeddings(layers.Layer):\n",
    "  def __init__(self, embedding_dim,\n",
    "               embeddings_initializer='glorot_uniform',\n",
    "               embeddings_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               embeddings_constraint=None):\n",
    "    super(GenoEmbeddings, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
    "    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
    "    self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "    self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # print(input_shape)\n",
    "\n",
    "    self.num_of_allels = input_shape[-1]\n",
    "    self.n_snps = input_shape[-2]\n",
    "    self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.n_snps, output_dim=self.embedding_dim\n",
    "        )\n",
    "    self.embedding = self.add_weight(\n",
    "            shape=(self.num_of_allels, self.embedding_dim),\n",
    "            initializer=self.embeddings_initializer,\n",
    "            trainable=True, name='geno_embeddings',\n",
    "            regularizer=self.embeddings_regularizer,\n",
    "            constraint=self.embeddings_constraint,\n",
    "            experimental_autocast=False\n",
    "        )\n",
    "    self.positions = tf.range(start=0, limit=self.n_snps, delta=1)\n",
    "  def call(self, inputs):\n",
    "    self.immediate_result = tf.einsum('ijk,kl->ijl', inputs, self.embedding)\n",
    "    return self.immediate_result + self.position_embedding(self.positions)\n",
    "\n",
    "\n",
    "class Chunker(layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, chk_size=chunk_size,\n",
    "               activation=tf.nn.gelu, dropout_rate=0.25, attn_block_repeats=1,\n",
    "               attention_range=attention_range, include_embedding_layer=False):\n",
    "    super(Chunker, self).__init__()\n",
    "    self.concat = layers.Concatenate(axis=-2)\n",
    "    self.chunk_size = chk_size\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.ff_dim = ff_dim\n",
    "    self.activation = activation\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.attention_range = attention_range\n",
    "    self.attn_block_repeats = attn_block_repeats\n",
    "    self.include_embedding_layer = include_embedding_layer\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.chunk_starts = list(range(0, input_shape[1], self.chunk_size))\n",
    "    self.chunk_ends = []\n",
    "    for cs in self.chunk_starts:\n",
    "      self.chunk_ends.append(min(cs+self.chunk_size, input_shape[1]))\n",
    "    self.mask_starts = [max(0, cs-self.attention_range) for cs in self.chunk_starts]\n",
    "    self.mask_ends = [min(ce+self.attention_range, input_shape[1]) for ce in self.chunk_ends]\n",
    "    self.chunkers = [SelfAttnChunk(self.embed_dim, self.num_heads, self.ff_dim,\n",
    "                           attention_range,\n",
    "                           include_embedding_layer=self.include_embedding_layer,\n",
    "                           start_offset=cs - self.mask_starts[i],\n",
    "                            end_offset=self.mask_ends[i]-self.chunk_ends[i],\n",
    "                           attn_block_repeats=self.attn_block_repeats) for i, cs in enumerate(self.chunk_starts)]\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = inputs\n",
    "    chunks = [chunker(x[:, self.mask_starts[i]:self.mask_ends[i]]) for i, chunker in enumerate(self.chunkers)]\n",
    "    y = self.concat(chunks)\n",
    "    return y\n",
    "\n",
    "\n",
    "class SelfAttnChunk(layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, attention_range,\n",
    "               start_offset=0, end_offset=0,\n",
    "               attn_block_repeats=1,\n",
    "               include_embedding_layer=False):\n",
    "    super(SelfAttnChunk, self).__init__()\n",
    "    self.attention_range = attention_range\n",
    "    self.ff_dim = ff_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.embed_dim = embed_dim\n",
    "    self.attn_block_repeats = attn_block_repeats\n",
    "    self.include_embedding_layer = include_embedding_layer\n",
    "\n",
    "    self.attention_block = MaskedTransformerBlock(self.embed_dim,\n",
    "                                                   self.num_heads, self.ff_dim,\n",
    "                                                   attention_range, start_offset,\n",
    "                                                   end_offset, attn_block_repeats=1)\n",
    "    if include_embedding_layer:\n",
    "      self.embedding = GenoEmbeddings(embed_dim)\n",
    "\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    pass\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    if self.include_embedding_layer:\n",
    "      x = self.embedding(inputs)\n",
    "    else:\n",
    "      x = inputs\n",
    "    x = self.attention_block(x)\n",
    "    return x\n",
    "\n",
    "class CrossAttnChunk(layers.Layer):\n",
    "  def __init__(self, start_offset=0, end_offset=0, n_heads = 8):\n",
    "    super(CrossAttnChunk, self).__init__()\n",
    "    self.attention_range = attention_range\n",
    "    self.start_offset = start_offset\n",
    "    self.end_offset = end_offset\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.local_dim = input_shape[0][-1]\n",
    "    self.global_dim = input_shape[1][-1]\n",
    "    self.attention_block = CrossAttentionLayer(self.local_dim, self.global_dim,\n",
    "                                              self.start_offset, self.end_offset,\n",
    "                                              n_heads=self.n_heads)\n",
    "    pass\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = inputs\n",
    "    x = self.attention_block(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqmHhclvOPiU"
   },
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134514242,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "tE4IWuS-N9cp"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "  def __init__(self, embed_dim):\n",
    "    super(ConvBlock, self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.const = None\n",
    "    self.conv000 = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,\n",
    "                    )\n",
    "    self.conv010 = layers.Conv1D(embed_dim, 5, padding='same', activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,\n",
    "                    )\n",
    "    self.conv011 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,\n",
    "                    )\n",
    "\n",
    "    self.conv020 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,\n",
    "                    )\n",
    "    self.conv021 = layers.Conv1D(embed_dim, 15, padding='same', activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,\n",
    "                    )\n",
    "    self.add = layers.Add()\n",
    "\n",
    "    self.conv100 = layers.Conv1D(embed_dim, 3, padding='same',\n",
    "                                 activation=tf.nn.gelu,\n",
    "                                 kernel_constraint=self.const,)\n",
    "    self.bn0 = layers.BatchNormalization()\n",
    "    self.bn1 = layers.BatchNormalization()\n",
    "    self.dw_conv = layers.DepthwiseConv1D(embed_dim, 1, padding='same')\n",
    "    self.activation = layers.Activation(tf.nn.gelu)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    # Could add skip connection here?\n",
    "    xa = self.conv000(inputs)\n",
    "\n",
    "    xb = self.conv010(xa)\n",
    "    xb = self.conv011(xb)\n",
    "\n",
    "    xc = self.conv020(xa)\n",
    "    xc = self.conv021(xc)\n",
    "\n",
    "    xa = self.add([xb, xc])\n",
    "    xa = self.conv100(xa)\n",
    "    xa = self.bn0(xa)\n",
    "    xa = self.dw_conv(xa)\n",
    "    xa = self.bn1(xa)\n",
    "    xa = self.activation(xa)\n",
    "    return xa\n",
    "\n",
    "def chunk_module(embed_dim, num_heads, input_len, input_channels, attention_range,\n",
    "               start_offset=0, end_offset=0,\n",
    "               attn_block_repeats=1, include_embedding=False):\n",
    "  projection_dim = embed_dim\n",
    "  inputs = layers.Input(shape=(input_len, embed_dim))\n",
    "  xa = inputs\n",
    "  xa0 = SelfAttnChunk(projection_dim, num_heads, projection_dim//2, attention_range,\n",
    "            start_offset, end_offset, 1, include_embedding_layer=False)(xa)\n",
    "\n",
    "  xa = ConvBlock(projection_dim)(xa0)\n",
    "  xa_skip = ConvBlock(projection_dim)(xa)\n",
    "\n",
    "  xa = layers.Dense(projection_dim, activation=tf.nn.gelu)(xa)\n",
    "  xa = ConvBlock(projection_dim)(xa)\n",
    "  xa = CrossAttnChunk(0, 0)([xa, xa0])\n",
    "  xa = layers.Dropout(0.25)(xa)\n",
    "  xa = ConvBlock(projection_dim)(xa)\n",
    "\n",
    "  xa = layers.Concatenate(axis=-1)([xa_skip, xa])\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=xa)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCkow4q5MJk_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1694134514242,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "vgGz9_6u8u-O"
   },
   "outputs": [],
   "source": [
    "class SplitTransformer(keras.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      embed_dim,\n",
    "      num_heads,\n",
    "      offset_before=0,\n",
    "      offset_after=0,\n",
    "      chunk_size=chunk_size,\n",
    "      activation=tf.nn.gelu,\n",
    "      dropout_rate=0.25,\n",
    "      attn_block_repeats=1,\n",
    "      attention_range=attention_range):\n",
    "    super(SplitTransformer, self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.chunk_size = chunk_size\n",
    "    self.activation = activation\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.attn_block_repeats = attn_block_repeats\n",
    "    self.attention_range = attention_range\n",
    "    self.offset_before = offset_before\n",
    "    self.offset_after = offset_after\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.seq_len = input_shape[1]\n",
    "    self.chunk_starts = list(range(0, input_shape[1], self.chunk_size))\n",
    "    self.chunk_ends = []\n",
    "    for cs in self.chunk_starts:\n",
    "      self.chunk_ends.append(min(cs+self.chunk_size, input_shape[1]))\n",
    "    self.mask_starts = [max(0, cs-self.attention_range) for cs in self.chunk_starts]\n",
    "    self.mask_ends = [min(ce+self.attention_range, input_shape[1]) for ce in self.chunk_ends]\n",
    "    self.chunkers = [chunk_module(self.embed_dim, self.num_heads,\n",
    "                                  self.mask_ends[i] - self.mask_starts[i],\n",
    "                                  inChannel, self.attention_range,\n",
    "                                  start_offset=cs - self.mask_starts[i],\n",
    "                                  end_offset=self.mask_ends[i]-self.chunk_ends[i],\n",
    "                                  attn_block_repeats=1, include_embedding=True) for i,cs in enumerate(self.chunk_starts)]\n",
    "\n",
    "    self.concat_layer = layers.Concatenate(axis=-2)\n",
    "    self.embedding = GenoEmbeddings(self.embed_dim)\n",
    "    self.slice_layer = layers.Lambda(lambda x: x[:, self.offset_before:self.seq_len-self.offset_after], name=\"output_slicer\")\n",
    "    self.after_concat_layer = layers.Conv1D(self.embed_dim//2, 5, padding='same', activation=tf.nn.gelu)\n",
    "    self.last_conv = layers.Conv1D(inChannel - 1, 5, padding='same', activation=tf.nn.softmax)\n",
    "    super(SplitTransformer, self).build(input_shape)\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.embedding(inputs)\n",
    "    chunks = [self.chunkers[i](x[:,\n",
    "                self.mask_starts[i]:self.mask_ends[i]]) for i, chunker\\\n",
    "                                                    in enumerate(self.chunkers)]\n",
    "    x = self.concat_layer(chunks)\n",
    "    x = self.after_concat_layer(x)\n",
    "    x = self.last_conv(x)\n",
    "    x = self.slice_layer(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134514243,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "69stM-Pzhskh"
   },
   "outputs": [],
   "source": [
    "class MyCustomLoss(tf.keras.losses.Loss):\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    cat_loss = loss_obj(y_true, y_pred)\n",
    "\n",
    "    loss_obj = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    kl_loss = loss_obj(y_true, y_pred)\n",
    "\n",
    "    return cat_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1694134514243,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "vcbAoT90Ihvc"
   },
   "outputs": [],
   "source": [
    "def create_model(offset_before=0, offset_after=0):\n",
    "  model =  SplitTransformer(embed_dim=128,\n",
    "      num_heads=16,\n",
    "      attn_block_repeats=1,\n",
    "      chunk_size=chunk_size,\n",
    "      activation=\"gelu\",\n",
    "      offset_before=offset_before,\n",
    "      offset_after=offset_after)\n",
    "  optimizer = tfa.optimizers.LAMB(learning_rate=learning_rate)\n",
    "  model.compile(optimizer, loss=MyCustomLoss(), metrics=tf.keras.metrics.CategoricalAccuracy())\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54110,
     "status": "ok",
     "timestamp": 1694134568345,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "D0Py1VVVTfdf",
    "outputId": "fffa8ba4-39b3-4f1b-9c8c-bb9915c61614"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"split_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_4 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_5 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_6 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_7 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_8 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_9 (Functional)        (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_10 (Functional)       (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_11 (Functional)       (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_12 (Functional)       (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_13 (Functional)       (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_14 (Functional)       (None, 2000, 256)         4327936   \n",
      "                                                                 \n",
      " model_15 (Functional)       (None, 1143, 256)         4327936   \n",
      "                                                                 \n",
      " concatenate_16 (Concatenate  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " geno_embeddings (GenoEmbedd  multiple                 3986688   \n",
      " ings)                                                           \n",
      "                                                                 \n",
      " output_slicer (Lambda)      multiple                  0         \n",
      "                                                                 \n",
      " conv1d_384 (Conv1D)         multiple                  81984     \n",
      "                                                                 \n",
      " conv1d_385 (Conv1D)         multiple                  642       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,316,290\n",
      "Trainable params: 73,283,522\n",
      "Non-trainable params: 32,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(offset_before=2*attention_range, offset_after=2*attention_range)\n",
    "model.build((1, max_features_len_per_model, inChannel))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1694134568345,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "61EgnuNmoFLT"
   },
   "outputs": [],
   "source": [
    "METRIC = \"val_loss\"\n",
    "\n",
    "def create_callbacks(kfold=0, metric = METRIC):\n",
    "    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor= metric,\n",
    "        mode='auto',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor= metric,\n",
    "        mode='auto',\n",
    "        patience= 10,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "                 reducelr,\n",
    "                 earlystop\n",
    "                 ]\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOA_NexzN5Qq"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1694134568506,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "JW5woVoYH6ie"
   },
   "outputs": [],
   "source": [
    "save_dir = \"[save_path]\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694134568506,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "Ey5lpPElXlUd",
    "outputId": "e7ea9698-efdd-468b-f993-7cf07054289d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# A TPU V3-8 has 8 computing cores, the global batch size will be 1/16 x 8 = 8/128\n",
    "BATCH_SIZE_BASE = 2\n",
    "# Training configuration\n",
    "BATCH_SIZE = BATCH_SIZE_BASE * N_REPLICAS if TPU else 32\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1694134568506,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     },
     "user_tz": 240
    },
    "id": "0Za5CpvwqIkk",
    "outputId": "59966850-38a3-49cf-8d88-0edef9ef1969"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31143"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "dl.VARIANT_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timing test\n",
    "\n",
    "The first time it tries to predict, takes longer (maybe because it tries to re-build the model on the local machine) but subsequent calls are much faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 128 - 32\n",
    "NUM_EPOCHS = 1000\n",
    "results = None\n",
    "x_train_indices, x_valid_indices = train_test_split(range(dl.get_ref_set(0, 1).shape[0]),\n",
    "                                                    test_size=0.10,\n",
    "                                                      random_state=2022,\n",
    "                                                      shuffle=True,)\n",
    "steps_per_epoch = len(x_train_indices)//BATCH_SIZE\n",
    "validation_steps = len(x_valid_indices)//BATCH_SIZE\n",
    "\n",
    "break_points = list(np.arange(0, dl.VARIANT_COUNT, max_features_len_per_model)) + [dl.VARIANT_COUNT]\n",
    "\n",
    "for w in range(len(break_points)-1):\n",
    "  print(f\"Doing part {w} out of {len(break_points)-2}\")\n",
    "  final_start_pos = max(0, break_points[w]-2*attention_range)\n",
    "  final_end_pos = min(dl.VARIANT_COUNT, break_points[w+1]+2*attention_range)\n",
    "  offset_before = break_points[w] - final_start_pos\n",
    "  offset_after = final_end_pos - break_points[w+1]\n",
    "  ref_set = dl.get_ref_set(final_start_pos, final_end_pos).astype(np.int32)\n",
    "  print(f\"Data shape: {ref_set.shape}\")\n",
    "  train_dataset = get_dataset(ref_set[x_train_indices], BATCH_SIZE,\n",
    "                              offset_before=offset_before,\n",
    "                              offset_after=offset_after)\n",
    "  valid_dataset = get_dataset(ref_set[x_valid_indices], BATCH_SIZE,\n",
    "                              offset_before=offset_before,\n",
    "                              offset_after=offset_after, training=False)\n",
    "  del ref_set\n",
    "  K.clear_session()\n",
    "  tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "  callbacks = create_callbacks()\n",
    "  with strategy.scope():\n",
    "    model = create_model(offset_before=offset_before,\n",
    "                              offset_after=offset_after)\n",
    "    history = model.fit(train_dataset, steps_per_epoch=steps_per_epoch, epochs=1,\n",
    "            validation_data=valid_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks, verbose=1)\n",
    "\n",
    "  save_name = save_dir + f\"beadchip_hmr_probs_window_{w+1}\"\n",
    "  test_dataset_np = dl.get_target_set(final_start_pos, final_end_pos).astype(np.int32)\n",
    "  test_dataset = get_test_dataset(test_dataset_np, BATCH_SIZE*4)\n",
    "  predict_onehot = model.predict(test_dataset, verbose=1)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgaklj8yRECE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694136116782,
     "user_tz": 240,
     "elapsed": 1548279,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "3b93a03d-a4b2-4852-d006-0a5bba6d371e"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doing part 0 out of 0\n",
      "Data shape: (4808, 31143)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.28.108.50:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "270/270 [==============================] - 1326s 1s/step - loss: 33584.8008 - categorical_accuracy: 0.8821 - val_loss: 75134.7188 - val_categorical_accuracy: 0.7122 - lr: 0.0010\n",
      "4/4 [==============================] - 129s 36s/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predict_onehot = model.predict(test_dataset, verbose=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4pEkU7mXxsL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694136123558,
     "user_tz": 240,
     "elapsed": 5707,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    },
    "outputId": "7ef70864-0d74-4cf2-a902-9d296cdc1c62"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4/4 [==============================] - 5s 956ms/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 128 - 32\n",
    "NUM_EPOCHS = 1000\n",
    "results = None\n",
    "x_train_indices, x_valid_indices = train_test_split(range(dl.get_ref_set(0, 1).shape[0]), test_size=0.10,\n",
    "                                      random_state=2022,\n",
    "                                      shuffle=True,)\n",
    "steps_per_epoch = len(x_train_indices)//BATCH_SIZE\n",
    "validation_steps = len(x_valid_indices)//BATCH_SIZE\n",
    "\n",
    "break_points = list(np.arange(0, dl.VARIANT_COUNT, max_features_len_per_model)) + [dl.VARIANT_COUNT]\n",
    "\n",
    "for w in range(len(break_points)-1):\n",
    "  print(f\"Doing part {w} out of {len(break_points)-2}\")\n",
    "  final_start_pos = max(0, break_points[w]-2*attention_range)\n",
    "  final_end_pos = min(dl.VARIANT_COUNT, break_points[w+1]+2*attention_range)\n",
    "  offset_before = break_points[w] - final_start_pos\n",
    "  offset_after = final_end_pos - break_points[w+1]\n",
    "  ref_set = dl.get_ref_set(final_start_pos, final_end_pos).astype(np.int32)\n",
    "  print(f\"Data shape: {ref_set.shape}\")\n",
    "  train_dataset = get_dataset(ref_set[x_train_indices], BATCH_SIZE,\n",
    "                              offset_before=offset_before,\n",
    "                              offset_after=offset_after)\n",
    "  valid_dataset = get_dataset(ref_set[x_valid_indices], BATCH_SIZE,\n",
    "                              offset_before=offset_before,\n",
    "                              offset_after=offset_after, training=False)\n",
    "  del ref_set\n",
    "  K.clear_session()\n",
    "  tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "  callbacks = create_callbacks()\n",
    "  with strategy.scope():\n",
    "    model = create_model(offset_before=offset_before,\n",
    "                              offset_after=offset_after)\n",
    "    history = model.fit(train_dataset, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCHS,\n",
    "            validation_data=valid_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks, verbose=1)\n",
    "\n",
    "  save_name = save_dir + f\"beadchip_hmr_probs_window_{w+1}\"\n",
    "  test_dataset_np = dl.get_target_set(final_start_pos, final_end_pos).astype(np.int32)\n",
    "  test_dataset = get_test_dataset(test_dataset_np, BATCH_SIZE)\n",
    "  predict_onehot = model.predict(test_dataset, verbose=1)\n",
    "  predict_onehot = predict_onehot.astype(np.float32)\n",
    "\n",
    "  # test_X_missing = to_categorical(dl.get_target_set(break_points[w], break_points[w+1]).astype(np.int32), dl.SEQ_DEPTH)\n",
    "  # predict_onehot = np.empty((test_X_missing.shape[0], test_X_missing.shape[1], dl.SEQ_DEPTH-1), dtype=np.float32)\n",
    "  # for i in tqdm(range(len(test_X_missing))):\n",
    "  #   predict_onehot[i] = model.predict(test_X_missing[i:i+1], verbose=0)\n",
    "  np.save(save_name, predict_onehot)\n",
    "  print(\"=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l-tqJSNxFOb",
    "outputId": "88ba8421-d8f6-4b09-9234-089feee082e4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1690430297032,
     "user_tz": 240,
     "elapsed": 42455522,
     "user": {
      "displayName": "Erfan Mowlaei",
      "userId": "07890245825539195560"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doing part 0 out of 0\n",
      "Data shape: (4808, 31143)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.1.146.170:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "270/270 [==============================] - 1380s 1s/step - loss: 33392.7969 - categorical_accuracy: 0.8830 - val_loss: 77765.2109 - val_categorical_accuracy: 0.7139 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 18497.0625 - categorical_accuracy: 0.9421 - val_loss: 39398.5547 - val_categorical_accuracy: 0.8948 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 16117.8652 - categorical_accuracy: 0.9501 - val_loss: 18057.4473 - val_categorical_accuracy: 0.9441 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 14926.0391 - categorical_accuracy: 0.9539 - val_loss: 16301.0518 - val_categorical_accuracy: 0.9503 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "270/270 [==============================] - 295s 1s/step - loss: 14165.7686 - categorical_accuracy: 0.9564 - val_loss: 14763.2979 - val_categorical_accuracy: 0.9549 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 13570.7256 - categorical_accuracy: 0.9583 - val_loss: 14325.2598 - val_categorical_accuracy: 0.9562 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 13147.1104 - categorical_accuracy: 0.9597 - val_loss: 14339.8555 - val_categorical_accuracy: 0.9562 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "270/270 [==============================] - 295s 1s/step - loss: 12797.5762 - categorical_accuracy: 0.9608 - val_loss: 13812.7998 - val_categorical_accuracy: 0.9575 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 12513.9268 - categorical_accuracy: 0.9616 - val_loss: 12862.2812 - val_categorical_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 12285.4590 - categorical_accuracy: 0.9623 - val_loss: 12854.3477 - val_categorical_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 12078.5547 - categorical_accuracy: 0.9629 - val_loss: 12502.4199 - val_categorical_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 11900.8027 - categorical_accuracy: 0.9635 - val_loss: 12255.7363 - val_categorical_accuracy: 0.9626 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 11734.7227 - categorical_accuracy: 0.9640 - val_loss: 12128.6387 - val_categorical_accuracy: 0.9630 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 11570.3125 - categorical_accuracy: 0.9645 - val_loss: 12085.1240 - val_categorical_accuracy: 0.9630 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 11478.4678 - categorical_accuracy: 0.9648 - val_loss: 11777.4307 - val_categorical_accuracy: 0.9640 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 11357.5303 - categorical_accuracy: 0.9651 - val_loss: 11518.9717 - val_categorical_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 11247.2197 - categorical_accuracy: 0.9654 - val_loss: 11658.8730 - val_categorical_accuracy: 0.9644 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 11145.2285 - categorical_accuracy: 0.9657 - val_loss: 11542.3955 - val_categorical_accuracy: 0.9647 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 11057.3848 - categorical_accuracy: 0.9660 - val_loss: 11373.5312 - val_categorical_accuracy: 0.9651 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "270/270 [==============================] - 287s 1s/step - loss: 10948.0078 - categorical_accuracy: 0.9664 - val_loss: 11380.5869 - val_categorical_accuracy: 0.9652 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 10901.6240 - categorical_accuracy: 0.9665 - val_loss: 11209.2744 - val_categorical_accuracy: 0.9658 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "270/270 [==============================] - 287s 1s/step - loss: 10835.0869 - categorical_accuracy: 0.9667 - val_loss: 11250.9893 - val_categorical_accuracy: 0.9656 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "270/270 [==============================] - 301s 1s/step - loss: 10760.1348 - categorical_accuracy: 0.9669 - val_loss: 11099.8223 - val_categorical_accuracy: 0.9662 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 10682.5742 - categorical_accuracy: 0.9672 - val_loss: 11063.0400 - val_categorical_accuracy: 0.9662 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 10605.3271 - categorical_accuracy: 0.9674 - val_loss: 10936.7607 - val_categorical_accuracy: 0.9666 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "270/270 [==============================] - 295s 1s/step - loss: 10551.5801 - categorical_accuracy: 0.9675 - val_loss: 10918.7568 - val_categorical_accuracy: 0.9666 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 10509.5059 - categorical_accuracy: 0.9677 - val_loss: 10881.9434 - val_categorical_accuracy: 0.9668 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10426.3057 - categorical_accuracy: 0.9679 - val_loss: 10737.4463 - val_categorical_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10407.9893 - categorical_accuracy: 0.9680 - val_loss: 10710.2090 - val_categorical_accuracy: 0.9673 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 10352.6006 - categorical_accuracy: 0.9681 - val_loss: 10712.6885 - val_categorical_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10280.8311 - categorical_accuracy: 0.9683 - val_loss: 10616.3076 - val_categorical_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10259.7852 - categorical_accuracy: 0.9684 - val_loss: 10611.9678 - val_categorical_accuracy: 0.9676 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10171.3125 - categorical_accuracy: 0.9686 - val_loss: 10473.9014 - val_categorical_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10116.8662 - categorical_accuracy: 0.9688 - val_loss: 10466.3076 - val_categorical_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10089.9951 - categorical_accuracy: 0.9689 - val_loss: 10399.9697 - val_categorical_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 10070.0762 - categorical_accuracy: 0.9690 - val_loss: 10397.2832 - val_categorical_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9949.9980 - categorical_accuracy: 0.9693 - val_loss: 10316.1514 - val_categorical_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 9966.3633 - categorical_accuracy: 0.9693 - val_loss: 10283.8887 - val_categorical_accuracy: 0.9685 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9880.6016 - categorical_accuracy: 0.9695 - val_loss: 10246.0186 - val_categorical_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "270/270 [==============================] - 300s 1s/step - loss: 9868.6113 - categorical_accuracy: 0.9696 - val_loss: 10213.7607 - val_categorical_accuracy: 0.9688 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9802.8916 - categorical_accuracy: 0.9698 - val_loss: 10165.9365 - val_categorical_accuracy: 0.9690 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9730.1094 - categorical_accuracy: 0.9700 - val_loss: 10090.2910 - val_categorical_accuracy: 0.9691 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 9721.6602 - categorical_accuracy: 0.9700 - val_loss: 10105.4365 - val_categorical_accuracy: 0.9692 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9664.2051 - categorical_accuracy: 0.9702 - val_loss: 9963.2031 - val_categorical_accuracy: 0.9695 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9629.2158 - categorical_accuracy: 0.9703 - val_loss: 9914.9697 - val_categorical_accuracy: 0.9697 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 9573.6230 - categorical_accuracy: 0.9705 - val_loss: 9924.2275 - val_categorical_accuracy: 0.9696 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9527.3506 - categorical_accuracy: 0.9706 - val_loss: 9868.6104 - val_categorical_accuracy: 0.9700 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9468.7861 - categorical_accuracy: 0.9708 - val_loss: 9857.0273 - val_categorical_accuracy: 0.9699 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 9430.0674 - categorical_accuracy: 0.9709 - val_loss: 9796.7568 - val_categorical_accuracy: 0.9700 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 9390.6045 - categorical_accuracy: 0.9710 - val_loss: 9736.4668 - val_categorical_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 9357.2119 - categorical_accuracy: 0.9711 - val_loss: 9752.4834 - val_categorical_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9312.7832 - categorical_accuracy: 0.9712 - val_loss: 9697.1162 - val_categorical_accuracy: 0.9704 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9269.6611 - categorical_accuracy: 0.9714 - val_loss: 9645.4258 - val_categorical_accuracy: 0.9705 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 9257.4189 - categorical_accuracy: 0.9714 - val_loss: 9638.6592 - val_categorical_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 9200.0869 - categorical_accuracy: 0.9716 - val_loss: 9614.2812 - val_categorical_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 9137.1377 - categorical_accuracy: 0.9718 - val_loss: 9639.0381 - val_categorical_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 9145.1006 - categorical_accuracy: 0.9718 - val_loss: 9571.4883 - val_categorical_accuracy: 0.9708 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9096.6172 - categorical_accuracy: 0.9719 - val_loss: 9528.1631 - val_categorical_accuracy: 0.9708 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9071.5205 - categorical_accuracy: 0.9720 - val_loss: 9463.1523 - val_categorical_accuracy: 0.9712 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9052.6914 - categorical_accuracy: 0.9720 - val_loss: 9458.7061 - val_categorical_accuracy: 0.9710 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 9019.6396 - categorical_accuracy: 0.9721 - val_loss: 9435.1289 - val_categorical_accuracy: 0.9712 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 8986.4766 - categorical_accuracy: 0.9723 - val_loss: 9375.1963 - val_categorical_accuracy: 0.9714 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8952.2354 - categorical_accuracy: 0.9724 - val_loss: 9409.8564 - val_categorical_accuracy: 0.9713 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8915.0146 - categorical_accuracy: 0.9725 - val_loss: 9363.5732 - val_categorical_accuracy: 0.9715 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8878.3789 - categorical_accuracy: 0.9726 - val_loss: 9312.2490 - val_categorical_accuracy: 0.9716 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8887.2686 - categorical_accuracy: 0.9726 - val_loss: 9351.7998 - val_categorical_accuracy: 0.9715 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8850.5957 - categorical_accuracy: 0.9727 - val_loss: 9277.9873 - val_categorical_accuracy: 0.9717 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8838.8047 - categorical_accuracy: 0.9727 - val_loss: 9288.2783 - val_categorical_accuracy: 0.9717 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8788.7617 - categorical_accuracy: 0.9729 - val_loss: 9245.7451 - val_categorical_accuracy: 0.9718 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8749.2109 - categorical_accuracy: 0.9730 - val_loss: 9259.0254 - val_categorical_accuracy: 0.9718 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 8769.4180 - categorical_accuracy: 0.9729 - val_loss: 9186.2598 - val_categorical_accuracy: 0.9720 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8712.7402 - categorical_accuracy: 0.9731 - val_loss: 9202.6650 - val_categorical_accuracy: 0.9720 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 8722.1836 - categorical_accuracy: 0.9731 - val_loss: 9145.5615 - val_categorical_accuracy: 0.9721 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8656.2949 - categorical_accuracy: 0.9733 - val_loss: 9174.3486 - val_categorical_accuracy: 0.9720 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8670.4922 - categorical_accuracy: 0.9732 - val_loss: 9173.1436 - val_categorical_accuracy: 0.9721 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8639.1436 - categorical_accuracy: 0.9733 - val_loss: 9141.8174 - val_categorical_accuracy: 0.9722 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8612.3506 - categorical_accuracy: 0.9734 - val_loss: 9104.1191 - val_categorical_accuracy: 0.9723 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8574.7295 - categorical_accuracy: 0.9735 - val_loss: 9094.3477 - val_categorical_accuracy: 0.9723 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8592.5078 - categorical_accuracy: 0.9735 - val_loss: 9092.9268 - val_categorical_accuracy: 0.9724 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8534.4248 - categorical_accuracy: 0.9737 - val_loss: 9076.6768 - val_categorical_accuracy: 0.9725 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 8550.3281 - categorical_accuracy: 0.9736 - val_loss: 9044.0488 - val_categorical_accuracy: 0.9725 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8543.2656 - categorical_accuracy: 0.9736 - val_loss: 9040.6592 - val_categorical_accuracy: 0.9724 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 8478.2803 - categorical_accuracy: 0.9738 - val_loss: 9036.9111 - val_categorical_accuracy: 0.9725 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 8502.2168 - categorical_accuracy: 0.9737 - val_loss: 8981.4434 - val_categorical_accuracy: 0.9728 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "270/270 [==============================] - 287s 1s/step - loss: 8444.8496 - categorical_accuracy: 0.9739 - val_loss: 9010.2480 - val_categorical_accuracy: 0.9726 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8465.9404 - categorical_accuracy: 0.9739 - val_loss: 8994.6094 - val_categorical_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "270/270 [==============================] - 299s 1s/step - loss: 8433.5127 - categorical_accuracy: 0.9739 - val_loss: 8979.6182 - val_categorical_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8403.8564 - categorical_accuracy: 0.9740 - val_loss: 8945.5977 - val_categorical_accuracy: 0.9728 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "270/270 [==============================] - 301s 1s/step - loss: 8389.0781 - categorical_accuracy: 0.9741 - val_loss: 8942.4570 - val_categorical_accuracy: 0.9728 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8391.1660 - categorical_accuracy: 0.9741 - val_loss: 8937.2705 - val_categorical_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8352.3770 - categorical_accuracy: 0.9742 - val_loss: 8924.3193 - val_categorical_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8373.1299 - categorical_accuracy: 0.9741 - val_loss: 8887.4814 - val_categorical_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 8311.9141 - categorical_accuracy: 0.9743 - val_loss: 8868.2256 - val_categorical_accuracy: 0.9731 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 8323.3438 - categorical_accuracy: 0.9743 - val_loss: 8917.7920 - val_categorical_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 8294.5303 - categorical_accuracy: 0.9744 - val_loss: 8913.1406 - val_categorical_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 8296.9375 - categorical_accuracy: 0.9744 - val_loss: 8879.1631 - val_categorical_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 8033.4990 - categorical_accuracy: 0.9752 - val_loss: 8563.3799 - val_categorical_accuracy: 0.9740 - lr: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7935.9194 - categorical_accuracy: 0.9754 - val_loss: 8518.0635 - val_categorical_accuracy: 0.9742 - lr: 5.0000e-04\n",
      "Epoch 99/1000\n",
      "270/270 [==============================] - 295s 1s/step - loss: 7907.2046 - categorical_accuracy: 0.9755 - val_loss: 8545.8096 - val_categorical_accuracy: 0.9741 - lr: 5.0000e-04\n",
      "Epoch 100/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7860.8979 - categorical_accuracy: 0.9757 - val_loss: 8526.4287 - val_categorical_accuracy: 0.9742 - lr: 5.0000e-04\n",
      "Epoch 101/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7857.6353 - categorical_accuracy: 0.9756 - val_loss: 8480.2764 - val_categorical_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 102/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7822.6777 - categorical_accuracy: 0.9758 - val_loss: 8481.4980 - val_categorical_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 103/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7804.5483 - categorical_accuracy: 0.9758 - val_loss: 8489.7490 - val_categorical_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 104/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7800.5093 - categorical_accuracy: 0.9758 - val_loss: 8494.1064 - val_categorical_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 105/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 7704.5869 - categorical_accuracy: 0.9761 - val_loss: 8314.3662 - val_categorical_accuracy: 0.9747 - lr: 2.5000e-04\n",
      "Epoch 106/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7564.1504 - categorical_accuracy: 0.9766 - val_loss: 8338.4141 - val_categorical_accuracy: 0.9747 - lr: 2.5000e-04\n",
      "Epoch 107/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 7598.3818 - categorical_accuracy: 0.9764 - val_loss: 8306.5752 - val_categorical_accuracy: 0.9748 - lr: 2.5000e-04\n",
      "Epoch 108/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7582.7871 - categorical_accuracy: 0.9765 - val_loss: 8270.4355 - val_categorical_accuracy: 0.9750 - lr: 2.5000e-04\n",
      "Epoch 109/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7575.4194 - categorical_accuracy: 0.9765 - val_loss: 8312.8105 - val_categorical_accuracy: 0.9748 - lr: 2.5000e-04\n",
      "Epoch 110/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 7544.0894 - categorical_accuracy: 0.9766 - val_loss: 8270.3086 - val_categorical_accuracy: 0.9749 - lr: 2.5000e-04\n",
      "Epoch 111/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7532.8115 - categorical_accuracy: 0.9766 - val_loss: 8311.5195 - val_categorical_accuracy: 0.9748 - lr: 2.5000e-04\n",
      "Epoch 112/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7512.0132 - categorical_accuracy: 0.9767 - val_loss: 8327.1035 - val_categorical_accuracy: 0.9748 - lr: 2.5000e-04\n",
      "Epoch 113/1000\n",
      "270/270 [==============================] - 287s 1s/step - loss: 7499.0801 - categorical_accuracy: 0.9767 - val_loss: 8294.3721 - val_categorical_accuracy: 0.9749 - lr: 2.5000e-04\n",
      "Epoch 114/1000\n",
      "270/270 [==============================] - 295s 1s/step - loss: 7459.5132 - categorical_accuracy: 0.9769 - val_loss: 8228.8564 - val_categorical_accuracy: 0.9751 - lr: 1.2500e-04\n",
      "Epoch 115/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7425.8467 - categorical_accuracy: 0.9769 - val_loss: 8203.7949 - val_categorical_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 116/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7405.9746 - categorical_accuracy: 0.9770 - val_loss: 8214.8574 - val_categorical_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 117/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7400.4341 - categorical_accuracy: 0.9770 - val_loss: 8183.7002 - val_categorical_accuracy: 0.9753 - lr: 1.2500e-04\n",
      "Epoch 118/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7392.5547 - categorical_accuracy: 0.9770 - val_loss: 8186.4575 - val_categorical_accuracy: 0.9753 - lr: 1.2500e-04\n",
      "Epoch 119/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7406.9600 - categorical_accuracy: 0.9770 - val_loss: 8201.9648 - val_categorical_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 120/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7367.8491 - categorical_accuracy: 0.9771 - val_loss: 8193.5625 - val_categorical_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 121/1000\n",
      "270/270 [==============================] - 296s 1s/step - loss: 7339.5078 - categorical_accuracy: 0.9772 - val_loss: 8136.3159 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 122/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7302.9097 - categorical_accuracy: 0.9773 - val_loss: 8159.1519 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 123/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7320.3516 - categorical_accuracy: 0.9773 - val_loss: 8134.5146 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 124/1000\n",
      "270/270 [==============================] - 287s 1s/step - loss: 7318.5640 - categorical_accuracy: 0.9773 - val_loss: 8161.7314 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 125/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7293.0376 - categorical_accuracy: 0.9773 - val_loss: 8164.2104 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 126/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7333.3818 - categorical_accuracy: 0.9772 - val_loss: 8139.2280 - val_categorical_accuracy: 0.9754 - lr: 6.2500e-05\n",
      "Epoch 127/1000\n",
      "270/270 [==============================] - 297s 1s/step - loss: 7286.9478 - categorical_accuracy: 0.9773 - val_loss: 8098.3657 - val_categorical_accuracy: 0.9755 - lr: 3.1250e-05\n",
      "Epoch 128/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7274.2100 - categorical_accuracy: 0.9774 - val_loss: 8144.1729 - val_categorical_accuracy: 0.9755 - lr: 3.1250e-05\n",
      "Epoch 129/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7307.1353 - categorical_accuracy: 0.9773 - val_loss: 8120.3291 - val_categorical_accuracy: 0.9754 - lr: 3.1250e-05\n",
      "Epoch 130/1000\n",
      "270/270 [==============================] - 298s 1s/step - loss: 7262.1260 - categorical_accuracy: 0.9774 - val_loss: 8089.4390 - val_categorical_accuracy: 0.9756 - lr: 3.1250e-05\n",
      "Epoch 131/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7274.0620 - categorical_accuracy: 0.9774 - val_loss: 8126.9443 - val_categorical_accuracy: 0.9754 - lr: 3.1250e-05\n",
      "Epoch 132/1000\n",
      "270/270 [==============================] - 289s 1s/step - loss: 7301.4492 - categorical_accuracy: 0.9773 - val_loss: 8118.6226 - val_categorical_accuracy: 0.9754 - lr: 3.1250e-05\n",
      "Epoch 133/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7257.6421 - categorical_accuracy: 0.9775 - val_loss: 8149.1353 - val_categorical_accuracy: 0.9754 - lr: 3.1250e-05\n",
      "Epoch 134/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7266.2368 - categorical_accuracy: 0.9774 - val_loss: 8124.7676 - val_categorical_accuracy: 0.9754 - lr: 1.5625e-05\n",
      "Epoch 135/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7250.6084 - categorical_accuracy: 0.9775 - val_loss: 8139.2407 - val_categorical_accuracy: 0.9755 - lr: 1.5625e-05\n",
      "Epoch 136/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7303.8979 - categorical_accuracy: 0.9773 - val_loss: 8119.9160 - val_categorical_accuracy: 0.9755 - lr: 1.5625e-05\n",
      "Epoch 137/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7226.9033 - categorical_accuracy: 0.9775 - val_loss: 8110.5020 - val_categorical_accuracy: 0.9755 - lr: 7.8125e-06\n",
      "Epoch 138/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7266.6860 - categorical_accuracy: 0.9774 - val_loss: 8130.0381 - val_categorical_accuracy: 0.9755 - lr: 7.8125e-06\n",
      "Epoch 139/1000\n",
      "270/270 [==============================] - 288s 1s/step - loss: 7257.4434 - categorical_accuracy: 0.9774 - val_loss: 8131.3486 - val_categorical_accuracy: 0.9754 - lr: 7.8125e-06\n",
      "Epoch 140/1000\n",
      "270/270 [==============================] - ETA: 0s - loss: 7237.3760 - categorical_accuracy: 0.9775Restoring model weights from the end of the best epoch: 130.\n",
      "270/270 [==============================] - 325s 1s/step - loss: 7237.3760 - categorical_accuracy: 0.9775 - val_loss: 8109.1963 - val_categorical_accuracy: 0.9755 - lr: 3.9063e-06\n",
      "Epoch 140: early stopping\n",
      "13/13 [==============================] - 78s 4s/step\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFkkfi8QGMzi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1690430297033,
     "user_tz": 240,
     "elapsed": 25,
     "user": {
      "displayName": "Erfan Mowlaei",
      "userId": "07890245825539195560"
     }
    },
    "outputId": "72ae3470-69f4-4c26-8c00-13fdac08383a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: '0|0', 1: '0|1', 2: '1|0', 3: '1|1', 4: '.|.'}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "dl.reverse_replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xQ7TbkVjCnw8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1694136146481,
     "user_tz": 240,
     "elapsed": 289,
     "user": {
      "displayName": "net crowmaster",
      "userId": "14126946344391667924"
     }
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTVTcGHXJFrW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1lo9cmptjRRESsVlVECvlJFHbLeaygq2s",
     "timestamp": 1640128751823
    },
    {
     "file_id": "https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb",
     "timestamp": 1621552889682
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
