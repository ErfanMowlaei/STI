{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14597,"status":"ok","timestamp":1672338950164,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"doSuiA3dyTyg","outputId":"12acc4d7-c382-4049-e5ce-473e0c6cee3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11867,"status":"ok","timestamp":1672338962028,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"liJJGzQp4qzO","outputId":"4d1ccd6d-5617-449c-844f-cd6a6781b15f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.19.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (0.12.0)\n","Collecting scikit-allel\n","  Downloading scikit_allel-1.3.5-cp38-cp38-manylinux2010_x86_64.whl (7.2 MB)\n","\u001b[K     |████████████████████████████████| 7.2 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from scikit-allel) (1.21.6)\n","Requirement already satisfied: dask[array] in /usr/local/lib/python3.8/dist-packages (from scikit-allel) (2022.2.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (21.3)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (6.0)\n","Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (1.5.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (2022.11.0)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[array]->scikit-allel) (1.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->dask[array]->scikit-allel) (3.0.9)\n","Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[array]->scikit-allel) (1.0.0)\n","Installing collected packages: scikit-allel\n","Successfully installed scikit-allel-1.3.5\n"]}],"source":["!pip install tensorflow-addons\n","!pip install pyyaml h5py\n","!pip install toolz scikit-allel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2653,"status":"ok","timestamp":1672338964678,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"ZWzi3Z3L5RO2","outputId":"20b0faa5-9518-4bb1-c650-149c0c6ef5c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Tensorflow version 2.9.2\n"]}],"source":["import numpy as np\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"RjGOO5PdFPf7"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3546,"status":"ok","timestamp":1672338973190,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"odmhCqSVFPf8","outputId":"73165cc3-bd01-43d3-e95c-4f3dff5b6aa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version 2.9.2\n"]}],"source":["import os\n","# os.environ[\"MODIN_CPUS\"] = \"8\"\n","# from distributed import Client\n","# client = Client()\n","import numpy as np\n","import math\n","import re\n","import itertools\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_addons as tfa\n","from sklearn import metrics\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import constraints\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.applications import efficientnet as efn\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from tensorflow.keras.constraints import Constraint\n","# import allel\n","from scipy.spatial.distance import squareform\n","%matplotlib inline   \n","from toolz import interleave\n","from tqdm import tqdm\n","import allel\n","from scipy.spatial.distance import squareform\n","from matplotlib import pyplot as plt\n","import tensorflow_datasets as tfds\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LassoCV, ElasticNetCV\n","from sklearn.model_selection import KFold,StratifiedKFold\n","\n","print(\"Tensorflow version \" + tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"SLd26RspFhaS"},"source":["## Hardware Config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672338973190,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"SLd7mAFgFUnR","outputId":"10a20943-c597-4b54-925d-5a333f707ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on GPU\n","N_REPLICAS: 1\n"]}],"source":["# Detect hardware, return appropriate distribution strategy\n","try:\n","    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","    print('Running on TPU ', TPU.master())\n","except ValueError:\n","    print('Running on GPU')\n","    TPU = None\n","\n","if TPU:\n","    tf.config.experimental_connect_to_cluster(TPU)\n","    tf.tpu.experimental.initialize_tpu_system(TPU)\n","    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n","else:\n","    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","N_REPLICAS = strategy.num_replicas_in_sync\n","# Number of computing cores, is 8 for a TPU V3-8\n","print(f'N_REPLICAS: {N_REPLICAS}')"]},{"cell_type":"markdown","metadata":{"id":"A77GFE3xFPf8"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3zy8i_8FPf_","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1672344974719,"user_tz":300,"elapsed":755,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"92c55250-bc15-4801-ef25-76e8626b5df1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         0    1    2    3    4    5    6    7    8    9    ...  838  839  840  \\\n","HG00096  3|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","HG00097  0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","HG00099  0|0  0|0  0|0  0|0  0|0  0|1  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","HG00100  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","HG00101  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n","NA21137  3|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","NA21141  3|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","NA21142  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","NA21143  0|0  0|0  0|0  0|0  0|0  0|0  1|0  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","NA21144  0|0  0|0  0|0  0|0  0|0  0|0  0|1  0|0  0|0  0|0  ...  0|0  0|0  0|0   \n","\n","         841  842  843  844  845  846  847  \n","HG00096  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00097  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00099  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00100  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","HG00101  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","...      ...  ...  ...  ...  ...  ...  ...  \n","NA21137  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21141  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21142  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21143  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","NA21144  0|0  0|0  0|0  0|0  0|0  0|0  0|0  \n","\n","[2504 rows x 848 columns]"],"text/html":["\n","  <div id=\"df-12c8a166-1ef1-4dd2-bdc4-0537d3267406\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>838</th>\n","      <th>839</th>\n","      <th>840</th>\n","      <th>841</th>\n","      <th>842</th>\n","      <th>843</th>\n","      <th>844</th>\n","      <th>845</th>\n","      <th>846</th>\n","      <th>847</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>HG00096</th>\n","      <td>3|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00097</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00099</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00100</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>HG00101</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>NA21137</th>\n","      <td>3|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21141</th>\n","      <td>3|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21142</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21143</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>1|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","    <tr>\n","      <th>NA21144</th>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|1</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>...</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","      <td>0|0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2504 rows × 848 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12c8a166-1ef1-4dd2-bdc4-0537d3267406')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-12c8a166-1ef1-4dd2-bdc4-0537d3267406 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-12c8a166-1ef1-4dd2-bdc4-0537d3267406');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":131}],"source":["root_dir = '[path]/'\n","new_data_header = \"\"\n","# get header\n","with open(root_dir + \"ALL.chr22.mergedSV.v8.20130502.svs.genotypes.vcf\", 'r') as f_in:\n","    # skip info\n","    for line_num in range(70):\n","        f_in.readline()\n","\n","    new_data_header = f_in.readline()\n","# load data\n","\n","# load genotype\n","genotypes = pd.read_csv(root_dir + \"ALL.chr22.mergedSV.v8.20130502.svs.genotypes.vcf\", comment='#', sep='\\t', names=new_data_header.strip().split('\\t'), header=None).iloc[:, 9:]\n","genotypes = genotypes.T\n","headers = genotypes.columns[:]\n","genotypes"]},{"cell_type":"code","source":["ped_file = '[path]/integrated_call_samples.20130502.ALL.ped'\n","pedigree = pd.read_csv(ped_file, sep='\\t', index_col='Individual ID')"],"metadata":{"id":"gp5LsBCPWjdF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_train = pedigree.loc[genotypes.index]['Population']\n","Y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sc53l7EXWmuk","executionInfo":{"status":"ok","timestamp":1672344985510,"user_tz":300,"elapsed":3,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"e46e09dc-7ff2-47c5-f124-7626c48228ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2504,)"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672344985701,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"hiP0EEwsEMNP","outputId":"01c51085-0f45-47bb-b3bf-822dda313762"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2504, 848)"]},"metadata":{},"execution_count":135}],"source":["X = genotypes[:]\n","X.shape"]},{"cell_type":"code","source":["def get_max_genotype(g):\n","  v1, v2 = g.split(\"|\")\n","  return max(int(v1), int(v2)) + 1\n","\n","def key_gen(v1, v2):\n","  return f\"{v1}|{v2}\""],"metadata":{"id":"q2Q455LrQEc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genotype_keys = np.unique(X.values)\n","max_gen = max(map(get_max_genotype, genotype_keys))\n","onehot_encoding_depth = max_gen**2 + 1\n","MISSING_VALUE = [0 for _ in range(onehot_encoding_depth)]\n","MISSING_VALUE[-1] = 1\n","genotype_keys = [key_gen(i,j) for i in range(max_gen) for j in range(max_gen)]\n","replacement_dict = {k:i for i,k in enumerate(genotype_keys)}\n","reverse_replacement_dict = {v:k for k,v in replacement_dict.items()}\n","# replacement_dict"],"metadata":{"id":"0tZbhQKILnvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2054,"status":"ok","timestamp":1672344989729,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"},"user_tz":300},"id":"DNU6deVHXIdE","outputId":"3665ae4f-225a-48d2-b9a2-8efc2182331e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2504, 848)"]},"metadata":{},"execution_count":138}],"source":["X = X.replace(replacement_dict)\n","X.shape"]},{"cell_type":"markdown","source":["## LD"],"metadata":{"id":"Utkz6Di_pMRJ"}},{"cell_type":"code","source":["callset = allel.read_vcf(root_dir + 'ALL.chr22.mergedSV.v8.20130502.svs.genotypes.vcf')\n","g = allel.GenotypeArray(callset['calldata/GT'][callset['variants/CHROM'] == '22'])\n","gn = g.to_n_alt(fill=-1)\n","r = allel.rogers_huff_r(gn)\n","LD = squareform(r ** 2)\n","LD.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkgK-B2vnSqg","executionInfo":{"status":"ok","timestamp":1672344993175,"user_tz":300,"elapsed":3448,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"8a92f9a3-1783-4f4b-fe30-f4fb59cc4bb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(848, 848)"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.imshow(LD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"juhDH1Hfnb5Q","executionInfo":{"status":"ok","timestamp":1672344993376,"user_tz":300,"elapsed":205,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"5c5b91fb-8054-4dea-9515-bfa56e161f5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7faa097310d0>"]},"metadata":{},"execution_count":140},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n","plt.hist(np.amax(LD, axis=1), bins=bins)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"VPcxBWGbn1dq","executionInfo":{"status":"ok","timestamp":1672344993529,"user_tz":300,"elapsed":157,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"a67ce9ea-dcdf-4f40-f4e5-ae1814fa29d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([553.,  97.,  81.,  19.,  98.]),\n"," array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]),\n"," <a list of 5 Patch objects>)"]},"metadata":{},"execution_count":141},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["LD_max_freqs = np.amax(LD, axis=1)\n","bin_labels = np.digitize(LD_max_freqs, bins=bins, right=True)\n","bin_general_labels, bin_counts = np.unique(bin_labels, return_counts=True)\n","bin_general_labels, bin_counts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWiEzZuzeABb","executionInfo":{"status":"ok","timestamp":1672344993705,"user_tz":300,"elapsed":10,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"251dc25b-024d-4a23-f522-ee5f1b8455a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1, 2, 3, 4, 5]), array([553,  97,  81,  19,  98]))"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pZoO-FvKdr3"},"outputs":[],"source":["# hyperparameters\n","feature_size = X.shape[1]\n","inChannel = onehot_encoding_depth\n","# optimizer learning rate\n","learning_rate = 0.001\n","epochs = 60 \n","#epochs = 100   # chr20 LOS 5K\n","\n","\n","# training batch size\n","#batch_size = 32   # u19, 4984 samples\n","bs = 32\n","\n","\n","lr = 1e-3\n","\n","\n","\n","# l1 regulalization\n","kr = 1e-4\n","k_initial = 'glorot_uniform'\n","\n","\n","# channel = inChannel\n","channel = inChannel\n","\n","ndf_num = 128\n","kernel_len = 32\n","num_latent = ndf_num*4\n","p_size = 2\n","\n","\n","\n","#dr_rate = drop_prec\n","dr_rate = 0.2  # avoid overfitting for missing ratio of 0.7"]},{"cell_type":"markdown","metadata":{"id":"OCkow4q5MJk_"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgGz9_6u8u-O"},"outputs":[],"source":["# AE model in one cell\n","# # V1: verify our own ae model with yeast genotype data\n","\n","# # 2.2.8 build variatial autoencoder for snp with subclassing function. \n","\n","class SNP_ENCODER(tf.keras.Model):    \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):    \n","        super(SNP_ENCODER, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate\n","\n","        # object, can be saved in tf mode\n","        self.stride=1\n","        self.kl = kernel_len\n","\n","        \n","    def build(self, inputs): \n","        #encoder\n","        # dense layer 1\n","        self.c1 = layers.Conv1D(filters=self.ndf, kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr), \n","                      input_shape=(self.feature_size, self.channel))\n","        self.p1 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop1 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 2\n","        self.c2 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n","                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n","        self.p2 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop2 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 3\n","        self.c3 = layers.Conv1D(filters=(4*self.ndf), kernel_size=self.kl, strides=1, padding=\"same\", \n","                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n","                      kernel_regularizer=tf.keras.regularizers.L1(kr))\n","        self.p3 = layers.MaxPooling1D(pool_size=p_size)\n","        self.drop3 = layers.Dropout(rate=self.dr)\n","\n","        super(SNP_ENCODER, self).build(inputs)\n","    \n","    def call(self, inputs, training=True):\n","        #print('SNP_ENCODER training flag: ', training)\n","        x = self.c1(inputs)\n","        x = self.p1(x)\n","        x = self.drop1(x, training=training)\n","\n","        x = self.c2(x)\n","        x = self.p2(x)\n","        x = self.drop2(x, training=training)\n","        \n","        x = self.c3(x)\n","        return x\n","\n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size, self.channel))\n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    \n","    \n","    \n","# SNP_DECODER(keras.Model):   \n","class SNP_DECODER(tf.keras.Model):     \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, kernel_len=kernel_len, n_latent=num_latent, dr=dr_rate):\n","        super(SNP_DECODER, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate\n","        \n","        # object, can be saved in tf mode\n","        self.stride=1\n","        self.kl=kernel_len\n","        \n","    def build(self, inputs):\n","        #decoder        \n","        self.c1 = layers.Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr),\n","                      input_shape=((self.feature_size>>2), self.n_latent))\n","        \n","        \n","        self.s1 = layers.UpSampling1D(size=p_size)\n","        self.drop1 = layers.Dropout(rate=self.dr)\n","        \n","        # dense layer 2\n","        self.c2 = layers.Conv1D(filters=(1*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n","                      activation='relu', use_bias=True, \n","                      kernel_initializer=k_initial, kernel_regularizer=tf.keras.regularizers.L1(kr))\n","\n","        self.s2 = layers.UpSampling1D(size=p_size)\n","        self.drop2 = layers.Dropout(rate=self.dr)\n","        \n","\n","        # dense layer6\n","        self.c3 = layers.Conv1D(filters=self.channel, kernel_size=self.kl, strides=1, padding=\"same\", \n","                      activation='softmax', use_bias=True)\n","        \n","        super(SNP_DECODER, self).build(inputs)\n","        \n","    def call(self, inputs, training = True):\n","        #print('SNP_DECODER training flag: ', training)\n","        x = self.c1(inputs)\n","        x = self.s1(x)\n","        x = self.drop1(x, training=training)\n","        \n","        x = self.c2(x)\n","        x = self.s2(x)\n","        x = self.drop2(x, training=training)\n","\n","        \n","        d_out = self.c3(x)\n","        return d_out\n","    \n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size>>2, self.n_latent))\n","        \n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    \n","\n","    \n","#class SNP_AE(keras.Model):\n","class SNP_AE(tf.keras.Model):    \n","    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):    \n","        super(SNP_AE, self).__init__()\n","        self.feature_size = feature_size\n","        self.channel = channel\n","        self.ndf = ndf\n","        self.n_latent = n_latent\n","        self.dr = dr # dropout rate      \n","        \n","        self.encoder = SNP_ENCODER(self.feature_size)\n","        self.decoder = SNP_DECODER(self.feature_size)\n","    \n","\n","    def call(self, x, training=True): \n","        latent = self.encoder(x, training)     \n","        res = self.decoder(latent, training)\n","\n","        return res, latent\n","    \n","    # AFAIK: The most convenient method to print model.summary() \n","    # similar to the sequential or functional API like.\n","    def build_graph(self):\n","        x = layers.Input(shape=(self.feature_size, self.channel))\n","        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n","    "]},{"cell_type":"code","source":["# plot snp vae encoder model\n","\n","SNP_encoder = SNP_ENCODER(feature_size)\n","SNP_encoder.build((None, feature_size, channel))\n","SNP_encoder.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaAkHl0XKyIM","executionInfo":{"status":"ok","timestamp":1672344993905,"user_tz":300,"elapsed":206,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"fc7032d4-8e1d-4861-d796-f4a117cab185"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 848, 37)]         0         \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, 848, 128)          151680    \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 424, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_5 (Dropout)         (None, 424, 128)          0         \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, 424, 256)          1048832   \n","                                                                 \n"," max_pooling1d_4 (MaxPooling  (None, 212, 256)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_6 (Dropout)         (None, 212, 256)          0         \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 212, 512)          4194816   \n","                                                                 \n","=================================================================\n","Total params: 5,395,328\n","Trainable params: 5,395,328\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# plot snp vae decoder model\n","\n","SNP_decoder = SNP_DECODER(feature_size)\n","\n","SNP_decoder.build((None, feature_size>>2, num_latent))\n","SNP_decoder.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SEoV7YaLzwv","executionInfo":{"status":"ok","timestamp":1672344994046,"user_tz":300,"elapsed":148,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"0cc47d53-efd6-430d-d507-776b8d836a09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 212, 512)]        0         \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 212, 256)          4194560   \n","                                                                 \n"," up_sampling1d_2 (UpSampling  (None, 424, 256)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_8 (Dropout)         (None, 424, 256)          0         \n","                                                                 \n"," conv1d_10 (Conv1D)          (None, 424, 128)          1048704   \n","                                                                 \n"," up_sampling1d_3 (UpSampling  (None, 848, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," dropout_9 (Dropout)         (None, 848, 128)          0         \n","                                                                 \n"," conv1d_11 (Conv1D)          (None, 848, 37)           151589    \n","                                                                 \n","=================================================================\n","Total params: 5,394,853\n","Trainable params: 5,394,853\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# plot snp vae model\n","\n","SNP_ae = SNP_AE(feature_size)\n","SNP_ae.build((None, feature_size, channel))\n","SNP_ae.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmP5dBy-L2Ep","executionInfo":{"status":"ok","timestamp":1672344994495,"user_tz":300,"elapsed":451,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"327aa470-4977-44fd-cd7c-384dd94d575c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 848, 37)]         0         \n","                                                                 \n"," snp_encoder_2 (SNP_ENCODER)  (None, 212, 512)         5395328   \n","                                                                 \n"," snp_decoder_2 (SNP_DECODER)  (None, 848, 37)          5394853   \n","                                                                 \n","=================================================================\n","Total params: 10,790,181\n","Trainable params: 10,790,181\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# AE model sub-functions\n","\n","def generate_fake_missing(x_in, missing_ratio=0.5):\n","\n","        # Generates missing genotypes\n","        # different missing loci for each individuals\n","        x_fake = x_in.copy()   # with .copy() to not overwrite the original data\n","        \n","        for i in range(x_in.shape[0]):\n","            missing_size = int(missing_ratio * x_in.shape[1])\n","          \n","            # without repeat random numbers: set replace with false\n","            missing_index = np.random.choice(x_in.shape[1], size=missing_size, replace=False)\n","            \n","            # missing loci are encoded as [0, 0]\n","            # x_fake[i, missing_index, :] = [0, 0, 1]  # yeast\n","            x_fake[i, missing_index, :] = MISSING_VALUE  # human\n","\n","        return x_fake\n","        #return x_fake, x_in\n","    \n","    \n","\n","def loss_function_cce(recon_x, x):\n","    # orders: y_true, y_pred\n","    cce = tf.keras.losses.categorical_crossentropy(x, recon_x)\n","\n","    #cce = np.double(cce)\n","    cce = K.cast(cce, dtype='float32')\n","\n","    lamb1 = 1.0\n","    loss = lamb1*cce\n","    #print('loss:', loss)\n","    \n","    loss = tf.reduce_mean(loss)\n","    #print('ave loss:', loss)\n","    \n","    return loss\n"],"metadata":{"id":"-EXc-erqK16d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TOA_NexzN5Qq"},"source":["## Training"]},{"cell_type":"code","source":["# With constraint\n","N_SPLITS=3\n","results = None\n","kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=2022)\n","fold = 0\n","_x = tf.keras.utils.to_categorical(X.to_numpy(), inChannel)\n","print(_x.shape)\n","_y = Y_train.to_numpy()\n","for train_index, test_index in kf.split(_x):\n","  fold += 1\n","  accuracies = []\n","  print(f\"Training using fold {fold}\")\n","  print(\"*******************************************\")\n","  print(\"*******************************************\")\n","  \n","  x_train, y_train, test_dataset, test_indices = _x[train_index], _y[train_index], (_x[test_index], _y[test_index]),Y_train.index[test_index]\n","  x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10,\n","                                      random_state=fold,\n","                                      shuffle=True,\n","                                      stratify=y_train)\n","  \n","  \n","  for missing_ratio in [\n","                        0.05,\n","                        0.1,\n","                        0.2\n","                        ]:\n","    train_X = np.copy(x_train)\n","    valid_X = np.copy(x_valid)\n","    print(f\"Missing rate {missing_ratio}\")\n","    print(\"=====================================================\")\n","    train_X_fake = generate_fake_missing(train_X, missing_ratio)\n","\n","    diff = np.absolute(np.array(train_X) - np.array(train_X_fake))\n","    print('train_X_fake diff:', np.sum(diff))\n","\n","\n","    valid_X_fake = generate_fake_missing(valid_X, missing_ratio)\n","    diff = np.absolute(np.array(valid_X) - np.array(valid_X_fake))\n","    print('valid_X_fake diff:', np.sum(diff))\n","\n","\n","    K.clear_session()\n","    with strategy.scope():\n","      model = SNP_AE(feature_size)\n","      optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","      train_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n","      val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n","\n","      train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","      val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","\n","      for epoch in range(epochs):\n","        print('Start of epoch %d' % (epoch,))\n","\n","        #----shuffle train data and lablel for each epoch-----------------------------------------------------------------------------------#    \n","        # shuffle data and labels at the same time\n","        idx = train_X.shape[0]\n","\n","\n","        indices = tf.range(start=0, limit=idx, dtype=tf.int32)\n","\n","        shuffled_indices = tf.random.shuffle(indices)  \n","      \n","        train_X_fake = tf.gather(train_X_fake, shuffled_indices)\n","        train_X = tf.gather(train_X, shuffled_indices)        \n","        \n","        snp_x = tf.data.Dataset.from_tensor_slices(train_X_fake).batch(bs, drop_remainder=True) \n","        snp_y = tf.data.Dataset.from_tensor_slices(train_X).batch(bs, drop_remainder=True)\n","\n","        snp_x_v = tf.data.Dataset.from_tensor_slices(valid_X_fake).batch(bs, drop_remainder=True)\n","        snp_y_v = tf.data.Dataset.from_tensor_slices(valid_X).batch(bs, drop_remainder=True)\n","\n","        loss_batch = []\n","\n","        # Iterate over the batches of the dataset.\n","        for step, (snp_fake_batch, snp_label_batch) in enumerate(zip(snp_x, snp_y)): \n","          with tf.GradientTape() as tape:\n","              recon_inputs, latents= model(snp_fake_batch, training=True)\n","              loss = loss_function_cce(recon_inputs, snp_label_batch)\n","\n","\n","          grads = tape.gradient(loss, model.trainable_variables)\n","          optimizer.apply_gradients(zip(grads, model.trainable_variables))    \n","\n","          loss_batch.append(loss.numpy())\n","\n","\n","          # Update training metric.\n","          train_acc_metric.update_state(snp_label_batch, recon_inputs)\n","          train_loss_metric.update_state(snp_label_batch, recon_inputs)\n","\n","\n","        # Display metrics at the end of each epoch.\n","        train_loss = train_loss_metric.result()\n","        print(\"Training loss over epoch: \", epoch, train_loss.numpy())\n","\n","        train_acc = train_acc_metric.result()\n","        print(\"Training acc over epoch: \", epoch, train_acc.numpy())\n","\n","\n","        # Reset training metrics at the end of each epoch\n","        train_loss_metric.reset_states()\n","        train_acc_metric.reset_states()\n","\n","\n","        # Run a validation loop at the end of each epoch.\n","        for x_batch_val, y_batch_val in zip(snp_x_v, snp_y_v):   \n","\n","            val_recons, latents = model(x_batch_val, training=False)\n","            # Update val metrics\n","            val_loss_metric.update_state(y_batch_val, val_recons)\n","            val_acc_metric.update_state(y_batch_val, val_recons)\n","\n","        val_loss = val_loss_metric.result()\n","        val_acc = val_acc_metric.result()\n","\n","        val_loss_metric.reset_states()\n","        val_acc_metric.reset_states()\n","        #print(\"Validation acc: %.4f\" % (float(val_acc),))\n","        print(\"Validation loss: \", epoch, val_loss.numpy())\n","        print(\"Validation acc: \", epoch, val_acc.numpy())\n","\n","\n","        #print('epoch %s: batch loss = %s' % (epoch, loss_batch))\n","        loss_epoch = np.mean(loss_batch)    \n","        print('epoch %s: loss = %s' % (epoch, loss_epoch))    \n","    \n","    save_name = f\"[path]/Chr.22.ALL/AE/preds_mixed_mr_{missing_ratio}_fold_{fold}_.csv\"\n","    avg_accuracy = []\n","    preds = []\n","    true_labels = []\n","    \n","    to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n","    test_X_missing = np.copy(test_dataset[0])\n","    for i in tqdm(list(range(test_dataset[0].shape[0]))):\n","      missing_index, _ = train_test_split(np.arange(x_train.shape[1]), train_size=missing_ratio,\n","                                    random_state=i + fold,\n","                                    shuffle=True,\n","                                    stratify=bin_labels\n","                                    )\n","      test_X_missing[i:i+1, missing_index, :] = MISSING_VALUE\n","      # predict\n","      predict_onehot, _ = model(test_X_missing[i:i+1, :, :], training=False)\n","      predict_onehot = predict_onehot.numpy()\n","      # only care the missing position\n","      predict_missing_onehot = predict_onehot[0:1, missing_index, :]\n","      # predict label\n","      predict_missing = np.argmax(predict_missing_onehot, axis=2)\n","      \n","      preds.extend(predict_missing.ravel().tolist())\n","      \n","      predict_haplotypes = np.argmax(predict_onehot, axis=2)\n","      # Only for haploids\n","      to_save_array[i] = predict_haplotypes\n","      # real label\n","      label_missing_onehot = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n","      label_missing = np.argmax(test_dataset[0][i:i + 1, missing_index], axis=2)\n","      true_labels.extend(label_missing.ravel().tolist())\n","      # accuracy\n","      correct_prediction = np.equal(predict_missing, label_missing)\n","      accuracy = np.mean(correct_prediction)\n","\n","      avg_accuracy.append(accuracy)\n","\n","    df = pd.DataFrame(to_save_array, columns= headers[:], index = Y_train.index[test_index])\n","    df.to_csv(save_name)\n","    print('The average imputation accuracy' \\\n","          'on test data with {} missing genotypes is {:.4f}: '\n","        .format(missing_ratio, np.mean(avg_accuracy)))\n","    cnf_matrix = confusion_matrix(true_labels, preds)\n","    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n","    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n","    TP = np.diag(cnf_matrix)\n","    TN = cnf_matrix.sum() - (FP + FN + TP)\n","    FP = FP.astype(float)\n","    FN = FN.astype(float)\n","    TP = TP.astype(float)\n","    TN = TN.astype(float)\n","    # Sensitivity, hit rate, recall, or true positive rate\n","    TPR = TP/(TP+FN)\n","    # Specificity or true negative rate\n","    TNR = TN/(TN+FP)\n","    print(f\"Sensitivity: {np.mean(TPR)}\")\n","    print(f\"Specificity: {np.mean(TNR)}\")\n","    print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n","    print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n","    accuracies.append(np.mean(avg_accuracy))\n","        \n","    "],"metadata":{"id":"Gq5VWuKWVZVx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672347869204,"user_tz":300,"elapsed":2874712,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"e9245594-f120-431b-c704-9b5619ed6a6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2504, 848, 37)\n","Training using fold 1\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 126168.0\n","valid_X_fake diff: 14028.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.99226964\n","Training acc over epoch:  0 0.8216968\n","Validation loss:  0 0.28677747\n","Validation acc:  0 0.94966096\n","epoch 0: loss = 0.9922699\n","Start of epoch 1\n","Training loss over epoch:  1 0.28748706\n","Training acc over epoch:  1 0.94945747\n","Validation loss:  1 0.27584726\n","Validation acc:  1 0.94966096\n","epoch 1: loss = 0.28748712\n","Start of epoch 2\n","Training loss over epoch:  2 0.27628425\n","Training acc over epoch:  2 0.9494399\n","Validation loss:  2 0.2638351\n","Validation acc:  2 0.94966096\n","epoch 2: loss = 0.27628422\n","Start of epoch 3\n","Training loss over epoch:  3 0.26254973\n","Training acc over epoch:  3 0.94943506\n","Validation loss:  3 0.24729724\n","Validation acc:  3 0.94966096\n","epoch 3: loss = 0.26254973\n","Start of epoch 4\n","Training loss over epoch:  4 0.24572328\n","Training acc over epoch:  4 0.94944066\n","Validation loss:  4 0.23464881\n","Validation acc:  4 0.94966096\n","epoch 4: loss = 0.2457233\n","Start of epoch 5\n","Training loss over epoch:  5 0.23702793\n","Training acc over epoch:  5 0.94947433\n","Validation loss:  5 0.22910069\n","Validation acc:  5 0.94966096\n","epoch 5: loss = 0.23702793\n","Start of epoch 6\n","Training loss over epoch:  6 0.23228306\n","Training acc over epoch:  6 0.9493726\n","Validation loss:  6 0.22206706\n","Validation acc:  6 0.94966096\n","epoch 6: loss = 0.23228307\n","Start of epoch 7\n","Training loss over epoch:  7 0.22182386\n","Training acc over epoch:  7 0.9494022\n","Validation loss:  7 0.20648907\n","Validation acc:  7 0.94966096\n","epoch 7: loss = 0.22182386\n","Start of epoch 8\n","Training loss over epoch:  8 0.21184482\n","Training acc over epoch:  8 0.94948554\n","Validation loss:  8 0.1974736\n","Validation acc:  8 0.9496978\n","epoch 8: loss = 0.2118448\n","Start of epoch 9\n","Training loss over epoch:  9 0.20277575\n","Training acc over epoch:  9 0.94978917\n","Validation loss:  9 0.18697205\n","Validation acc:  9 0.95041275\n","epoch 9: loss = 0.20277573\n","Start of epoch 10\n","Training loss over epoch:  10 0.19451505\n","Training acc over epoch:  10 0.950124\n","Validation loss:  10 0.17855279\n","Validation acc:  10 0.9514667\n","epoch 10: loss = 0.19451506\n","Start of epoch 11\n","Training loss over epoch:  11 0.18590161\n","Training acc over epoch:  11 0.9506183\n","Validation loss:  11 0.16952163\n","Validation acc:  11 0.95198995\n","epoch 11: loss = 0.18590163\n","Start of epoch 12\n","Training loss over epoch:  12 0.17478237\n","Training acc over epoch:  12 0.95135295\n","Validation loss:  12 0.157981\n","Validation acc:  12 0.952727\n","epoch 12: loss = 0.17478237\n","Start of epoch 13\n","Training loss over epoch:  13 0.15993094\n","Training acc over epoch:  13 0.9526227\n","Validation loss:  13 0.14188012\n","Validation acc:  13 0.9540315\n","epoch 13: loss = 0.15993097\n","Start of epoch 14\n","Training loss over epoch:  14 0.1475302\n","Training acc over epoch:  14 0.95394135\n","Validation loss:  14 0.12923104\n","Validation acc:  14 0.9566701\n","epoch 14: loss = 0.14753021\n","Start of epoch 15\n","Training loss over epoch:  15 0.13782568\n","Training acc over epoch:  15 0.95567334\n","Validation loss:  15 0.12169709\n","Validation acc:  15 0.9610849\n","epoch 15: loss = 0.1378257\n","Start of epoch 16\n","Training loss over epoch:  16 0.13106726\n","Training acc over epoch:  16 0.95720667\n","Validation loss:  16 0.1118337\n","Validation acc:  16 0.96317804\n","epoch 16: loss = 0.13106728\n","Start of epoch 17\n","Training loss over epoch:  17 0.123849794\n","Training acc over epoch:  17 0.9592928\n","Validation loss:  17 0.103834905\n","Validation acc:  17 0.9650354\n","epoch 17: loss = 0.12384979\n","Start of epoch 18\n","Training loss over epoch:  18 0.117266\n","Training acc over epoch:  18 0.961471\n","Validation loss:  18 0.09532663\n","Validation acc:  18 0.9677255\n","epoch 18: loss = 0.11726602\n","Start of epoch 19\n","Training loss over epoch:  19 0.11136207\n","Training acc over epoch:  19 0.9632367\n","Validation loss:  19 0.09102455\n","Validation acc:  19 0.970541\n","epoch 19: loss = 0.11136206\n","Start of epoch 20\n","Training loss over epoch:  20 0.10611779\n","Training acc over epoch:  20 0.9652475\n","Validation loss:  20 0.08562165\n","Validation acc:  20 0.9713001\n","epoch 20: loss = 0.10611777\n","Start of epoch 21\n","Training loss over epoch:  21 0.10108081\n","Training acc over epoch:  21 0.9670765\n","Validation loss:  21 0.07862349\n","Validation acc:  21 0.9748526\n","epoch 21: loss = 0.1010808\n","Start of epoch 22\n","Training loss over epoch:  22 0.0949341\n","Training acc over epoch:  22 0.9691994\n","Validation loss:  22 0.07302804\n","Validation acc:  22 0.9778007\n","epoch 22: loss = 0.09493411\n","Start of epoch 23\n","Training loss over epoch:  23 0.08947916\n","Training acc over epoch:  23 0.9712583\n","Validation loss:  23 0.070798814\n","Validation acc:  23 0.9795032\n","epoch 23: loss = 0.08947918\n","Start of epoch 24\n","Training loss over epoch:  24 0.08363785\n","Training acc over epoch:  24 0.97334844\n","Validation loss:  24 0.06289174\n","Validation acc:  24 0.9815522\n","epoch 24: loss = 0.08363784\n","Start of epoch 25\n","Training loss over epoch:  25 0.07792003\n","Training acc over epoch:  25 0.9754273\n","Validation loss:  25 0.05890163\n","Validation acc:  25 0.98352003\n","epoch 25: loss = 0.07792003\n","Start of epoch 26\n","Training loss over epoch:  26 0.07362989\n","Training acc over epoch:  26 0.97703433\n","Validation loss:  26 0.056193113\n","Validation acc:  26 0.98471403\n","epoch 26: loss = 0.07362989\n","Start of epoch 27\n","Training loss over epoch:  27 0.06870894\n","Training acc over epoch:  27 0.9787263\n","Validation loss:  27 0.050351493\n","Validation acc:  27 0.98563534\n","epoch 27: loss = 0.068708956\n","Start of epoch 28\n","Training loss over epoch:  28 0.064547434\n","Training acc over epoch:  28 0.97997284\n","Validation loss:  28 0.047925316\n","Validation acc:  28 0.9865935\n","epoch 28: loss = 0.06454741\n","Start of epoch 29\n","Training loss over epoch:  29 0.061271153\n","Training acc over epoch:  29 0.9811409\n","Validation loss:  29 0.045542028\n","Validation acc:  29 0.9874484\n","epoch 29: loss = 0.061271153\n","Start of epoch 30\n","Training loss over epoch:  30 0.05898409\n","Training acc over epoch:  30 0.98201734\n","Validation loss:  30 0.04291289\n","Validation acc:  30 0.9880159\n","epoch 30: loss = 0.05898409\n","Start of epoch 31\n","Training loss over epoch:  31 0.055804733\n","Training acc over epoch:  31 0.9831381\n","Validation loss:  31 0.04069473\n","Validation acc:  31 0.9887898\n","epoch 31: loss = 0.05580473\n","Start of epoch 32\n","Training loss over epoch:  32 0.053789098\n","Training acc over epoch:  32 0.9836692\n","Validation loss:  32 0.039271876\n","Validation acc:  32 0.9892541\n","epoch 32: loss = 0.0537891\n","Start of epoch 33\n","Training loss over epoch:  33 0.051431276\n","Training acc over epoch:  33 0.98454726\n","Validation loss:  33 0.03782761\n","Validation acc:  33 0.9895416\n","epoch 33: loss = 0.051431283\n","Start of epoch 34\n","Training loss over epoch:  34 0.049341258\n","Training acc over epoch:  34 0.9851705\n","Validation loss:  34 0.035312705\n","Validation acc:  34 0.9902344\n","epoch 34: loss = 0.049341254\n","Start of epoch 35\n","Training loss over epoch:  35 0.047748145\n","Training acc over epoch:  35 0.985821\n","Validation loss:  35 0.036476336\n","Validation acc:  35 0.99046284\n","epoch 35: loss = 0.047748137\n","Start of epoch 36\n","Training loss over epoch:  36 0.04616863\n","Training acc over epoch:  36 0.9862937\n","Validation loss:  36 0.03351876\n","Validation acc:  36 0.99098617\n","epoch 36: loss = 0.04616863\n","Start of epoch 37\n","Training loss over epoch:  37 0.043938044\n","Training acc over epoch:  37 0.9870371\n","Validation loss:  37 0.03224569\n","Validation acc:  37 0.991421\n","epoch 37: loss = 0.043938044\n","Start of epoch 38\n","Training loss over epoch:  38 0.042046357\n","Training acc over epoch:  38 0.9875154\n","Validation loss:  38 0.030779742\n","Validation acc:  38 0.99168634\n","epoch 38: loss = 0.042046353\n","Start of epoch 39\n","Training loss over epoch:  39 0.04145061\n","Training acc over epoch:  39 0.98796\n","Validation loss:  39 0.030841589\n","Validation acc:  39 0.9923054\n","epoch 39: loss = 0.04145061\n","Start of epoch 40\n","Training loss over epoch:  40 0.039412424\n","Training acc over epoch:  40 0.9884839\n","Validation loss:  40 0.029809205\n","Validation acc:  40 0.99251914\n","epoch 40: loss = 0.039412424\n","Start of epoch 41\n","Training loss over epoch:  41 0.038177717\n","Training acc over epoch:  41 0.98899347\n","Validation loss:  41 0.028357638\n","Validation acc:  41 0.9925929\n","epoch 41: loss = 0.038177725\n","Start of epoch 42\n","Training loss over epoch:  42 0.036884826\n","Training acc over epoch:  42 0.9893331\n","Validation loss:  42 0.027086517\n","Validation acc:  42 0.99302036\n","epoch 42: loss = 0.03688483\n","Start of epoch 43\n","Training loss over epoch:  43 0.035710853\n","Training acc over epoch:  43 0.98961914\n","Validation loss:  43 0.026679507\n","Validation acc:  43 0.9933667\n","epoch 43: loss = 0.03571085\n","Start of epoch 44\n","Training loss over epoch:  44 0.03492447\n","Training acc over epoch:  44 0.98994917\n","Validation loss:  44 0.025366463\n","Validation acc:  44 0.99358046\n","epoch 44: loss = 0.03492447\n","Start of epoch 45\n","Training loss over epoch:  45 0.03395633\n","Training acc over epoch:  45 0.99030244\n","Validation loss:  45 0.024528395\n","Validation acc:  45 0.993949\n","epoch 45: loss = 0.03395634\n","Start of epoch 46\n","Training loss over epoch:  46 0.03268149\n","Training acc over epoch:  46 0.990671\n","Validation loss:  46 0.024601018\n","Validation acc:  46 0.994089\n","epoch 46: loss = 0.03268149\n","Start of epoch 47\n","Training loss over epoch:  47 0.03204662\n","Training acc over epoch:  47 0.99093616\n","Validation loss:  47 0.023688903\n","Validation acc:  47 0.99421436\n","epoch 47: loss = 0.032046616\n","Start of epoch 48\n","Training loss over epoch:  48 0.030812105\n","Training acc over epoch:  48 0.9911949\n","Validation loss:  48 0.023904577\n","Validation acc:  48 0.99436176\n","epoch 48: loss = 0.030812098\n","Start of epoch 49\n","Training loss over epoch:  49 0.029927263\n","Training acc over epoch:  49 0.9915795\n","Validation loss:  49 0.02265521\n","Validation acc:  49 0.9945313\n","epoch 49: loss = 0.029927265\n","Start of epoch 50\n","Training loss over epoch:  50 0.029464616\n","Training acc over epoch:  50 0.9915754\n","Validation loss:  50 0.021705955\n","Validation acc:  50 0.99476707\n","epoch 50: loss = 0.029464608\n","Start of epoch 51\n","Training loss over epoch:  51 0.02823874\n","Training acc over epoch:  51 0.99207455\n","Validation loss:  51 0.022163738\n","Validation acc:  51 0.99462706\n","epoch 51: loss = 0.028238734\n","Start of epoch 52\n","Training loss over epoch:  52 0.027717926\n","Training acc over epoch:  52 0.9921795\n","Validation loss:  52 0.020985113\n","Validation acc:  52 0.994885\n","epoch 52: loss = 0.027717924\n","Start of epoch 53\n","Training loss over epoch:  53 0.026721608\n","Training acc over epoch:  53 0.99248713\n","Validation loss:  53 0.020855436\n","Validation acc:  53 0.99495137\n","epoch 53: loss = 0.02672161\n","Start of epoch 54\n","Training loss over epoch:  54 0.02627882\n","Training acc over epoch:  54 0.9925777\n","Validation loss:  54 0.020124832\n","Validation acc:  54 0.9951503\n","epoch 54: loss = 0.026278825\n","Start of epoch 55\n","Training loss over epoch:  55 0.02547383\n","Training acc over epoch:  55 0.99286443\n","Validation loss:  55 0.019249944\n","Validation acc:  55 0.99534935\n","epoch 55: loss = 0.025473827\n","Start of epoch 56\n","Training loss over epoch:  56 0.024813643\n","Training acc over epoch:  56 0.99300945\n","Validation loss:  56 0.019510021\n","Validation acc:  56 0.99535674\n","epoch 56: loss = 0.02481364\n","Start of epoch 57\n","Training loss over epoch:  57 0.024250783\n","Training acc over epoch:  57 0.9930623\n","Validation loss:  57 0.018994939\n","Validation acc:  57 0.9955631\n","epoch 57: loss = 0.024250781\n","Start of epoch 58\n","Training loss over epoch:  58 0.023644367\n","Training acc over epoch:  58 0.9933403\n","Validation loss:  58 0.018380232\n","Validation acc:  58 0.9955705\n","epoch 58: loss = 0.02364437\n","Start of epoch 59\n","Training loss over epoch:  59 0.022749199\n","Training acc over epoch:  59 0.9934973\n","Validation loss:  59 0.018346263\n","Validation acc:  59 0.99549675\n","epoch 59: loss = 0.022749197\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:11<00:00, 74.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9503: \n","Sensitivity: nan\n","Specificity: 0.9247459322091413\n","F1-score macro: 0.1957550233047236\n","F1-score micro: 0.9502708867978329\n","Missing rate 0.1\n","=====================================================\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-149-6db1d8498207>:190: RuntimeWarning: invalid value encountered in true_divide\n","  TPR = TP/(TP+FN)\n"]},{"output_type":"stream","name":"stdout","text":["train_X_fake diff: 252336.0\n","valid_X_fake diff: 28056.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.0077953\n","Training acc over epoch:  0 0.840729\n","Validation loss:  0 0.28634626\n","Validation acc:  0 0.94966096\n","epoch 0: loss = 1.0077955\n","Start of epoch 1\n","Training loss over epoch:  1 0.27801722\n","Training acc over epoch:  1 0.9494302\n","Validation loss:  1 0.25423872\n","Validation acc:  1 0.94966096\n","epoch 1: loss = 0.27801713\n","Start of epoch 2\n","Training loss over epoch:  2 0.24932674\n","Training acc over epoch:  2 0.949415\n","Validation loss:  2 0.23421755\n","Validation acc:  2 0.94966096\n","epoch 2: loss = 0.24932675\n","Start of epoch 3\n","Training loss over epoch:  3 0.23599026\n","Training acc over epoch:  3 0.9494479\n","Validation loss:  3 0.22493635\n","Validation acc:  3 0.94966096\n","epoch 3: loss = 0.23599026\n","Start of epoch 4\n","Training loss over epoch:  4 0.22614619\n","Training acc over epoch:  4 0.9494591\n","Validation loss:  4 0.21120174\n","Validation acc:  4 0.94969046\n","epoch 4: loss = 0.22614622\n","Start of epoch 5\n","Training loss over epoch:  5 0.21283299\n","Training acc over epoch:  5 0.94987726\n","Validation loss:  5 0.1971823\n","Validation acc:  5 0.9512603\n","epoch 5: loss = 0.21283297\n","Start of epoch 6\n","Training loss over epoch:  6 0.20104922\n","Training acc over epoch:  6 0.9504925\n","Validation loss:  6 0.18290332\n","Validation acc:  6 0.9518573\n","epoch 6: loss = 0.20104922\n","Start of epoch 7\n","Training loss over epoch:  7 0.19077258\n","Training acc over epoch:  7 0.95115745\n","Validation loss:  7 0.16999578\n","Validation acc:  7 0.95318395\n","epoch 7: loss = 0.19077258\n","Start of epoch 8\n","Training loss over epoch:  8 0.17912036\n","Training acc over epoch:  8 0.9520779\n","Validation loss:  8 0.15553577\n","Validation acc:  8 0.9543042\n","epoch 8: loss = 0.17912038\n","Start of epoch 9\n","Training loss over epoch:  9 0.16371529\n","Training acc over epoch:  9 0.9534062\n","Validation loss:  9 0.1357677\n","Validation acc:  9 0.9572818\n","epoch 9: loss = 0.16371526\n","Start of epoch 10\n","Training loss over epoch:  10 0.14442651\n","Training acc over epoch:  10 0.9564464\n","Validation loss:  10 0.11643835\n","Validation acc:  10 0.9647553\n","epoch 10: loss = 0.14442651\n","Start of epoch 11\n","Training loss over epoch:  11 0.12626521\n","Training acc over epoch:  11 0.9608237\n","Validation loss:  11 0.101221174\n","Validation acc:  11 0.9690522\n","epoch 11: loss = 0.12626524\n","Start of epoch 12\n","Training loss over epoch:  12 0.11469233\n","Training acc over epoch:  12 0.96440876\n","Validation loss:  12 0.089882\n","Validation acc:  12 0.97273\n","epoch 12: loss = 0.11469231\n","Start of epoch 13\n","Training loss over epoch:  13 0.105351776\n","Training acc over epoch:  13 0.96720946\n","Validation loss:  13 0.08460076\n","Validation acc:  13 0.9750737\n","epoch 13: loss = 0.10535178\n","Start of epoch 14\n","Training loss over epoch:  14 0.09846561\n","Training acc over epoch:  14 0.9695343\n","Validation loss:  14 0.07764563\n","Validation acc:  14 0.97712266\n","epoch 14: loss = 0.0984656\n","Start of epoch 15\n","Training loss over epoch:  15 0.0927141\n","Training acc over epoch:  15 0.971203\n","Validation loss:  15 0.07286929\n","Validation acc:  15 0.9786704\n","epoch 15: loss = 0.09271409\n","Start of epoch 16\n","Training loss over epoch:  16 0.08881402\n","Training acc over epoch:  16 0.9724856\n","Validation loss:  16 0.06921612\n","Validation acc:  16 0.97973174\n","epoch 16: loss = 0.08881403\n","Start of epoch 17\n","Training loss over epoch:  17 0.083198264\n","Training acc over epoch:  17 0.97437304\n","Validation loss:  17 0.06414691\n","Validation acc:  17 0.98119104\n","epoch 17: loss = 0.083198264\n","Start of epoch 18\n","Training loss over epoch:  18 0.07948528\n","Training acc over epoch:  18 0.97590315\n","Validation loss:  18 0.059655827\n","Validation acc:  18 0.9827093\n","epoch 18: loss = 0.07948527\n","Start of epoch 19\n","Training loss over epoch:  19 0.07455124\n","Training acc over epoch:  19 0.97760797\n","Validation loss:  19 0.056125835\n","Validation acc:  19 0.9845298\n","epoch 19: loss = 0.074551255\n","Start of epoch 20\n","Training loss over epoch:  20 0.06989548\n","Training acc over epoch:  20 0.97932315\n","Validation loss:  20 0.053156905\n","Validation acc:  20 0.9856132\n","epoch 20: loss = 0.06989548\n","Start of epoch 21\n","Training loss over epoch:  21 0.065735556\n","Training acc over epoch:  21 0.98090297\n","Validation loss:  21 0.049127046\n","Validation acc:  21 0.9872199\n","epoch 21: loss = 0.065735556\n","Start of epoch 22\n","Training loss over epoch:  22 0.06260226\n","Training acc over epoch:  22 0.98218393\n","Validation loss:  22 0.047397185\n","Validation acc:  22 0.98773587\n","epoch 22: loss = 0.06260226\n","Start of epoch 23\n","Training loss over epoch:  23 0.059639014\n","Training acc over epoch:  23 0.98307717\n","Validation loss:  23 0.044214725\n","Validation acc:  23 0.98874557\n","epoch 23: loss = 0.059639007\n","Start of epoch 24\n","Training loss over epoch:  24 0.05620018\n","Training acc over epoch:  24 0.98427004\n","Validation loss:  24 0.041844632\n","Validation acc:  24 0.9895563\n","epoch 24: loss = 0.056200176\n","Start of epoch 25\n","Training loss over epoch:  25 0.054770716\n","Training acc over epoch:  25 0.9847259\n","Validation loss:  25 0.04060013\n","Validation acc:  25 0.9901754\n","epoch 25: loss = 0.054770723\n","Start of epoch 26\n","Training loss over epoch:  26 0.052103844\n","Training acc over epoch:  26 0.98556143\n","Validation loss:  26 0.038773656\n","Validation acc:  26 0.9905366\n","epoch 26: loss = 0.052103847\n","Start of epoch 27\n","Training loss over epoch:  27 0.050376546\n","Training acc over epoch:  27 0.9861214\n","Validation loss:  27 0.037906982\n","Validation acc:  27 0.9905881\n","epoch 27: loss = 0.05037654\n","Start of epoch 28\n","Training loss over epoch:  28 0.048565608\n","Training acc over epoch:  28 0.9867447\n","Validation loss:  28 0.03661774\n","Validation acc:  28 0.99110407\n","epoch 28: loss = 0.048565622\n","Start of epoch 29\n","Training loss over epoch:  29 0.04681178\n","Training acc over epoch:  29 0.9872879\n","Validation loss:  29 0.03524072\n","Validation acc:  29 0.99137676\n","epoch 29: loss = 0.046811793\n","Start of epoch 30\n","Training loss over epoch:  30 0.04582713\n","Training acc over epoch:  30 0.9876019\n","Validation loss:  30 0.035625227\n","Validation acc:  30 0.99153894\n","epoch 30: loss = 0.045827128\n","Start of epoch 31\n","Training loss over epoch:  31 0.04456469\n","Training acc over epoch:  31 0.98798645\n","Validation loss:  31 0.033630222\n","Validation acc:  31 0.99168634\n","epoch 31: loss = 0.0445647\n","Start of epoch 32\n","Training loss over epoch:  32 0.042753067\n","Training acc over epoch:  32 0.9884551\n","Validation loss:  32 0.032932322\n","Validation acc:  32 0.9919959\n","epoch 32: loss = 0.042753067\n","Start of epoch 33\n","Training loss over epoch:  33 0.041756634\n","Training acc over epoch:  33 0.988669\n","Validation loss:  33 0.03339314\n","Validation acc:  33 0.99215066\n","epoch 33: loss = 0.04175663\n","Start of epoch 34\n","Training loss over epoch:  34 0.041013014\n","Training acc over epoch:  34 0.9889558\n","Validation loss:  34 0.03198184\n","Validation acc:  34 0.9921801\n","epoch 34: loss = 0.041013017\n","Start of epoch 35\n","Training loss over epoch:  35 0.03949797\n","Training acc over epoch:  35 0.98919696\n","Validation loss:  35 0.030931156\n","Validation acc:  35 0.99254125\n","epoch 35: loss = 0.03949797\n","Start of epoch 36\n","Training loss over epoch:  36 0.03861411\n","Training acc over epoch:  36 0.98951656\n","Validation loss:  36 0.030150412\n","Validation acc:  36 0.99266654\n","epoch 36: loss = 0.038614105\n","Start of epoch 37\n","Training loss over epoch:  37 0.03762586\n","Training acc over epoch:  37 0.9897321\n","Validation loss:  37 0.029619073\n","Validation acc:  37 0.9926813\n","epoch 37: loss = 0.037625868\n","Start of epoch 38\n","Training loss over epoch:  38 0.03640744\n","Training acc over epoch:  38 0.9899892\n","Validation loss:  38 0.029027617\n","Validation acc:  38 0.9930498\n","epoch 38: loss = 0.036407445\n","Start of epoch 39\n","Training loss over epoch:  39 0.03540641\n","Training acc over epoch:  39 0.99030167\n","Validation loss:  39 0.02875508\n","Validation acc:  39 0.99296874\n","epoch 39: loss = 0.03540641\n","Start of epoch 40\n","Training loss over epoch:  40 0.03461531\n","Training acc over epoch:  40 0.99058926\n","Validation loss:  40 0.029074471\n","Validation acc:  40 0.99286556\n","epoch 40: loss = 0.034615308\n","Start of epoch 41\n","Training loss over epoch:  41 0.033702116\n","Training acc over epoch:  41 0.9907623\n","Validation loss:  41 0.028139817\n","Validation acc:  41 0.99331516\n","epoch 41: loss = 0.033702113\n","Start of epoch 42\n","Training loss over epoch:  42 0.03312447\n","Training acc over epoch:  42 0.99084324\n","Validation loss:  42 0.02774081\n","Validation acc:  42 0.99315304\n","epoch 42: loss = 0.033124465\n","Start of epoch 43\n","Training loss over epoch:  43 0.032113936\n","Training acc over epoch:  43 0.9911693\n","Validation loss:  43 0.027479297\n","Validation acc:  43 0.9931972\n","epoch 43: loss = 0.03211394\n","Start of epoch 44\n","Training loss over epoch:  44 0.03172653\n","Training acc over epoch:  44 0.99116206\n","Validation loss:  44 0.02775251\n","Validation acc:  44 0.99341094\n","epoch 44: loss = 0.031726535\n","Start of epoch 45\n","Training loss over epoch:  45 0.03103776\n","Training acc over epoch:  45 0.9912975\n","Validation loss:  45 0.02678106\n","Validation acc:  45 0.99346995\n","epoch 45: loss = 0.03103776\n","Start of epoch 46\n","Training loss over epoch:  46 0.030144846\n","Training acc over epoch:  46 0.991525\n","Validation loss:  46 0.026596693\n","Validation acc:  46 0.993551\n","epoch 46: loss = 0.030144852\n","Start of epoch 47\n","Training loss over epoch:  47 0.02948214\n","Training acc over epoch:  47 0.99172044\n","Validation loss:  47 0.026682835\n","Validation acc:  47 0.9935142\n","epoch 47: loss = 0.029482141\n","Start of epoch 48\n","Training loss over epoch:  48 0.02902738\n","Training acc over epoch:  48 0.99172604\n","Validation loss:  48 0.026810486\n","Validation acc:  48 0.99344045\n","epoch 48: loss = 0.029027378\n","Start of epoch 49\n","Training loss over epoch:  49 0.028066142\n","Training acc over epoch:  49 0.99195194\n","Validation loss:  49 0.026642846\n","Validation acc:  49 0.9935215\n","epoch 49: loss = 0.02806615\n","Start of epoch 50\n","Training loss over epoch:  50 0.027731717\n","Training acc over epoch:  50 0.9920641\n","Validation loss:  50 0.026189951\n","Validation acc:  50 0.9935215\n","epoch 50: loss = 0.027731717\n","Start of epoch 51\n","Training loss over epoch:  51 0.027098928\n","Training acc over epoch:  51 0.9922115\n","Validation loss:  51 0.025799789\n","Validation acc:  51 0.9936616\n","epoch 51: loss = 0.027098922\n","Start of epoch 52\n","Training loss over epoch:  52 0.026362106\n","Training acc over epoch:  52 0.9923301\n","Validation loss:  52 0.02655546\n","Validation acc:  52 0.993809\n","epoch 52: loss = 0.0263621\n","Start of epoch 53\n","Training loss over epoch:  53 0.026122952\n","Training acc over epoch:  53 0.99249595\n","Validation loss:  53 0.02616392\n","Validation acc:  53 0.99363947\n","epoch 53: loss = 0.026122948\n","Start of epoch 54\n","Training loss over epoch:  54 0.025600024\n","Training acc over epoch:  54 0.99242544\n","Validation loss:  54 0.02573923\n","Validation acc:  54 0.99358785\n","epoch 54: loss = 0.025600027\n","Start of epoch 55\n","Training loss over epoch:  55 0.025125597\n","Training acc over epoch:  55 0.9926049\n","Validation loss:  55 0.026142359\n","Validation acc:  55 0.9934552\n","epoch 55: loss = 0.025125599\n","Start of epoch 56\n","Training loss over epoch:  56 0.024259191\n","Training acc over epoch:  56 0.99273545\n","Validation loss:  56 0.026510527\n","Validation acc:  56 0.9938163\n","epoch 56: loss = 0.02425919\n","Start of epoch 57\n","Training loss over epoch:  57 0.023946624\n","Training acc over epoch:  57 0.99287486\n","Validation loss:  57 0.025957564\n","Validation acc:  57 0.99391216\n","epoch 57: loss = 0.023946622\n","Start of epoch 58\n","Training loss over epoch:  58 0.023425614\n","Training acc over epoch:  58 0.99298865\n","Validation loss:  58 0.025899896\n","Validation acc:  58 0.9938458\n","epoch 58: loss = 0.023425614\n","Start of epoch 59\n","Training loss over epoch:  59 0.023246033\n","Training acc over epoch:  59 0.9931152\n","Validation loss:  59 0.025551606\n","Validation acc:  59 0.9936689\n","epoch 59: loss = 0.023246031\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:11<00:00, 75.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9509: \n","Sensitivity: 0.18064045407514415\n","Specificity: 0.9395821403491156\n","F1-score macro: 0.2093057608839328\n","F1-score micro: 0.9509409751924722\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 507676.0\n","valid_X_fake diff: 56446.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.0259941\n","Training acc over epoch:  0 0.82111925\n","Validation loss:  0 0.2898954\n","Validation acc:  0 0.94966096\n","epoch 0: loss = 1.025994\n","Start of epoch 1\n","Training loss over epoch:  1 0.2800139\n","Training acc over epoch:  1 0.94946146\n","Validation loss:  1 0.26235276\n","Validation acc:  1 0.94966096\n","epoch 1: loss = 0.28001386\n","Start of epoch 2\n","Training loss over epoch:  2 0.25922915\n","Training acc over epoch:  2 0.94942707\n","Validation loss:  2 0.24491425\n","Validation acc:  2 0.94966096\n","epoch 2: loss = 0.25922915\n","Start of epoch 3\n","Training loss over epoch:  3 0.2469185\n","Training acc over epoch:  3 0.9494302\n","Validation loss:  3 0.23654099\n","Validation acc:  3 0.94966096\n","epoch 3: loss = 0.24691848\n","Start of epoch 4\n","Training loss over epoch:  4 0.24093984\n","Training acc over epoch:  4 0.94942385\n","Validation loss:  4 0.23266117\n","Validation acc:  4 0.94966096\n","epoch 4: loss = 0.24093981\n","Start of epoch 5\n","Training loss over epoch:  5 0.235457\n","Training acc over epoch:  5 0.9494679\n","Validation loss:  5 0.22303511\n","Validation acc:  5 0.94966096\n","epoch 5: loss = 0.235457\n","Start of epoch 6\n","Training loss over epoch:  6 0.22554328\n","Training acc over epoch:  6 0.94949913\n","Validation loss:  6 0.20824713\n","Validation acc:  6 0.95025057\n","epoch 6: loss = 0.22554334\n","Start of epoch 7\n","Training loss over epoch:  7 0.21277721\n","Training acc over epoch:  7 0.9500647\n","Validation loss:  7 0.19579552\n","Validation acc:  7 0.95142984\n","epoch 7: loss = 0.21277723\n","Start of epoch 8\n","Training loss over epoch:  8 0.20136523\n","Training acc over epoch:  8 0.9506303\n","Validation loss:  8 0.18391533\n","Validation acc:  8 0.9519458\n","epoch 8: loss = 0.20136522\n","Start of epoch 9\n","Training loss over epoch:  9 0.19010715\n","Training acc over epoch:  9 0.9514795\n","Validation loss:  9 0.17179969\n","Validation acc:  9 0.9530292\n","epoch 9: loss = 0.19010714\n","Start of epoch 10\n","Training loss over epoch:  10 0.17891014\n","Training acc over epoch:  10 0.952287\n","Validation loss:  10 0.1605013\n","Validation acc:  10 0.95437795\n","epoch 10: loss = 0.17891014\n","Start of epoch 11\n","Training loss over epoch:  11 0.16735114\n","Training acc over epoch:  11 0.9534791\n","Validation loss:  11 0.14757815\n","Validation acc:  11 0.955653\n","epoch 11: loss = 0.16735111\n","Start of epoch 12\n","Training loss over epoch:  12 0.15549786\n","Training acc over epoch:  12 0.9549804\n","Validation loss:  12 0.13257197\n","Validation acc:  12 0.9590065\n","epoch 12: loss = 0.15549788\n","Start of epoch 13\n","Training loss over epoch:  13 0.14298391\n","Training acc over epoch:  13 0.957846\n","Validation loss:  13 0.11861284\n","Validation acc:  13 0.9638709\n","epoch 13: loss = 0.1429839\n","Start of epoch 14\n","Training loss over epoch:  14 0.13250433\n","Training acc over epoch:  14 0.96083814\n","Validation loss:  14 0.10917259\n","Validation acc:  14 0.9681677\n","epoch 14: loss = 0.13250434\n","Start of epoch 15\n","Training loss over epoch:  15 0.12333223\n","Training acc over epoch:  15 0.963429\n","Validation loss:  15 0.10201343\n","Validation acc:  15 0.9711085\n","epoch 15: loss = 0.12333223\n","Start of epoch 16\n","Training loss over epoch:  16 0.11717281\n","Training acc over epoch:  16 0.9652355\n","Validation loss:  16 0.09397937\n","Validation acc:  16 0.9727226\n","epoch 16: loss = 0.11717279\n","Start of epoch 17\n","Training loss over epoch:  17 0.11043756\n","Training acc over epoch:  17 0.9673889\n","Validation loss:  17 0.08871754\n","Validation acc:  17 0.9749116\n","epoch 17: loss = 0.110437535\n","Start of epoch 18\n","Training loss over epoch:  18 0.104495175\n","Training acc over epoch:  18 0.9694213\n","Validation loss:  18 0.08295776\n","Validation acc:  18 0.976732\n","epoch 18: loss = 0.10449519\n","Start of epoch 19\n","Training loss over epoch:  19 0.09943762\n","Training acc over epoch:  19 0.97113895\n","Validation loss:  19 0.07923603\n","Validation acc:  19 0.9780071\n","epoch 19: loss = 0.0994376\n","Start of epoch 20\n","Training loss over epoch:  20 0.09451366\n","Training acc over epoch:  20 0.9727035\n","Validation loss:  20 0.07400741\n","Validation acc:  20 0.9795401\n","epoch 20: loss = 0.09451368\n","Start of epoch 21\n","Training loss over epoch:  21 0.090570524\n","Training acc over epoch:  21 0.9740534\n","Validation loss:  21 0.071105525\n","Validation acc:  21 0.98079306\n","epoch 21: loss = 0.09057055\n","Start of epoch 22\n","Training loss over epoch:  22 0.08677433\n","Training acc over epoch:  22 0.97543776\n","Validation loss:  22 0.06828167\n","Validation acc:  22 0.98136055\n","epoch 22: loss = 0.08677433\n","Start of epoch 23\n","Training loss over epoch:  23 0.08385486\n","Training acc over epoch:  23 0.9763566\n","Validation loss:  23 0.069395155\n","Validation acc:  23 0.9819723\n","epoch 23: loss = 0.08385487\n","Start of epoch 24\n","Training loss over epoch:  24 0.080846995\n","Training acc over epoch:  24 0.9772475\n","Validation loss:  24 0.06498176\n","Validation acc:  24 0.98267245\n","epoch 24: loss = 0.08084698\n","Start of epoch 25\n","Training loss over epoch:  25 0.07910929\n","Training acc over epoch:  25 0.9779861\n","Validation loss:  25 0.063839145\n","Validation acc:  25 0.98284197\n","epoch 25: loss = 0.079109296\n","Start of epoch 26\n","Training loss over epoch:  26 0.07675245\n","Training acc over epoch:  26 0.9787343\n","Validation loss:  26 0.061445184\n","Validation acc:  26 0.9837854\n","epoch 26: loss = 0.07675243\n","Start of epoch 27\n","Training loss over epoch:  27 0.0746808\n","Training acc over epoch:  27 0.979376\n","Validation loss:  27 0.0599013\n","Validation acc:  27 0.984375\n","epoch 27: loss = 0.07468079\n","Start of epoch 28\n","Training loss over epoch:  28 0.07286445\n","Training acc over epoch:  28 0.9800161\n","Validation loss:  28 0.058795284\n","Validation acc:  28 0.98458135\n","epoch 28: loss = 0.07286446\n","Start of epoch 29\n","Training loss over epoch:  29 0.07133177\n","Training acc over epoch:  29 0.9804888\n","Validation loss:  29 0.05715589\n","Validation acc:  29 0.9849425\n","epoch 29: loss = 0.071331784\n","Start of epoch 30\n","Training loss over epoch:  30 0.069919616\n","Training acc over epoch:  30 0.98087573\n","Validation loss:  30 0.057137392\n","Validation acc:  30 0.9850752\n","epoch 30: loss = 0.06991961\n","Start of epoch 31\n","Training loss over epoch:  31 0.06801459\n","Training acc over epoch:  31 0.9813724\n","Validation loss:  31 0.05565358\n","Validation acc:  31 0.98565745\n","epoch 31: loss = 0.06801459\n","Start of epoch 32\n","Training loss over epoch:  32 0.06669344\n","Training acc over epoch:  32 0.98191714\n","Validation loss:  32 0.055002153\n","Validation acc:  32 0.9856943\n","epoch 32: loss = 0.06669342\n","Start of epoch 33\n","Training loss over epoch:  33 0.065659516\n","Training acc over epoch:  33 0.9821559\n","Validation loss:  33 0.05361181\n","Validation acc:  33 0.9860186\n","epoch 33: loss = 0.065659516\n","Start of epoch 34\n","Training loss over epoch:  34 0.06414341\n","Training acc over epoch:  34 0.9824796\n","Validation loss:  34 0.054107074\n","Validation acc:  34 0.98593014\n","epoch 34: loss = 0.06414343\n","Start of epoch 35\n","Training loss over epoch:  35 0.06280503\n","Training acc over epoch:  35 0.9828713\n","Validation loss:  35 0.052004363\n","Validation acc:  35 0.9864755\n","epoch 35: loss = 0.06280501\n","Start of epoch 36\n","Training loss over epoch:  36 0.061536435\n","Training acc over epoch:  36 0.9832494\n","Validation loss:  36 0.0518825\n","Validation acc:  36 0.9867188\n","epoch 36: loss = 0.061536428\n","Start of epoch 37\n","Training loss over epoch:  37 0.059942394\n","Training acc over epoch:  37 0.9836268\n","Validation loss:  37 0.051259827\n","Validation acc:  37 0.9869325\n","epoch 37: loss = 0.059942402\n","Start of epoch 38\n","Training loss over epoch:  38 0.058920044\n","Training acc over epoch:  38 0.9838775\n","Validation loss:  38 0.050180018\n","Validation acc:  38 0.98691773\n","epoch 38: loss = 0.058920052\n","Start of epoch 39\n","Training loss over epoch:  39 0.05788823\n","Training acc over epoch:  39 0.98413545\n","Validation loss:  39 0.050130107\n","Validation acc:  39 0.98682195\n","epoch 39: loss = 0.05788823\n","Start of epoch 40\n","Training loss over epoch:  40 0.056934636\n","Training acc over epoch:  40 0.98446393\n","Validation loss:  40 0.049234435\n","Validation acc:  40 0.9870209\n","epoch 40: loss = 0.056934644\n","Start of epoch 41\n","Training loss over epoch:  41 0.055966992\n","Training acc over epoch:  41 0.9845945\n","Validation loss:  41 0.04869171\n","Validation acc:  41 0.9874926\n","epoch 41: loss = 0.05596699\n","Start of epoch 42\n","Training loss over epoch:  42 0.05483366\n","Training acc over epoch:  42 0.98491734\n","Validation loss:  42 0.04879722\n","Validation acc:  42 0.9873305\n","epoch 42: loss = 0.054833677\n","Start of epoch 43\n","Training loss over epoch:  43 0.054144222\n","Training acc over epoch:  43 0.98505276\n","Validation loss:  43 0.048408896\n","Validation acc:  43 0.98754424\n","epoch 43: loss = 0.05414421\n","Start of epoch 44\n","Training loss over epoch:  44 0.053101145\n","Training acc over epoch:  44 0.98534596\n","Validation loss:  44 0.047509603\n","Validation acc:  44 0.9878906\n","epoch 44: loss = 0.05310114\n","Start of epoch 45\n","Training loss over epoch:  45 0.05190994\n","Training acc over epoch:  45 0.98556465\n","Validation loss:  45 0.047358546\n","Validation acc:  45 0.9879201\n","epoch 45: loss = 0.051909927\n","Start of epoch 46\n","Training loss over epoch:  46 0.051507816\n","Training acc over epoch:  46 0.98563033\n","Validation loss:  46 0.047262564\n","Validation acc:  46 0.98780954\n","epoch 46: loss = 0.05150781\n","Start of epoch 47\n","Training loss over epoch:  47 0.05080276\n","Training acc over epoch:  47 0.9858394\n","Validation loss:  47 0.046569623\n","Validation acc:  47 0.98815596\n","epoch 47: loss = 0.050802764\n","Start of epoch 48\n","Training loss over epoch:  48 0.049732648\n","Training acc over epoch:  48 0.98608696\n","Validation loss:  48 0.04669757\n","Validation acc:  48 0.9878685\n","epoch 48: loss = 0.049732637\n","Start of epoch 49\n","Training loss over epoch:  49 0.048717916\n","Training acc over epoch:  49 0.9861527\n","Validation loss:  49 0.046203528\n","Validation acc:  49 0.9881781\n","epoch 49: loss = 0.04871791\n","Start of epoch 50\n","Training loss over epoch:  50 0.047785502\n","Training acc over epoch:  50 0.9865196\n","Validation loss:  50 0.04625699\n","Validation acc:  50 0.9882444\n","epoch 50: loss = 0.047785494\n","Start of epoch 51\n","Training loss over epoch:  51 0.04734011\n","Training acc over epoch:  51 0.98669344\n","Validation loss:  51 0.045759965\n","Validation acc:  51 0.9879717\n","epoch 51: loss = 0.047340114\n","Start of epoch 52\n","Training loss over epoch:  52 0.046241574\n","Training acc over epoch:  52 0.986816\n","Validation loss:  52 0.046053182\n","Validation acc:  52 0.9882075\n","epoch 52: loss = 0.046241563\n","Start of epoch 53\n","Training loss over epoch:  53 0.045467842\n","Training acc over epoch:  53 0.9870315\n","Validation loss:  53 0.045896783\n","Validation acc:  53 0.98828125\n","epoch 53: loss = 0.04546784\n","Start of epoch 54\n","Training loss over epoch:  54 0.04473745\n","Training acc over epoch:  54 0.98715085\n","Validation loss:  54 0.045770224\n","Validation acc:  54 0.98795694\n","epoch 54: loss = 0.044737436\n","Start of epoch 55\n","Training loss over epoch:  55 0.043854743\n","Training acc over epoch:  55 0.98729825\n","Validation loss:  55 0.045464605\n","Validation acc:  55 0.98813385\n","epoch 55: loss = 0.043854743\n","Start of epoch 56\n","Training loss over epoch:  56 0.043255556\n","Training acc over epoch:  56 0.9875522\n","Validation loss:  56 0.04492745\n","Validation acc:  56 0.9883181\n","epoch 56: loss = 0.043255556\n","Start of epoch 57\n","Training loss over epoch:  57 0.042361584\n","Training acc over epoch:  57 0.98770684\n","Validation loss:  57 0.045156345\n","Validation acc:  57 0.9882665\n","epoch 57: loss = 0.042361584\n","Start of epoch 58\n","Training loss over epoch:  58 0.041371558\n","Training acc over epoch:  58 0.9878943\n","Validation loss:  58 0.044413168\n","Validation acc:  58 0.98808223\n","epoch 58: loss = 0.041371565\n","Start of epoch 59\n","Training loss over epoch:  59 0.041015733\n","Training acc over epoch:  59 0.98794556\n","Validation loss:  59 0.045517553\n","Validation acc:  59 0.98827386\n","epoch 59: loss = 0.041015733\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:11<00:00, 75.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9532: \n","Sensitivity: 0.11678529248789078\n","Specificity: 0.957190665553605\n","F1-score macro: 0.13832989999134732\n","F1-score micro: 0.9531800304716012\n","Training using fold 2\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 126168.0\n","valid_X_fake diff: 14028.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.87990344\n","Training acc over epoch:  0 0.88961077\n","Validation loss:  0 0.2917544\n","Validation acc:  0 0.94892395\n","epoch 0: loss = 0.8799034\n","Start of epoch 1\n","Training loss over epoch:  1 0.28441942\n","Training acc over epoch:  1 0.94921315\n","Validation loss:  1 0.27215222\n","Validation acc:  1 0.94892395\n","epoch 1: loss = 0.28441945\n","Start of epoch 2\n","Training loss over epoch:  2 0.2622411\n","Training acc over epoch:  2 0.94927645\n","Validation loss:  2 0.2522594\n","Validation acc:  2 0.94892395\n","epoch 2: loss = 0.2622411\n","Start of epoch 3\n","Training loss over epoch:  3 0.25088766\n","Training acc over epoch:  3 0.9493125\n","Validation loss:  3 0.24370886\n","Validation acc:  3 0.94892395\n","epoch 3: loss = 0.25088766\n","Start of epoch 4\n","Training loss over epoch:  4 0.2394813\n","Training acc over epoch:  4 0.9492981\n","Validation loss:  4 0.22961468\n","Validation acc:  4 0.94892395\n","epoch 4: loss = 0.23948129\n","Start of epoch 5\n","Training loss over epoch:  5 0.22704467\n","Training acc over epoch:  5 0.949371\n","Validation loss:  5 0.21616198\n","Validation acc:  5 0.9496757\n","epoch 5: loss = 0.2270447\n","Start of epoch 6\n","Training loss over epoch:  6 0.21119468\n","Training acc over epoch:  6 0.9499245\n","Validation loss:  6 0.19420151\n","Validation acc:  6 0.95065594\n","epoch 6: loss = 0.21119468\n","Start of epoch 7\n","Training loss over epoch:  7 0.19337757\n","Training acc over epoch:  7 0.9507833\n","Validation loss:  7 0.17825215\n","Validation acc:  7 0.95154774\n","epoch 7: loss = 0.19337754\n","Start of epoch 8\n","Training loss over epoch:  8 0.17853212\n","Training acc over epoch:  8 0.951695\n","Validation loss:  8 0.1651945\n","Validation acc:  8 0.95305866\n","epoch 8: loss = 0.17853217\n","Start of epoch 9\n","Training loss over epoch:  9 0.16309203\n","Training acc over epoch:  9 0.9528446\n","Validation loss:  9 0.14255178\n","Validation acc:  9 0.954916\n","epoch 9: loss = 0.16309205\n","Start of epoch 10\n","Training loss over epoch:  10 0.14331599\n","Training acc over epoch:  10 0.9559161\n","Validation loss:  10 0.11732921\n","Validation acc:  10 0.9623747\n","epoch 10: loss = 0.14331599\n","Start of epoch 11\n","Training loss over epoch:  11 0.123682685\n","Training acc over epoch:  11 0.96081173\n","Validation loss:  11 0.098952875\n","Validation acc:  11 0.96982604\n","epoch 11: loss = 0.12368266\n","Start of epoch 12\n","Training loss over epoch:  12 0.106700405\n","Training acc over epoch:  12 0.96562564\n","Validation loss:  12 0.08347308\n","Validation acc:  12 0.97414505\n","epoch 12: loss = 0.106700376\n","Start of epoch 13\n","Training loss over epoch:  13 0.094032004\n","Training acc over epoch:  13 0.96970415\n","Validation loss:  13 0.072375685\n","Validation acc:  13 0.97752064\n","epoch 13: loss = 0.094032004\n","Start of epoch 14\n","Training loss over epoch:  14 0.084579974\n","Training acc over epoch:  14 0.97272193\n","Validation loss:  14 0.06605729\n","Validation acc:  14 0.98029184\n","epoch 14: loss = 0.08457997\n","Start of epoch 15\n","Training loss over epoch:  15 0.076618254\n","Training acc over epoch:  15 0.9754521\n","Validation loss:  15 0.059792962\n","Validation acc:  15 0.9823187\n","epoch 15: loss = 0.076618254\n","Start of epoch 16\n","Training loss over epoch:  16 0.07018592\n","Training acc over epoch:  16 0.97767365\n","Validation loss:  16 0.053961977\n","Validation acc:  16 0.98400646\n","epoch 16: loss = 0.07018594\n","Start of epoch 17\n","Training loss over epoch:  17 0.06511545\n","Training acc over epoch:  17 0.97945774\n","Validation loss:  17 0.04946233\n","Validation acc:  17 0.98534054\n","epoch 17: loss = 0.06511547\n","Start of epoch 18\n","Training loss over epoch:  18 0.060172033\n","Training acc over epoch:  18 0.9813011\n","Validation loss:  18 0.04456304\n","Validation acc:  18 0.9868588\n","epoch 18: loss = 0.06017204\n","Start of epoch 19\n","Training loss over epoch:  19 0.05568928\n","Training acc over epoch:  19 0.9829426\n","Validation loss:  19 0.041315317\n","Validation acc:  19 0.9883697\n","epoch 19: loss = 0.055689283\n","Start of epoch 20\n","Training loss over epoch:  20 0.051414788\n","Training acc over epoch:  20 0.98439103\n","Validation loss:  20 0.03825967\n","Validation acc:  20 0.9893573\n","epoch 20: loss = 0.051414784\n","Start of epoch 21\n","Training loss over epoch:  21 0.048338816\n","Training acc over epoch:  21 0.9853764\n","Validation loss:  21 0.035471275\n","Validation acc:  21 0.9902786\n","epoch 21: loss = 0.048338823\n","Start of epoch 22\n","Training loss over epoch:  22 0.04487807\n","Training acc over epoch:  22 0.9864932\n","Validation loss:  22 0.032293558\n","Validation acc:  22 0.99111146\n","epoch 22: loss = 0.044878077\n","Start of epoch 23\n","Training loss over epoch:  23 0.041984614\n","Training acc over epoch:  23 0.98750895\n","Validation loss:  23 0.03076843\n","Validation acc:  23 0.99196637\n","epoch 23: loss = 0.041984614\n","Start of epoch 24\n","Training loss over epoch:  24 0.0396766\n","Training acc over epoch:  24 0.9882556\n","Validation loss:  24 0.02854241\n","Validation acc:  24 0.99272555\n","epoch 24: loss = 0.039676603\n","Start of epoch 25\n","Training loss over epoch:  25 0.037748437\n","Training acc over epoch:  25 0.9889614\n","Validation loss:  25 0.027022026\n","Validation acc:  25 0.9930719\n","epoch 25: loss = 0.037748434\n","Start of epoch 26\n","Training loss over epoch:  26 0.03554176\n","Training acc over epoch:  26 0.98959506\n","Validation loss:  26 0.025700565\n","Validation acc:  26 0.993352\n","epoch 26: loss = 0.035541765\n","Start of epoch 27\n","Training loss over epoch:  27 0.033680573\n","Training acc over epoch:  27 0.9901799\n","Validation loss:  27 0.02466403\n","Validation acc:  27 0.9938163\n","epoch 27: loss = 0.03368057\n","Start of epoch 28\n","Training loss over epoch:  28 0.031973977\n","Training acc over epoch:  28 0.99066055\n","Validation loss:  28 0.02416795\n","Validation acc:  28 0.99415535\n","epoch 28: loss = 0.031973973\n","Start of epoch 29\n","Training loss over epoch:  29 0.030606084\n","Training acc over epoch:  29 0.9909434\n","Validation loss:  29 0.022489013\n","Validation acc:  29 0.9943765\n","epoch 29: loss = 0.03060608\n","Start of epoch 30\n","Training loss over epoch:  30 0.029237304\n","Training acc over epoch:  30 0.99152094\n","Validation loss:  30 0.022326281\n","Validation acc:  30 0.9945239\n","epoch 30: loss = 0.029237304\n","Start of epoch 31\n","Training loss over epoch:  31 0.028279662\n","Training acc over epoch:  31 0.9917493\n","Validation loss:  31 0.021317508\n","Validation acc:  31 0.9948113\n","epoch 31: loss = 0.028279662\n","Start of epoch 32\n","Training loss over epoch:  32 0.027095169\n","Training acc over epoch:  32 0.9920794\n","Validation loss:  32 0.021174766\n","Validation acc:  32 0.9947155\n","epoch 32: loss = 0.027095173\n","Start of epoch 33\n","Training loss over epoch:  33 0.026234306\n","Training acc over epoch:  33 0.9923309\n","Validation loss:  33 0.020663243\n","Validation acc:  33 0.9950324\n","epoch 33: loss = 0.026234305\n","Start of epoch 34\n","Training loss over epoch:  34 0.025266929\n","Training acc over epoch:  34 0.9926722\n","Validation loss:  34 0.01930645\n","Validation acc:  34 0.99504715\n","epoch 34: loss = 0.02526694\n","Start of epoch 35\n","Training loss over epoch:  35 0.024169357\n","Training acc over epoch:  35 0.99291974\n","Validation loss:  35 0.01900045\n","Validation acc:  35 0.9952904\n","epoch 35: loss = 0.024169354\n","Start of epoch 36\n","Training loss over epoch:  36 0.023373285\n","Training acc over epoch:  36 0.9931328\n","Validation loss:  36 0.01897522\n","Validation acc:  36 0.9954378\n","epoch 36: loss = 0.023373287\n","Start of epoch 37\n","Training loss over epoch:  37 0.022453174\n","Training acc over epoch:  37 0.9934509\n","Validation loss:  37 0.018650064\n","Validation acc:  37 0.995482\n","epoch 37: loss = 0.022453174\n","Start of epoch 38\n","Training loss over epoch:  38 0.021909371\n","Training acc over epoch:  38 0.99350613\n","Validation loss:  38 0.01811963\n","Validation acc:  38 0.9957252\n","epoch 38: loss = 0.02190937\n","Start of epoch 39\n","Training loss over epoch:  39 0.021032527\n","Training acc over epoch:  39 0.99386424\n","Validation loss:  39 0.017641138\n","Validation acc:  39 0.99569577\n","epoch 39: loss = 0.021032529\n","Start of epoch 40\n","Training loss over epoch:  40 0.02027113\n","Training acc over epoch:  40 0.9939708\n","Validation loss:  40 0.017828928\n","Validation acc:  40 0.99581367\n","epoch 40: loss = 0.020271132\n","Start of epoch 41\n","Training loss over epoch:  41 0.019685363\n","Training acc over epoch:  41 0.9940701\n","Validation loss:  41 0.017647315\n","Validation acc:  41 0.9957179\n","epoch 41: loss = 0.019685363\n","Start of epoch 42\n","Training loss over epoch:  42 0.019005708\n","Training acc over epoch:  42 0.9943297\n","Validation loss:  42 0.016769556\n","Validation acc:  42 0.99597585\n","epoch 42: loss = 0.01900571\n","Start of epoch 43\n","Training loss over epoch:  43 0.018708516\n","Training acc over epoch:  43 0.9944242\n","Validation loss:  43 0.01680636\n","Validation acc:  43 0.99592423\n","epoch 43: loss = 0.018708514\n","Start of epoch 44\n","Training loss over epoch:  44 0.017957225\n","Training acc over epoch:  44 0.9946077\n","Validation loss:  44 0.016510773\n","Validation acc:  44 0.99597585\n","epoch 44: loss = 0.017957227\n","Start of epoch 45\n","Training loss over epoch:  45 0.01730105\n","Training acc over epoch:  45 0.9946886\n","Validation loss:  45 0.016551508\n","Validation acc:  45 0.99581367\n","epoch 45: loss = 0.017301047\n","Start of epoch 46\n","Training loss over epoch:  46 0.016716937\n","Training acc over epoch:  46 0.99488163\n","Validation loss:  46 0.016312715\n","Validation acc:  46 0.99593896\n","epoch 46: loss = 0.016716942\n","Start of epoch 47\n","Training loss over epoch:  47 0.016648361\n","Training acc over epoch:  47 0.99493295\n","Validation loss:  47 0.016167019\n","Validation acc:  47 0.99593896\n","epoch 47: loss = 0.016648358\n","Start of epoch 48\n","Training loss over epoch:  48 0.015899843\n","Training acc over epoch:  48 0.9951308\n","Validation loss:  48 0.016261643\n","Validation acc:  48 0.9960348\n","epoch 48: loss = 0.015899843\n","Start of epoch 49\n","Training loss over epoch:  49 0.015309312\n","Training acc over epoch:  49 0.9952173\n","Validation loss:  49 0.016426424\n","Validation acc:  49 0.9961675\n","epoch 49: loss = 0.015309314\n","Start of epoch 50\n","Training loss over epoch:  50 0.015056083\n","Training acc over epoch:  50 0.99531347\n","Validation loss:  50 0.016712714\n","Validation acc:  50 0.99609375\n","epoch 50: loss = 0.015056083\n","Start of epoch 51\n","Training loss over epoch:  51 0.01460483\n","Training acc over epoch:  51 0.9955202\n","Validation loss:  51 0.016244624\n","Validation acc:  51 0.9961601\n","epoch 51: loss = 0.014604827\n","Start of epoch 52\n","Training loss over epoch:  52 0.014091329\n","Training acc over epoch:  52 0.9955474\n","Validation loss:  52 0.017267313\n","Validation acc:  52 0.9963222\n","epoch 52: loss = 0.014091331\n","Start of epoch 53\n","Training loss over epoch:  53 0.013746113\n","Training acc over epoch:  53 0.99566114\n","Validation loss:  53 0.016773397\n","Validation acc:  53 0.9964623\n","epoch 53: loss = 0.013746115\n","Start of epoch 54\n","Training loss over epoch:  54 0.013227164\n","Training acc over epoch:  54 0.99582696\n","Validation loss:  54 0.016029594\n","Validation acc:  54 0.9964254\n","epoch 54: loss = 0.013227164\n","Start of epoch 55\n","Training loss over epoch:  55 0.013168931\n","Training acc over epoch:  55 0.99586785\n","Validation loss:  55 0.015919441\n","Validation acc:  55 0.9963222\n","epoch 55: loss = 0.013168932\n","Start of epoch 56\n","Training loss over epoch:  56 0.012906147\n","Training acc over epoch:  56 0.99592954\n","Validation loss:  56 0.01606219\n","Validation acc:  56 0.9962854\n","epoch 56: loss = 0.012906147\n","Start of epoch 57\n","Training loss over epoch:  57 0.012144973\n","Training acc over epoch:  57 0.99607855\n","Validation loss:  57 0.016737202\n","Validation acc:  57 0.9964402\n","epoch 57: loss = 0.0121449735\n","Start of epoch 58\n","Training loss over epoch:  58 0.011842177\n","Training acc over epoch:  58 0.9961995\n","Validation loss:  58 0.015554264\n","Validation acc:  58 0.99649173\n","epoch 58: loss = 0.011842177\n","Start of epoch 59\n","Training loss over epoch:  59 0.011556402\n","Training acc over epoch:  59 0.9962644\n","Validation loss:  59 0.01678022\n","Validation acc:  59 0.99637383\n","epoch 59: loss = 0.011556403\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:11<00:00, 75.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9531: \n","Sensitivity: 0.274700892643173\n","Specificity: 0.9314055208883303\n","F1-score macro: 0.32350968151227877\n","F1-score micro: 0.9530652979754776\n","Missing rate 0.1\n","=====================================================\n","train_X_fake diff: 252336.0\n","valid_X_fake diff: 28056.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.7409554\n","Training acc over epoch:  0 0.68188816\n","Validation loss:  0 0.29982334\n","Validation acc:  0 0.94892395\n","epoch 0: loss = 1.7409554\n","Start of epoch 1\n","Training loss over epoch:  1 0.28195128\n","Training acc over epoch:  1 0.9492644\n","Validation loss:  1 0.27502963\n","Validation acc:  1 0.94892395\n","epoch 1: loss = 0.28195134\n","Start of epoch 2\n","Training loss over epoch:  2 0.27153024\n","Training acc over epoch:  2 0.9492716\n","Validation loss:  2 0.2659143\n","Validation acc:  2 0.94892395\n","epoch 2: loss = 0.27153027\n","Start of epoch 3\n","Training loss over epoch:  3 0.26489744\n","Training acc over epoch:  3 0.9493541\n","Validation loss:  3 0.26284808\n","Validation acc:  3 0.94892395\n","epoch 3: loss = 0.2648974\n","Start of epoch 4\n","Training loss over epoch:  4 0.26337788\n","Training acc over epoch:  4 0.9492796\n","Validation loss:  4 0.26189202\n","Validation acc:  4 0.94892395\n","epoch 4: loss = 0.2633779\n","Start of epoch 5\n","Training loss over epoch:  5 0.26283732\n","Training acc over epoch:  5 0.9492628\n","Validation loss:  5 0.2614157\n","Validation acc:  5 0.94892395\n","epoch 5: loss = 0.26283726\n","Start of epoch 6\n","Training loss over epoch:  6 0.26227143\n","Training acc over epoch:  6 0.949238\n","Validation loss:  6 0.2610704\n","Validation acc:  6 0.94892395\n","epoch 6: loss = 0.26227143\n","Start of epoch 7\n","Training loss over epoch:  7 0.2617983\n","Training acc over epoch:  7 0.94929403\n","Validation loss:  7 0.26098353\n","Validation acc:  7 0.94892395\n","epoch 7: loss = 0.26179832\n","Start of epoch 8\n","Training loss over epoch:  8 0.2616144\n","Training acc over epoch:  8 0.9492732\n","Validation loss:  8 0.2607961\n","Validation acc:  8 0.94892395\n","epoch 8: loss = 0.26161435\n","Start of epoch 9\n","Training loss over epoch:  9 0.26113558\n","Training acc over epoch:  9 0.94928205\n","Validation loss:  9 0.26067534\n","Validation acc:  9 0.94892395\n","epoch 9: loss = 0.26113558\n","Start of epoch 10\n","Training loss over epoch:  10 0.26096407\n","Training acc over epoch:  10 0.94929725\n","Validation loss:  10 0.26035777\n","Validation acc:  10 0.94892395\n","epoch 10: loss = 0.260964\n","Start of epoch 11\n","Training loss over epoch:  11 0.26103565\n","Training acc over epoch:  11 0.9492484\n","Validation loss:  11 0.26020104\n","Validation acc:  11 0.94892395\n","epoch 11: loss = 0.26103568\n","Start of epoch 12\n","Training loss over epoch:  12 0.2605158\n","Training acc over epoch:  12 0.94927883\n","Validation loss:  12 0.26006448\n","Validation acc:  12 0.94892395\n","epoch 12: loss = 0.26051587\n","Start of epoch 13\n","Training loss over epoch:  13 0.26073134\n","Training acc over epoch:  13 0.9492436\n","Validation loss:  13 0.25988457\n","Validation acc:  13 0.94892395\n","epoch 13: loss = 0.26073134\n","Start of epoch 14\n","Training loss over epoch:  14 0.25988862\n","Training acc over epoch:  14 0.9493093\n","Validation loss:  14 0.25972903\n","Validation acc:  14 0.94892395\n","epoch 14: loss = 0.2598886\n","Start of epoch 15\n","Training loss over epoch:  15 0.25987077\n","Training acc over epoch:  15 0.94928604\n","Validation loss:  15 0.25945297\n","Validation acc:  15 0.94892395\n","epoch 15: loss = 0.25987077\n","Start of epoch 16\n","Training loss over epoch:  16 0.25971162\n","Training acc over epoch:  16 0.9492965\n","Validation loss:  16 0.2591176\n","Validation acc:  16 0.94892395\n","epoch 16: loss = 0.25971165\n","Start of epoch 17\n","Training loss over epoch:  17 0.25954875\n","Training acc over epoch:  17 0.9492516\n","Validation loss:  17 0.25894046\n","Validation acc:  17 0.94892395\n","epoch 17: loss = 0.25954875\n","Start of epoch 18\n","Training loss over epoch:  18 0.2591774\n","Training acc over epoch:  18 0.94926363\n","Validation loss:  18 0.2585538\n","Validation acc:  18 0.94892395\n","epoch 18: loss = 0.25917745\n","Start of epoch 19\n","Training loss over epoch:  19 0.25890058\n","Training acc over epoch:  19 0.9492548\n","Validation loss:  19 0.25839737\n","Validation acc:  19 0.94892395\n","epoch 19: loss = 0.25890058\n","Start of epoch 20\n","Training loss over epoch:  20 0.2586138\n","Training acc over epoch:  20 0.9492195\n","Validation loss:  20 0.25821638\n","Validation acc:  20 0.94892395\n","epoch 20: loss = 0.25861377\n","Start of epoch 21\n","Training loss over epoch:  21 0.25840786\n","Training acc over epoch:  21 0.9492276\n","Validation loss:  21 0.25790098\n","Validation acc:  21 0.94892395\n","epoch 21: loss = 0.25840786\n","Start of epoch 22\n","Training loss over epoch:  22 0.25800362\n","Training acc over epoch:  22 0.9492772\n","Validation loss:  22 0.25784457\n","Validation acc:  22 0.94892395\n","epoch 22: loss = 0.2580036\n","Start of epoch 23\n","Training loss over epoch:  23 0.2577398\n","Training acc over epoch:  23 0.9492572\n","Validation loss:  23 0.25741956\n","Validation acc:  23 0.94892395\n","epoch 23: loss = 0.2577398\n","Start of epoch 24\n","Training loss over epoch:  24 0.25741035\n","Training acc over epoch:  24 0.9492476\n","Validation loss:  24 0.2570991\n","Validation acc:  24 0.94892395\n","epoch 24: loss = 0.25741035\n","Start of epoch 25\n","Training loss over epoch:  25 0.25721607\n","Training acc over epoch:  25 0.949254\n","Validation loss:  25 0.25773564\n","Validation acc:  25 0.94892395\n","epoch 25: loss = 0.25721604\n","Start of epoch 26\n","Training loss over epoch:  26 0.25688767\n","Training acc over epoch:  26 0.94928443\n","Validation loss:  26 0.2565873\n","Validation acc:  26 0.94892395\n","epoch 26: loss = 0.25688767\n","Start of epoch 27\n","Training loss over epoch:  27 0.25653002\n","Training acc over epoch:  27 0.9492604\n","Validation loss:  27 0.25659528\n","Validation acc:  27 0.94892395\n","epoch 27: loss = 0.25653005\n","Start of epoch 28\n","Training loss over epoch:  28 0.25618523\n","Training acc over epoch:  28 0.94928205\n","Validation loss:  28 0.25605068\n","Validation acc:  28 0.94892395\n","epoch 28: loss = 0.25618517\n","Start of epoch 29\n","Training loss over epoch:  29 0.2559518\n","Training acc over epoch:  29 0.94929886\n","Validation loss:  29 0.25582743\n","Validation acc:  29 0.94892395\n","epoch 29: loss = 0.25595185\n","Start of epoch 30\n","Training loss over epoch:  30 0.25556746\n","Training acc over epoch:  30 0.9492828\n","Validation loss:  30 0.25544688\n","Validation acc:  30 0.94892395\n","epoch 30: loss = 0.25556743\n","Start of epoch 31\n","Training loss over epoch:  31 0.25538397\n","Training acc over epoch:  31 0.94928205\n","Validation loss:  31 0.25525984\n","Validation acc:  31 0.94892395\n","epoch 31: loss = 0.25538397\n","Start of epoch 32\n","Training loss over epoch:  32 0.2554111\n","Training acc over epoch:  32 0.94924676\n","Validation loss:  32 0.25551867\n","Validation acc:  32 0.94892395\n","epoch 32: loss = 0.2554111\n","Start of epoch 33\n","Training loss over epoch:  33 0.2551325\n","Training acc over epoch:  33 0.9492484\n","Validation loss:  33 0.25482586\n","Validation acc:  33 0.94892395\n","epoch 33: loss = 0.2551325\n","Start of epoch 34\n","Training loss over epoch:  34 0.25493416\n","Training acc over epoch:  34 0.9492564\n","Validation loss:  34 0.25460643\n","Validation acc:  34 0.94892395\n","epoch 34: loss = 0.25493413\n","Start of epoch 35\n","Training loss over epoch:  35 0.25455433\n","Training acc over epoch:  35 0.94927645\n","Validation loss:  35 0.25433177\n","Validation acc:  35 0.94892395\n","epoch 35: loss = 0.25455433\n","Start of epoch 36\n","Training loss over epoch:  36 0.25457966\n","Training acc over epoch:  36 0.94923717\n","Validation loss:  36 0.25420195\n","Validation acc:  36 0.94892395\n","epoch 36: loss = 0.25457966\n","Start of epoch 37\n","Training loss over epoch:  37 0.2542807\n","Training acc over epoch:  37 0.94926363\n","Validation loss:  37 0.2538546\n","Validation acc:  37 0.94892395\n","epoch 37: loss = 0.25428066\n","Start of epoch 38\n","Training loss over epoch:  38 0.25420278\n","Training acc over epoch:  38 0.9491915\n","Validation loss:  38 0.25338066\n","Validation acc:  38 0.94892395\n","epoch 38: loss = 0.2542028\n","Start of epoch 39\n","Training loss over epoch:  39 0.25370586\n","Training acc over epoch:  39 0.94923395\n","Validation loss:  39 0.25309154\n","Validation acc:  39 0.94892395\n","epoch 39: loss = 0.25370583\n","Start of epoch 40\n","Training loss over epoch:  40 0.25359014\n","Training acc over epoch:  40 0.94923955\n","Validation loss:  40 0.25344002\n","Validation acc:  40 0.94892395\n","epoch 40: loss = 0.2535902\n","Start of epoch 41\n","Training loss over epoch:  41 0.25318912\n","Training acc over epoch:  41 0.9493037\n","Validation loss:  41 0.25276223\n","Validation acc:  41 0.94892395\n","epoch 41: loss = 0.25318912\n","Start of epoch 42\n","Training loss over epoch:  42 0.25332448\n","Training acc over epoch:  42 0.9492252\n","Validation loss:  42 0.25256738\n","Validation acc:  42 0.94892395\n","epoch 42: loss = 0.25332445\n","Start of epoch 43\n","Training loss over epoch:  43 0.2527933\n","Training acc over epoch:  43 0.94927\n","Validation loss:  43 0.2524281\n","Validation acc:  43 0.94892395\n","epoch 43: loss = 0.25279334\n","Start of epoch 44\n","Training loss over epoch:  44 0.2526825\n","Training acc over epoch:  44 0.9492348\n","Validation loss:  44 0.25185478\n","Validation acc:  44 0.94892395\n","epoch 44: loss = 0.25268254\n","Start of epoch 45\n","Training loss over epoch:  45 0.2521431\n","Training acc over epoch:  45 0.94923717\n","Validation loss:  45 0.25140613\n","Validation acc:  45 0.94892395\n","epoch 45: loss = 0.25214314\n","Start of epoch 46\n","Training loss over epoch:  46 0.25168023\n","Training acc over epoch:  46 0.94929326\n","Validation loss:  46 0.25132507\n","Validation acc:  46 0.94892395\n","epoch 46: loss = 0.2516802\n","Start of epoch 47\n","Training loss over epoch:  47 0.2514927\n","Training acc over epoch:  47 0.9492195\n","Validation loss:  47 0.25061825\n","Validation acc:  47 0.94892395\n","epoch 47: loss = 0.25149268\n","Start of epoch 48\n","Training loss over epoch:  48 0.2513524\n","Training acc over epoch:  48 0.94920594\n","Validation loss:  48 0.25036943\n","Validation acc:  48 0.94892395\n","epoch 48: loss = 0.2513524\n","Start of epoch 49\n","Training loss over epoch:  49 0.251104\n","Training acc over epoch:  49 0.949238\n","Validation loss:  49 0.24978493\n","Validation acc:  49 0.94892395\n","epoch 49: loss = 0.25110397\n","Start of epoch 50\n","Training loss over epoch:  50 0.25085902\n","Training acc over epoch:  50 0.9492476\n","Validation loss:  50 0.24929182\n","Validation acc:  50 0.94892395\n","epoch 50: loss = 0.25085905\n","Start of epoch 51\n","Training loss over epoch:  51 0.25009644\n","Training acc over epoch:  51 0.9493173\n","Validation loss:  51 0.24904716\n","Validation acc:  51 0.94892395\n","epoch 51: loss = 0.2500964\n","Start of epoch 52\n","Training loss over epoch:  52 0.24982227\n","Training acc over epoch:  52 0.94928926\n","Validation loss:  52 0.24883449\n","Validation acc:  52 0.94892395\n","epoch 52: loss = 0.24982229\n","Start of epoch 53\n","Training loss over epoch:  53 0.24957445\n","Training acc over epoch:  53 0.9493117\n","Validation loss:  53 0.24799387\n","Validation acc:  53 0.94892395\n","epoch 53: loss = 0.24957447\n","Start of epoch 54\n","Training loss over epoch:  54 0.24929714\n","Training acc over epoch:  54 0.9492909\n","Validation loss:  54 0.24727312\n","Validation acc:  54 0.94892395\n","epoch 54: loss = 0.24929719\n","Start of epoch 55\n","Training loss over epoch:  55 0.24845749\n","Training acc over epoch:  55 0.9493878\n","Validation loss:  55 0.2471517\n","Validation acc:  55 0.94892395\n","epoch 55: loss = 0.24845748\n","Start of epoch 56\n","Training loss over epoch:  56 0.24854112\n","Training acc over epoch:  56 0.9493317\n","Validation loss:  56 0.24627031\n","Validation acc:  56 0.94892395\n","epoch 56: loss = 0.24854113\n","Start of epoch 57\n","Training loss over epoch:  57 0.24772175\n","Training acc over epoch:  57 0.94942546\n","Validation loss:  57 0.24559985\n","Validation acc:  57 0.94949144\n","epoch 57: loss = 0.24772182\n","Start of epoch 58\n","Training loss over epoch:  58 0.24761035\n","Training acc over epoch:  58 0.9493966\n","Validation loss:  58 0.24640526\n","Validation acc:  58 0.94892395\n","epoch 58: loss = 0.24761032\n","Start of epoch 59\n","Training loss over epoch:  59 0.2471478\n","Training acc over epoch:  59 0.9494583\n","Validation loss:  59 0.24445035\n","Validation acc:  59 0.94949144\n","epoch 59: loss = 0.24714781\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:10<00:00, 76.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9503: \n","Sensitivity: 0.07222773796628683\n","Specificity: 0.9348977841771137\n","F1-score macro: 0.07464877976796348\n","F1-score micro: 0.9502994011976048\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 507676.0\n","valid_X_fake diff: 56446.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.5265919\n","Training acc over epoch:  0 0.6872629\n","Validation loss:  0 0.29326478\n","Validation acc:  0 0.94892395\n","epoch 0: loss = 1.5265918\n","Start of epoch 1\n","Training loss over epoch:  1 0.2908153\n","Training acc over epoch:  1 0.94927806\n","Validation loss:  1 0.273048\n","Validation acc:  1 0.94892395\n","epoch 1: loss = 0.29081523\n","Start of epoch 2\n","Training loss over epoch:  2 0.26943335\n","Training acc over epoch:  2 0.9492684\n","Validation loss:  2 0.25719884\n","Validation acc:  2 0.94892395\n","epoch 2: loss = 0.26943338\n","Start of epoch 3\n","Training loss over epoch:  3 0.25678423\n","Training acc over epoch:  3 0.9492732\n","Validation loss:  3 0.24995953\n","Validation acc:  3 0.94892395\n","epoch 3: loss = 0.25678417\n","Start of epoch 4\n","Training loss over epoch:  4 0.2499604\n","Training acc over epoch:  4 0.94929487\n","Validation loss:  4 0.24404043\n","Validation acc:  4 0.94892395\n","epoch 4: loss = 0.24996038\n","Start of epoch 5\n","Training loss over epoch:  5 0.24587372\n","Training acc over epoch:  5 0.94930124\n","Validation loss:  5 0.24107566\n","Validation acc:  5 0.94892395\n","epoch 5: loss = 0.24587367\n","Start of epoch 6\n","Training loss over epoch:  6 0.24372174\n","Training acc over epoch:  6 0.94927\n","Validation loss:  6 0.23934163\n","Validation acc:  6 0.94892395\n","epoch 6: loss = 0.24372174\n","Start of epoch 7\n","Training loss over epoch:  7 0.24153987\n","Training acc over epoch:  7 0.94923556\n","Validation loss:  7 0.23872991\n","Validation acc:  7 0.94892395\n","epoch 7: loss = 0.24153985\n","Start of epoch 8\n","Training loss over epoch:  8 0.23888847\n","Training acc over epoch:  8 0.94927484\n","Validation loss:  8 0.23477127\n","Validation acc:  8 0.94892395\n","epoch 8: loss = 0.23888852\n","Start of epoch 9\n","Training loss over epoch:  9 0.23682709\n","Training acc over epoch:  9 0.9492604\n","Validation loss:  9 0.23261316\n","Validation acc:  9 0.94893134\n","epoch 9: loss = 0.23682712\n","Start of epoch 10\n","Training loss over epoch:  10 0.2314047\n","Training acc over epoch:  10 0.9493205\n","Validation loss:  10 0.22158302\n","Validation acc:  10 0.9489682\n","epoch 10: loss = 0.23140472\n","Start of epoch 11\n","Training loss over epoch:  11 0.2217863\n","Training acc over epoch:  11 0.9495144\n","Validation loss:  11 0.21199435\n","Validation acc:  11 0.9494841\n","epoch 11: loss = 0.22178629\n","Start of epoch 12\n","Training loss over epoch:  12 0.21219273\n","Training acc over epoch:  12 0.949846\n","Validation loss:  12 0.20124717\n","Validation acc:  12 0.95008105\n","epoch 12: loss = 0.21219276\n","Start of epoch 13\n","Training loss over epoch:  13 0.20357445\n","Training acc over epoch:  13 0.9502522\n","Validation loss:  13 0.19267498\n","Validation acc:  13 0.9505012\n","epoch 13: loss = 0.20357446\n","Start of epoch 14\n","Training loss over epoch:  14 0.19548559\n","Training acc over epoch:  14 0.95076734\n","Validation loss:  14 0.18227994\n","Validation acc:  14 0.9513488\n","epoch 14: loss = 0.19548559\n","Start of epoch 15\n","Training loss over epoch:  15 0.18644759\n","Training acc over epoch:  15 0.9514803\n","Validation loss:  15 0.17094064\n","Validation acc:  15 0.95274913\n","epoch 15: loss = 0.18644758\n","Start of epoch 16\n","Training loss over epoch:  16 0.1766694\n","Training acc over epoch:  16 0.952404\n","Validation loss:  16 0.16093688\n","Validation acc:  16 0.9546138\n","epoch 16: loss = 0.1766694\n","Start of epoch 17\n","Training loss over epoch:  17 0.16611816\n","Training acc over epoch:  17 0.9534831\n","Validation loss:  17 0.14813553\n","Validation acc:  17 0.95631635\n","epoch 17: loss = 0.16611817\n","Start of epoch 18\n","Training loss over epoch:  18 0.15634261\n","Training acc over epoch:  18 0.9549243\n","Validation loss:  18 0.13782442\n","Validation acc:  18 0.9585569\n","epoch 18: loss = 0.15634258\n","Start of epoch 19\n","Training loss over epoch:  19 0.14834085\n","Training acc over epoch:  19 0.9562061\n","Validation loss:  19 0.12880085\n","Validation acc:  19 0.9609154\n","epoch 19: loss = 0.14834085\n","Start of epoch 20\n","Training loss over epoch:  20 0.14111982\n","Training acc over epoch:  20 0.9582185\n","Validation loss:  20 0.12106944\n","Validation acc:  20 0.96384877\n","epoch 20: loss = 0.14111981\n","Start of epoch 21\n","Training loss over epoch:  21 0.13355474\n","Training acc over epoch:  21 0.9604192\n","Validation loss:  21 0.11585564\n","Validation acc:  21 0.96677476\n","epoch 21: loss = 0.13355471\n","Start of epoch 22\n","Training loss over epoch:  22 0.12761648\n","Training acc over epoch:  22 0.9622626\n","Validation loss:  22 0.10924333\n","Validation acc:  22 0.96856576\n","epoch 22: loss = 0.1276165\n","Start of epoch 23\n","Training loss over epoch:  23 0.12149177\n","Training acc over epoch:  23 0.96406907\n","Validation loss:  23 0.10348395\n","Validation acc:  23 0.97032726\n","epoch 23: loss = 0.12149177\n","Start of epoch 24\n","Training loss over epoch:  24 0.11738575\n","Training acc over epoch:  24 0.965298\n","Validation loss:  24 0.10033852\n","Validation acc:  24 0.97192657\n","epoch 24: loss = 0.11738572\n","Start of epoch 25\n","Training loss over epoch:  25 0.11205657\n","Training acc over epoch:  25 0.9669827\n","Validation loss:  25 0.095348276\n","Validation acc:  25 0.9737913\n","epoch 25: loss = 0.11205655\n","Start of epoch 26\n","Training loss over epoch:  26 0.108011596\n","Training acc over epoch:  26 0.9683142\n","Validation loss:  26 0.09068884\n","Validation acc:  26 0.9747568\n","epoch 26: loss = 0.10801159\n","Start of epoch 27\n","Training loss over epoch:  27 0.10326185\n","Training acc over epoch:  27 0.9699605\n","Validation loss:  27 0.08649821\n","Validation acc:  27 0.97607607\n","epoch 27: loss = 0.10326184\n","Start of epoch 28\n","Training loss over epoch:  28 0.09958519\n","Training acc over epoch:  28 0.97138727\n","Validation loss:  28 0.08336691\n","Validation acc:  28 0.97721106\n","epoch 28: loss = 0.09958518\n","Start of epoch 29\n","Training loss over epoch:  29 0.09564615\n","Training acc over epoch:  29 0.97251445\n","Validation loss:  29 0.07970446\n","Validation acc:  29 0.97811025\n","epoch 29: loss = 0.09564615\n","Start of epoch 30\n","Training loss over epoch:  30 0.092256404\n","Training acc over epoch:  30 0.97378665\n","Validation loss:  30 0.0778048\n","Validation acc:  30 0.9787515\n","epoch 30: loss = 0.09225642\n","Start of epoch 31\n","Training loss over epoch:  31 0.088593096\n","Training acc over epoch:  31 0.97494507\n","Validation loss:  31 0.07542415\n","Validation acc:  31 0.9802108\n","epoch 31: loss = 0.0885931\n","Start of epoch 32\n","Training loss over epoch:  32 0.086472005\n","Training acc over epoch:  32 0.97571653\n","Validation loss:  32 0.072762445\n","Validation acc:  32 0.9807046\n","epoch 32: loss = 0.08647199\n","Start of epoch 33\n","Training loss over epoch:  33 0.083786644\n","Training acc over epoch:  33 0.9766522\n","Validation loss:  33 0.07032693\n","Validation acc:  33 0.98164064\n","epoch 33: loss = 0.08378664\n","Start of epoch 34\n","Training loss over epoch:  34 0.081148855\n","Training acc over epoch:  34 0.9776448\n","Validation loss:  34 0.0697575\n","Validation acc:  34 0.98195016\n","epoch 34: loss = 0.081148855\n","Start of epoch 35\n","Training loss over epoch:  35 0.079287015\n","Training acc over epoch:  35 0.97813267\n","Validation loss:  35 0.066522494\n","Validation acc:  35 0.98268723\n","epoch 35: loss = 0.07928703\n","Start of epoch 36\n","Training loss over epoch:  36 0.0767357\n","Training acc over epoch:  36 0.97894984\n","Validation loss:  36 0.06563857\n","Validation acc:  36 0.9829157\n","epoch 36: loss = 0.076735705\n","Start of epoch 37\n","Training loss over epoch:  37 0.07499517\n","Training acc over epoch:  37 0.9794994\n","Validation loss:  37 0.06430031\n","Validation acc:  37 0.9836601\n","epoch 37: loss = 0.07499517\n","Start of epoch 38\n","Training loss over epoch:  38 0.07320688\n","Training acc over epoch:  38 0.98002493\n","Validation loss:  38 0.06339217\n","Validation acc:  38 0.98383695\n","epoch 38: loss = 0.073206894\n","Start of epoch 39\n","Training loss over epoch:  39 0.07190879\n","Training acc over epoch:  39 0.9804007\n","Validation loss:  39 0.061785586\n","Validation acc:  39 0.98413914\n","epoch 39: loss = 0.071908794\n","Start of epoch 40\n","Training loss over epoch:  40 0.070574895\n","Training acc over epoch:  40 0.9809046\n","Validation loss:  40 0.06014153\n","Validation acc:  40 0.984574\n","epoch 40: loss = 0.07057488\n","Start of epoch 41\n","Training loss over epoch:  41 0.068672106\n","Training acc over epoch:  41 0.98147494\n","Validation loss:  41 0.059185725\n","Validation acc:  41 0.9846698\n","epoch 41: loss = 0.06867209\n","Start of epoch 42\n","Training loss over epoch:  42 0.06773007\n","Training acc over epoch:  42 0.98170966\n","Validation loss:  42 0.058342054\n","Validation acc:  42 0.9854658\n","epoch 42: loss = 0.06773008\n","Start of epoch 43\n","Training loss over epoch:  43 0.06603688\n","Training acc over epoch:  43 0.9822192\n","Validation loss:  43 0.056850683\n","Validation acc:  43 0.98557633\n","epoch 43: loss = 0.06603688\n","Start of epoch 44\n","Training loss over epoch:  44 0.064924404\n","Training acc over epoch:  44 0.98259974\n","Validation loss:  44 0.05646921\n","Validation acc:  44 0.98572373\n","epoch 44: loss = 0.064924404\n","Start of epoch 45\n","Training loss over epoch:  45 0.06336402\n","Training acc over epoch:  45 0.9829739\n","Validation loss:  45 0.055519894\n","Validation acc:  45 0.98594487\n","epoch 45: loss = 0.06336401\n","Start of epoch 46\n","Training loss over epoch:  46 0.06280779\n","Training acc over epoch:  46 0.9830876\n","Validation loss:  46 0.054597165\n","Validation acc:  46 0.9862839\n","epoch 46: loss = 0.06280779\n","Start of epoch 47\n","Training loss over epoch:  47 0.06146667\n","Training acc over epoch:  47 0.9833952\n","Validation loss:  47 0.054242495\n","Validation acc:  47 0.9862765\n","epoch 47: loss = 0.061466668\n","Start of epoch 48\n","Training loss over epoch:  48 0.060417883\n","Training acc over epoch:  48 0.98376375\n","Validation loss:  48 0.053476285\n","Validation acc:  48 0.9867114\n","epoch 48: loss = 0.060417876\n","Start of epoch 49\n","Training loss over epoch:  49 0.059690047\n","Training acc over epoch:  49 0.9839905\n","Validation loss:  49 0.05544633\n","Validation acc:  49 0.98577535\n","epoch 49: loss = 0.059690036\n","Start of epoch 50\n","Training loss over epoch:  50 0.059352107\n","Training acc over epoch:  50 0.98421395\n","Validation loss:  50 0.05263832\n","Validation acc:  50 0.9867409\n","epoch 50: loss = 0.059352122\n","Start of epoch 51\n","Training loss over epoch:  51 0.057383742\n","Training acc over epoch:  51 0.98448956\n","Validation loss:  51 0.05290582\n","Validation acc:  51 0.9871315\n","epoch 51: loss = 0.05738374\n","Start of epoch 52\n","Training loss over epoch:  52 0.056840993\n","Training acc over epoch:  52 0.9847259\n","Validation loss:  52 0.051556706\n","Validation acc:  52 0.9870725\n","epoch 52: loss = 0.05684099\n","Start of epoch 53\n","Training loss over epoch:  53 0.05579805\n","Training acc over epoch:  53 0.9849013\n","Validation loss:  53 0.050576553\n","Validation acc:  53 0.9872789\n","epoch 53: loss = 0.055798043\n","Start of epoch 54\n","Training loss over epoch:  54 0.055105697\n","Training acc over epoch:  54 0.98507756\n","Validation loss:  54 0.050925437\n","Validation acc:  54 0.9874484\n","epoch 54: loss = 0.05510569\n","Start of epoch 55\n","Training loss over epoch:  55 0.054218028\n","Training acc over epoch:  55 0.9852146\n","Validation loss:  55 0.05003123\n","Validation acc:  55 0.98738205\n","epoch 55: loss = 0.054218028\n","Start of epoch 56\n","Training loss over epoch:  56 0.053184096\n","Training acc over epoch:  56 0.9855751\n","Validation loss:  56 0.04974587\n","Validation acc:  56 0.9875737\n","epoch 56: loss = 0.053184085\n","Start of epoch 57\n","Training loss over epoch:  57 0.052174147\n","Training acc over epoch:  57 0.98579216\n","Validation loss:  57 0.04932034\n","Validation acc:  57 0.98752946\n","epoch 57: loss = 0.052174155\n","Start of epoch 58\n","Training loss over epoch:  58 0.051777758\n","Training acc over epoch:  58 0.9858242\n","Validation loss:  58 0.049162567\n","Validation acc:  58 0.9876032\n","epoch 58: loss = 0.051777758\n","Start of epoch 59\n","Training loss over epoch:  59 0.050756905\n","Training acc over epoch:  59 0.9860886\n","Validation loss:  59 0.048214298\n","Validation acc:  59 0.9879127\n","epoch 59: loss = 0.050756905\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 835/835 [00:11<00:00, 75.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9531: \n","Sensitivity: 0.1264710204301507\n","Specificity: 0.9558640240545643\n","F1-score macro: 0.15392476890481385\n","F1-score micro: 0.9531091662828189\n","Training using fold 3\n","*******************************************\n","*******************************************\n","Missing rate 0.05\n","=====================================================\n","train_X_fake diff: 126252.0\n","valid_X_fake diff: 14028.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.2901678\n","Training acc over epoch:  0 0.75845337\n","Validation loss:  0 0.29297647\n","Validation acc:  0 0.9494767\n","epoch 0: loss = 1.2901678\n","Start of epoch 1\n","Training loss over epoch:  1 0.28446037\n","Training acc over epoch:  1 0.9492436\n","Validation loss:  1 0.26839107\n","Validation acc:  1 0.9494767\n","epoch 1: loss = 0.2844604\n","Start of epoch 2\n","Training loss over epoch:  2 0.2654121\n","Training acc over epoch:  2 0.94920594\n","Validation loss:  2 0.25190634\n","Validation acc:  2 0.9494767\n","epoch 2: loss = 0.2654121\n","Start of epoch 3\n","Training loss over epoch:  3 0.25273263\n","Training acc over epoch:  3 0.9491995\n","Validation loss:  3 0.24331504\n","Validation acc:  3 0.9494767\n","epoch 3: loss = 0.25273263\n","Start of epoch 4\n","Training loss over epoch:  4 0.24551977\n","Training acc over epoch:  4 0.9491466\n","Validation loss:  4 0.23672746\n","Validation acc:  4 0.9494767\n","epoch 4: loss = 0.24551976\n","Start of epoch 5\n","Training loss over epoch:  5 0.24001837\n","Training acc over epoch:  5 0.949218\n","Validation loss:  5 0.23325527\n","Validation acc:  5 0.9494767\n","epoch 5: loss = 0.24001835\n","Start of epoch 6\n","Training loss over epoch:  6 0.23510692\n","Training acc over epoch:  6 0.949238\n","Validation loss:  6 0.22716711\n","Validation acc:  6 0.9494767\n","epoch 6: loss = 0.23510695\n","Start of epoch 7\n","Training loss over epoch:  7 0.23022924\n","Training acc over epoch:  7 0.9491771\n","Validation loss:  7 0.2218521\n","Validation acc:  7 0.9494767\n","epoch 7: loss = 0.23022921\n","Start of epoch 8\n","Training loss over epoch:  8 0.22402398\n","Training acc over epoch:  8 0.9491875\n","Validation loss:  8 0.21226403\n","Validation acc:  8 0.9496241\n","epoch 8: loss = 0.22402398\n","Start of epoch 9\n","Training loss over epoch:  9 0.21630073\n","Training acc over epoch:  9 0.9493053\n","Validation loss:  9 0.20439821\n","Validation acc:  9 0.9499779\n","epoch 9: loss = 0.21630074\n","Start of epoch 10\n","Training loss over epoch:  10 0.20720237\n","Training acc over epoch:  10 0.94963455\n","Validation loss:  10 0.18988764\n","Validation acc:  10 0.9512382\n","epoch 10: loss = 0.20720235\n","Start of epoch 11\n","Training loss over epoch:  11 0.19180115\n","Training acc over epoch:  11 0.9503692\n","Validation loss:  11 0.1731769\n","Validation acc:  11 0.9519089\n","epoch 11: loss = 0.19180113\n","Start of epoch 12\n","Training loss over epoch:  12 0.17853609\n","Training acc over epoch:  12 0.95122075\n","Validation loss:  12 0.1591207\n","Validation acc:  12 0.95271224\n","epoch 12: loss = 0.17853606\n","Start of epoch 13\n","Training loss over epoch:  13 0.16803308\n","Training acc over epoch:  13 0.9517567\n","Validation loss:  13 0.14935896\n","Validation acc:  13 0.9544443\n","epoch 13: loss = 0.16803308\n","Start of epoch 14\n","Training loss over epoch:  14 0.15965003\n","Training acc over epoch:  14 0.95288944\n","Validation loss:  14 0.13311714\n","Validation acc:  14 0.95648587\n","epoch 14: loss = 0.15965004\n","Start of epoch 15\n","Training loss over epoch:  15 0.14494029\n","Training acc over epoch:  15 0.9547312\n","Validation loss:  15 0.12086179\n","Validation acc:  15 0.9596993\n","epoch 15: loss = 0.1449403\n","Start of epoch 16\n","Training loss over epoch:  16 0.13559377\n","Training acc over epoch:  16 0.95644486\n","Validation loss:  16 0.11090089\n","Validation acc:  16 0.9637603\n","epoch 16: loss = 0.13559376\n","Start of epoch 17\n","Training loss over epoch:  17 0.12524511\n","Training acc over epoch:  17 0.95911497\n","Validation loss:  17 0.09923681\n","Validation acc:  17 0.9677255\n","epoch 17: loss = 0.12524511\n","Start of epoch 18\n","Training loss over epoch:  18 0.11603173\n","Training acc over epoch:  18 0.9618628\n","Validation loss:  18 0.08989209\n","Validation acc:  18 0.970821\n","epoch 18: loss = 0.11603171\n","Start of epoch 19\n","Training loss over epoch:  19 0.10815054\n","Training acc over epoch:  19 0.96443516\n","Validation loss:  19 0.08328867\n","Validation acc:  19 0.9738945\n","epoch 19: loss = 0.108150564\n","Start of epoch 20\n","Training loss over epoch:  20 0.101071425\n","Training acc over epoch:  20 0.9667873\n","Validation loss:  20 0.07535189\n","Validation acc:  20 0.9768352\n","epoch 20: loss = 0.1010714\n","Start of epoch 21\n","Training loss over epoch:  21 0.09421607\n","Training acc over epoch:  21 0.96886533\n","Validation loss:  21 0.07053154\n","Validation acc:  21 0.97975385\n","epoch 21: loss = 0.09421605\n","Start of epoch 22\n","Training loss over epoch:  22 0.088343225\n","Training acc over epoch:  22 0.9711053\n","Validation loss:  22 0.062714465\n","Validation acc:  22 0.98168486\n","epoch 22: loss = 0.088343225\n","Start of epoch 23\n","Training loss over epoch:  23 0.08243794\n","Training acc over epoch:  23 0.97316176\n","Validation loss:  23 0.05675244\n","Validation acc:  23 0.9833358\n","epoch 23: loss = 0.08243795\n","Start of epoch 24\n","Training loss over epoch:  24 0.07687862\n","Training acc over epoch:  24 0.9752022\n","Validation loss:  24 0.052862793\n","Validation acc:  24 0.98548794\n","epoch 24: loss = 0.07687862\n","Start of epoch 25\n","Training loss over epoch:  25 0.07316192\n","Training acc over epoch:  25 0.9767788\n","Validation loss:  25 0.049382783\n","Validation acc:  25 0.9861291\n","epoch 25: loss = 0.07316192\n","Start of epoch 26\n","Training loss over epoch:  26 0.06839296\n","Training acc over epoch:  26 0.97833776\n","Validation loss:  26 0.04648994\n","Validation acc:  26 0.98723465\n","epoch 26: loss = 0.06839296\n","Start of epoch 27\n","Training loss over epoch:  27 0.06544453\n","Training acc over epoch:  27 0.97930235\n","Validation loss:  27 0.04579363\n","Validation acc:  27 0.988097\n","epoch 27: loss = 0.06544453\n","Start of epoch 28\n","Training loss over epoch:  28 0.062360823\n","Training acc over epoch:  28 0.9805024\n","Validation loss:  28 0.041731205\n","Validation acc:  28 0.9892615\n","epoch 28: loss = 0.062360834\n","Start of epoch 29\n","Training loss over epoch:  29 0.05950119\n","Training acc over epoch:  29 0.98139966\n","Validation loss:  29 0.040121883\n","Validation acc:  29 0.9895858\n","epoch 29: loss = 0.059501186\n","Start of epoch 30\n","Training loss over epoch:  30 0.057515595\n","Training acc over epoch:  30 0.98212147\n","Validation loss:  30 0.03969009\n","Validation acc:  30 0.9899322\n","epoch 30: loss = 0.0575156\n","Start of epoch 31\n","Training loss over epoch:  31 0.05537918\n","Training acc over epoch:  31 0.98290014\n","Validation loss:  31 0.03766844\n","Validation acc:  31 0.99034494\n","epoch 31: loss = 0.055379175\n","Start of epoch 32\n","Training loss over epoch:  32 0.053230617\n","Training acc over epoch:  32 0.9836644\n","Validation loss:  32 0.03570463\n","Validation acc:  32 0.9910009\n","epoch 32: loss = 0.05323062\n","Start of epoch 33\n","Training loss over epoch:  33 0.050841488\n","Training acc over epoch:  33 0.9843542\n","Validation loss:  33 0.03491656\n","Validation acc:  33 0.9912957\n","epoch 33: loss = 0.050841488\n","Start of epoch 34\n","Training loss over epoch:  34 0.04917959\n","Training acc over epoch:  34 0.98499584\n","Validation loss:  34 0.033845\n","Validation acc:  34 0.9915979\n","epoch 34: loss = 0.04917958\n","Start of epoch 35\n","Training loss over epoch:  35 0.047229387\n","Training acc over epoch:  35 0.9855262\n","Validation loss:  35 0.033254385\n","Validation acc:  35 0.9920696\n","epoch 35: loss = 0.047229387\n","Start of epoch 36\n","Training loss over epoch:  36 0.04610051\n","Training acc over epoch:  36 0.9860045\n","Validation loss:  36 0.03137666\n","Validation acc:  36 0.99242336\n","epoch 36: loss = 0.0461005\n","Start of epoch 37\n","Training loss over epoch:  37 0.044405155\n","Training acc over epoch:  37 0.98655486\n","Validation loss:  37 0.029805114\n","Validation acc:  37 0.9925929\n","epoch 37: loss = 0.044405155\n","Start of epoch 38\n","Training loss over epoch:  38 0.042559367\n","Training acc over epoch:  38 0.9870379\n","Validation loss:  38 0.029982248\n","Validation acc:  38 0.99283606\n","epoch 38: loss = 0.042559363\n","Start of epoch 39\n","Training loss over epoch:  39 0.04074989\n","Training acc over epoch:  39 0.9877597\n","Validation loss:  39 0.028719492\n","Validation acc:  39 0.99299824\n","epoch 39: loss = 0.040749893\n","Start of epoch 40\n","Training loss over epoch:  40 0.03950984\n","Training acc over epoch:  40 0.98812103\n","Validation loss:  40 0.027917577\n","Validation acc:  40 0.9934036\n","epoch 40: loss = 0.03950984\n","Start of epoch 41\n","Training loss over epoch:  41 0.03803897\n","Training acc over epoch:  41 0.98864496\n","Validation loss:  41 0.026707623\n","Validation acc:  41 0.9937942\n","epoch 41: loss = 0.038038973\n","Start of epoch 42\n","Training loss over epoch:  42 0.037410315\n","Training acc over epoch:  42 0.9889053\n","Validation loss:  42 0.02591218\n","Validation acc:  42 0.99388266\n","epoch 42: loss = 0.03741031\n","Start of epoch 43\n","Training loss over epoch:  43 0.035631258\n","Training acc over epoch:  43 0.9893852\n","Validation loss:  43 0.025695095\n","Validation acc:  43 0.99403745\n","epoch 43: loss = 0.03563125\n","Start of epoch 44\n","Training loss over epoch:  44 0.034218736\n","Training acc over epoch:  44 0.9898258\n","Validation loss:  44 0.025394648\n","Validation acc:  44 0.99433225\n","epoch 44: loss = 0.034218736\n","Start of epoch 45\n","Training loss over epoch:  45 0.033074677\n","Training acc over epoch:  45 0.9902512\n","Validation loss:  45 0.024244914\n","Validation acc:  45 0.99461234\n","epoch 45: loss = 0.03307468\n","Start of epoch 46\n","Training loss over epoch:  46 0.031845864\n","Training acc over epoch:  46 0.99057966\n","Validation loss:  46 0.023576599\n","Validation acc:  46 0.9945239\n","epoch 46: loss = 0.031845864\n","Start of epoch 47\n","Training loss over epoch:  47 0.031082448\n","Training acc over epoch:  47 0.99072146\n","Validation loss:  47 0.022858992\n","Validation acc:  47 0.99462706\n","epoch 47: loss = 0.031082446\n","Start of epoch 48\n","Training loss over epoch:  48 0.030182602\n","Training acc over epoch:  48 0.990993\n","Validation loss:  48 0.022468753\n","Validation acc:  48 0.9947008\n","epoch 48: loss = 0.030182607\n","Start of epoch 49\n","Training loss over epoch:  49 0.029155953\n","Training acc over epoch:  49 0.9913407\n","Validation loss:  49 0.021576623\n","Validation acc:  49 0.9948334\n","epoch 49: loss = 0.029155951\n","Start of epoch 50\n","Training loss over epoch:  50 0.028035052\n","Training acc over epoch:  50 0.991702\n","Validation loss:  50 0.021207042\n","Validation acc:  50 0.99506193\n","epoch 50: loss = 0.02803505\n","Start of epoch 51\n","Training loss over epoch:  51 0.02706414\n","Training acc over epoch:  51 0.9919576\n","Validation loss:  51 0.020998705\n","Validation acc:  51 0.9952683\n","epoch 51: loss = 0.027064139\n","Start of epoch 52\n","Training loss over epoch:  52 0.026281511\n","Training acc over epoch:  52 0.99217147\n","Validation loss:  52 0.021213038\n","Validation acc:  52 0.99525356\n","epoch 52: loss = 0.02628151\n","Start of epoch 53\n","Training loss over epoch:  53 0.025790831\n","Training acc over epoch:  53 0.9923758\n","Validation loss:  53 0.020168427\n","Validation acc:  53 0.99522406\n","epoch 53: loss = 0.025790835\n","Start of epoch 54\n","Training loss over epoch:  54 0.024728986\n","Training acc over epoch:  54 0.9926329\n","Validation loss:  54 0.019795172\n","Validation acc:  54 0.9955705\n","epoch 54: loss = 0.024728987\n","Start of epoch 55\n","Training loss over epoch:  55 0.024340041\n","Training acc over epoch:  55 0.9928204\n","Validation loss:  55 0.019801704\n","Validation acc:  55 0.99549675\n","epoch 55: loss = 0.024340041\n","Start of epoch 56\n","Training loss over epoch:  56 0.02330855\n","Training acc over epoch:  56 0.9930351\n","Validation loss:  56 0.019187529\n","Validation acc:  56 0.9958579\n","epoch 56: loss = 0.023308547\n","Start of epoch 57\n","Training loss over epoch:  57 0.022619234\n","Training acc over epoch:  57 0.9931633\n","Validation loss:  57 0.01880923\n","Validation acc:  57 0.9958358\n","epoch 57: loss = 0.022619236\n","Start of epoch 58\n","Training loss over epoch:  58 0.021842977\n","Training acc over epoch:  58 0.9935294\n","Validation loss:  58 0.018477459\n","Validation acc:  58 0.9958284\n","epoch 58: loss = 0.021842971\n","Start of epoch 59\n","Training loss over epoch:  59 0.021471262\n","Training acc over epoch:  59 0.99358386\n","Validation loss:  59 0.018441416\n","Validation acc:  59 0.99581367\n","epoch 59: loss = 0.02147126\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:10<00:00, 76.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.05 missing genotypes is 0.9511: \n","Sensitivity: 0.15709974932064313\n","Specificity: 0.938108121040098\n","F1-score macro: 0.18848827601971244\n","F1-score micro: 0.9510962658444673\n","Missing rate 0.1\n","=====================================================\n","train_X_fake diff: 252504.0\n","valid_X_fake diff: 28056.0\n","Start of epoch 0\n","Training loss over epoch:  0 1.4849354\n","Training acc over epoch:  0 0.7199613\n","Validation loss:  0 0.29120836\n","Validation acc:  0 0.9494767\n","epoch 0: loss = 1.484935\n","Start of epoch 1\n","Training loss over epoch:  1 0.28113443\n","Training acc over epoch:  1 0.94925\n","Validation loss:  1 0.26292506\n","Validation acc:  1 0.9494767\n","epoch 1: loss = 0.28113437\n","Start of epoch 2\n","Training loss over epoch:  2 0.26064447\n","Training acc over epoch:  2 0.9491835\n","Validation loss:  2 0.24882446\n","Validation acc:  2 0.9494767\n","epoch 2: loss = 0.26064447\n","Start of epoch 3\n","Training loss over epoch:  3 0.25256747\n","Training acc over epoch:  3 0.94923556\n","Validation loss:  3 0.24428228\n","Validation acc:  3 0.9494767\n","epoch 3: loss = 0.2525675\n","Start of epoch 4\n","Training loss over epoch:  4 0.24787374\n","Training acc over epoch:  4 0.9492612\n","Validation loss:  4 0.23776457\n","Validation acc:  4 0.9494767\n","epoch 4: loss = 0.24787374\n","Start of epoch 5\n","Training loss over epoch:  5 0.24099672\n","Training acc over epoch:  5 0.94919074\n","Validation loss:  5 0.22974229\n","Validation acc:  5 0.9494767\n","epoch 5: loss = 0.24099675\n","Start of epoch 6\n","Training loss over epoch:  6 0.23592982\n","Training acc over epoch:  6 0.94916666\n","Validation loss:  6 0.22401197\n","Validation acc:  6 0.9494767\n","epoch 6: loss = 0.2359298\n","Start of epoch 7\n","Training loss over epoch:  7 0.23108459\n","Training acc over epoch:  7 0.9491827\n","Validation loss:  7 0.21918127\n","Validation acc:  7 0.9494767\n","epoch 7: loss = 0.23108457\n","Start of epoch 8\n","Training loss over epoch:  8 0.22584772\n","Training acc over epoch:  8 0.9491707\n","Validation loss:  8 0.2107927\n","Validation acc:  8 0.9494767\n","epoch 8: loss = 0.22584772\n","Start of epoch 9\n","Training loss over epoch:  9 0.2177486\n","Training acc over epoch:  9 0.94921875\n","Validation loss:  9 0.20069355\n","Validation acc:  9 0.9494767\n","epoch 9: loss = 0.21774864\n","Start of epoch 10\n","Training loss over epoch:  10 0.20903817\n","Training acc over epoch:  10 0.9494014\n","Validation loss:  10 0.19145197\n","Validation acc:  10 0.9500368\n","epoch 10: loss = 0.2090382\n","Start of epoch 11\n","Training loss over epoch:  11 0.20121284\n","Training acc over epoch:  11 0.9498212\n","Validation loss:  11 0.18195726\n","Validation acc:  11 0.95117927\n","epoch 11: loss = 0.20121278\n","Start of epoch 12\n","Training loss over epoch:  12 0.1887917\n","Training acc over epoch:  12 0.9506936\n","Validation loss:  12 0.1690261\n","Validation acc:  12 0.9523364\n","epoch 12: loss = 0.18879174\n","Start of epoch 13\n","Training loss over epoch:  13 0.17473845\n","Training acc over epoch:  13 0.95151395\n","Validation loss:  13 0.15221037\n","Validation acc:  13 0.9536925\n","epoch 13: loss = 0.17473845\n","Start of epoch 14\n","Training loss over epoch:  14 0.15977146\n","Training acc over epoch:  14 0.9532275\n","Validation loss:  14 0.13553005\n","Validation acc:  14 0.95717126\n","epoch 14: loss = 0.15977146\n","Start of epoch 15\n","Training loss over epoch:  15 0.14252314\n","Training acc over epoch:  15 0.9565914\n","Validation loss:  15 0.1182409\n","Validation acc:  15 0.9659935\n","epoch 15: loss = 0.14252311\n","Start of epoch 16\n","Training loss over epoch:  16 0.12698986\n","Training acc over epoch:  16 0.96076685\n","Validation loss:  16 0.100325115\n","Validation acc:  16 0.9687942\n","epoch 16: loss = 0.12698986\n","Start of epoch 17\n","Training loss over epoch:  17 0.11548546\n","Training acc over epoch:  17 0.9637118\n","Validation loss:  17 0.09051299\n","Validation acc:  17 0.97203714\n","epoch 17: loss = 0.11548546\n","Start of epoch 18\n","Training loss over epoch:  18 0.10640583\n","Training acc over epoch:  18 0.9664628\n","Validation loss:  18 0.08497261\n","Validation acc:  18 0.9745799\n","epoch 18: loss = 0.10640584\n","Start of epoch 19\n","Training loss over epoch:  19 0.09830619\n","Training acc over epoch:  19 0.96907365\n","Validation loss:  19 0.077305004\n","Validation acc:  19 0.9769605\n","epoch 19: loss = 0.09830619\n","Start of epoch 20\n","Training loss over epoch:  20 0.09143882\n","Training acc over epoch:  20 0.9715595\n","Validation loss:  20 0.0721897\n","Validation acc:  20 0.9786262\n","epoch 20: loss = 0.09143882\n","Start of epoch 21\n","Training loss over epoch:  21 0.085785866\n","Training acc over epoch:  21 0.97347176\n","Validation loss:  21 0.06773269\n","Validation acc:  21 0.9806899\n","epoch 21: loss = 0.085785866\n","Start of epoch 22\n","Training loss over epoch:  22 0.08008518\n","Training acc over epoch:  22 0.975622\n","Validation loss:  22 0.060822446\n","Validation acc:  22 0.98294514\n","epoch 22: loss = 0.080085166\n","Start of epoch 23\n","Training loss over epoch:  23 0.074805506\n","Training acc over epoch:  23 0.9774918\n","Validation loss:  23 0.056916054\n","Validation acc:  23 0.9846551\n","epoch 23: loss = 0.07480549\n","Start of epoch 24\n","Training loss over epoch:  24 0.069863655\n","Training acc over epoch:  24 0.97938484\n","Validation loss:  24 0.05364802\n","Validation acc:  24 0.9858638\n","epoch 24: loss = 0.06986364\n","Start of epoch 25\n","Training loss over epoch:  25 0.06634782\n","Training acc over epoch:  25 0.980665\n","Validation loss:  25 0.04978874\n","Validation acc:  25 0.9872199\n","epoch 25: loss = 0.066347815\n","Start of epoch 26\n","Training loss over epoch:  26 0.06222655\n","Training acc over epoch:  26 0.98209184\n","Validation loss:  26 0.04685429\n","Validation acc:  26 0.9880675\n","epoch 26: loss = 0.062226556\n","Start of epoch 27\n","Training loss over epoch:  27 0.05952018\n","Training acc over epoch:  27 0.982981\n","Validation loss:  27 0.044468913\n","Validation acc:  27 0.9886645\n","epoch 27: loss = 0.059520178\n","Start of epoch 28\n","Training loss over epoch:  28 0.05736307\n","Training acc over epoch:  28 0.98382944\n","Validation loss:  28 0.04276177\n","Validation acc:  28 0.98957103\n","epoch 28: loss = 0.05736307\n","Start of epoch 29\n","Training loss over epoch:  29 0.054733958\n","Training acc over epoch:  29 0.98462814\n","Validation loss:  29 0.039877817\n","Validation acc:  29 0.9902712\n","epoch 29: loss = 0.05473395\n","Start of epoch 30\n","Training loss over epoch:  30 0.052482862\n","Training acc over epoch:  30 0.9852506\n","Validation loss:  30 0.03888998\n","Validation acc:  30 0.9906987\n","epoch 30: loss = 0.05248286\n","Start of epoch 31\n","Training loss over epoch:  31 0.050844066\n","Training acc over epoch:  31 0.9858811\n","Validation loss:  31 0.038819756\n","Validation acc:  31 0.990684\n","epoch 31: loss = 0.05084407\n","Start of epoch 32\n","Training loss over epoch:  32 0.049209036\n","Training acc over epoch:  32 0.9863489\n","Validation loss:  32 0.038818207\n","Validation acc:  32 0.9908019\n","epoch 32: loss = 0.049209047\n","Start of epoch 33\n","Training loss over epoch:  33 0.047771923\n","Training acc over epoch:  33 0.9868136\n","Validation loss:  33 0.03707965\n","Validation acc:  33 0.99090505\n","epoch 33: loss = 0.04777191\n","Start of epoch 34\n","Training loss over epoch:  34 0.0458951\n","Training acc over epoch:  34 0.9872951\n","Validation loss:  34 0.034934003\n","Validation acc:  34 0.99155366\n","epoch 34: loss = 0.045895096\n","Start of epoch 35\n","Training loss over epoch:  35 0.045066208\n","Training acc over epoch:  35 0.9876588\n","Validation loss:  35 0.034400165\n","Validation acc:  35 0.9916421\n","epoch 35: loss = 0.045066215\n","Start of epoch 36\n","Training loss over epoch:  36 0.043422613\n","Training acc over epoch:  36 0.98797923\n","Validation loss:  36 0.033179898\n","Validation acc:  36 0.99210644\n","epoch 36: loss = 0.043422613\n","Start of epoch 37\n","Training loss over epoch:  37 0.042360116\n","Training acc over epoch:  37 0.98819876\n","Validation loss:  37 0.033110637\n","Validation acc:  37 0.99198115\n","epoch 37: loss = 0.04236012\n","Start of epoch 38\n","Training loss over epoch:  38 0.041204125\n","Training acc over epoch:  38 0.98866177\n","Validation loss:  38 0.03213924\n","Validation acc:  38 0.992357\n","epoch 38: loss = 0.04120413\n","Start of epoch 39\n","Training loss over epoch:  39 0.040199235\n","Training acc over epoch:  39 0.9888436\n","Validation loss:  39 0.03158606\n","Validation acc:  39 0.9924676\n","epoch 39: loss = 0.04019923\n","Start of epoch 40\n","Training loss over epoch:  40 0.039688796\n","Training acc over epoch:  40 0.9890359\n","Validation loss:  40 0.03099299\n","Validation acc:  40 0.9926592\n","epoch 40: loss = 0.039688796\n","Start of epoch 41\n","Training loss over epoch:  41 0.038696803\n","Training acc over epoch:  41 0.98943645\n","Validation loss:  41 0.0304642\n","Validation acc:  41 0.99271077\n","epoch 41: loss = 0.038696796\n","Start of epoch 42\n","Training loss over epoch:  42 0.03767511\n","Training acc over epoch:  42 0.9895398\n","Validation loss:  42 0.031390563\n","Validation acc:  42 0.99252653\n","epoch 42: loss = 0.03767511\n","Start of epoch 43\n","Training loss over epoch:  43 0.03710892\n","Training acc over epoch:  43 0.9897401\n","Validation loss:  43 0.029775577\n","Validation acc:  43 0.9929614\n","epoch 43: loss = 0.03710892\n","Start of epoch 44\n","Training loss over epoch:  44 0.03620843\n","Training acc over epoch:  44 0.9899788\n","Validation loss:  44 0.029648729\n","Validation acc:  44 0.9931014\n","epoch 44: loss = 0.03620844\n","Start of epoch 45\n","Training loss over epoch:  45 0.035135772\n","Training acc over epoch:  45 0.9902552\n","Validation loss:  45 0.029361224\n","Validation acc:  45 0.9927624\n","epoch 45: loss = 0.035135776\n","Start of epoch 46\n","Training loss over epoch:  46 0.034689315\n","Training acc over epoch:  46 0.9902616\n","Validation loss:  46 0.02876174\n","Validation acc:  46 0.992954\n","epoch 46: loss = 0.03468931\n","Start of epoch 47\n","Training loss over epoch:  47 0.03429017\n","Training acc over epoch:  47 0.9904106\n","Validation loss:  47 0.028744763\n","Validation acc:  47 0.9929319\n","epoch 47: loss = 0.034290172\n","Start of epoch 48\n","Training loss over epoch:  48 0.033260524\n","Training acc over epoch:  48 0.99069184\n","Validation loss:  48 0.028799785\n","Validation acc:  48 0.9927845\n","epoch 48: loss = 0.033260528\n","Start of epoch 49\n","Training loss over epoch:  49 0.032581955\n","Training acc over epoch:  49 0.99072146\n","Validation loss:  49 0.028723354\n","Validation acc:  49 0.9930498\n","epoch 49: loss = 0.032581955\n","Start of epoch 50\n","Training loss over epoch:  50 0.032115947\n","Training acc over epoch:  50 0.99097383\n","Validation loss:  50 0.028889079\n","Validation acc:  50 0.9929172\n","epoch 50: loss = 0.032115947\n","Start of epoch 51\n","Training loss over epoch:  51 0.03121547\n","Training acc over epoch:  51 0.9911853\n","Validation loss:  51 0.028148033\n","Validation acc:  51 0.9931751\n","epoch 51: loss = 0.031215474\n","Start of epoch 52\n","Training loss over epoch:  52 0.030593688\n","Training acc over epoch:  52 0.9912454\n","Validation loss:  52 0.028440192\n","Validation acc:  52 0.9930351\n","epoch 52: loss = 0.030593688\n","Start of epoch 53\n","Training loss over epoch:  53 0.029867578\n","Training acc over epoch:  53 0.99138796\n","Validation loss:  53 0.027991852\n","Validation acc:  53 0.9935215\n","epoch 53: loss = 0.029867575\n","Start of epoch 54\n","Training loss over epoch:  54 0.029355763\n","Training acc over epoch:  54 0.99158424\n","Validation loss:  54 0.027531555\n","Validation acc:  54 0.9931899\n","epoch 54: loss = 0.029355763\n","Start of epoch 55\n","Training loss over epoch:  55 0.02911723\n","Training acc over epoch:  55 0.9915474\n","Validation loss:  55 0.027344486\n","Validation acc:  55 0.99344784\n","epoch 55: loss = 0.029117232\n","Start of epoch 56\n","Training loss over epoch:  56 0.028466979\n","Training acc over epoch:  56 0.9917357\n","Validation loss:  56 0.0273478\n","Validation acc:  56 0.99330044\n","epoch 56: loss = 0.028466979\n","Start of epoch 57\n","Training loss over epoch:  57 0.028157288\n","Training acc over epoch:  57 0.9917597\n","Validation loss:  57 0.027271327\n","Validation acc:  57 0.9933815\n","epoch 57: loss = 0.028157288\n","Start of epoch 58\n","Training loss over epoch:  58 0.02757475\n","Training acc over epoch:  58 0.9919504\n","Validation loss:  58 0.027560929\n","Validation acc:  58 0.993352\n","epoch 58: loss = 0.027574752\n","Start of epoch 59\n","Training loss over epoch:  59 0.026940815\n","Training acc over epoch:  59 0.99207133\n","Validation loss:  59 0.02803535\n","Validation acc:  59 0.99346995\n","epoch 59: loss = 0.026940815\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:10<00:00, 76.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.1 missing genotypes is 0.9526: \n","Sensitivity: 0.14721787088096427\n","Specificity: 0.9440126864391516\n","F1-score macro: 0.1784695841415567\n","F1-score micro: 0.9525665182140002\n","Missing rate 0.2\n","=====================================================\n","train_X_fake diff: 508014.0\n","valid_X_fake diff: 56446.0\n","Start of epoch 0\n","Training loss over epoch:  0 0.82743955\n","Training acc over epoch:  0 0.89998925\n","Validation loss:  0 0.29126823\n","Validation acc:  0 0.9494767\n","epoch 0: loss = 0.8274395\n","Start of epoch 1\n","Training loss over epoch:  1 0.2867021\n","Training acc over epoch:  1 0.9491915\n","Validation loss:  1 0.27279198\n","Validation acc:  1 0.9494767\n","epoch 1: loss = 0.2867021\n","Start of epoch 2\n","Training loss over epoch:  2 0.2661549\n","Training acc over epoch:  2 0.9491843\n","Validation loss:  2 0.2506277\n","Validation acc:  2 0.9494767\n","epoch 2: loss = 0.26615483\n","Start of epoch 3\n","Training loss over epoch:  3 0.25050694\n","Training acc over epoch:  3 0.9492292\n","Validation loss:  3 0.24258618\n","Validation acc:  3 0.9494767\n","epoch 3: loss = 0.25050694\n","Start of epoch 4\n","Training loss over epoch:  4 0.24504304\n","Training acc over epoch:  4 0.9491699\n","Validation loss:  4 0.23820037\n","Validation acc:  4 0.9494767\n","epoch 4: loss = 0.24504307\n","Start of epoch 5\n","Training loss over epoch:  5 0.2419812\n","Training acc over epoch:  5 0.9491995\n","Validation loss:  5 0.23524074\n","Validation acc:  5 0.9494767\n","epoch 5: loss = 0.2419812\n","Start of epoch 6\n","Training loss over epoch:  6 0.23916435\n","Training acc over epoch:  6 0.9491827\n","Validation loss:  6 0.23266618\n","Validation acc:  6 0.9494767\n","epoch 6: loss = 0.23916434\n","Start of epoch 7\n","Training loss over epoch:  7 0.23431495\n","Training acc over epoch:  7 0.9491955\n","Validation loss:  7 0.22393018\n","Validation acc:  7 0.9494767\n","epoch 7: loss = 0.23431493\n","Start of epoch 8\n","Training loss over epoch:  8 0.22642428\n","Training acc over epoch:  8 0.94915307\n","Validation loss:  8 0.21691422\n","Validation acc:  8 0.9494767\n","epoch 8: loss = 0.22642423\n","Start of epoch 9\n","Training loss over epoch:  9 0.21846092\n","Training acc over epoch:  9 0.94920754\n","Validation loss:  9 0.20755337\n","Validation acc:  9 0.94949144\n","epoch 9: loss = 0.21846087\n","Start of epoch 10\n","Training loss over epoch:  10 0.20907977\n","Training acc over epoch:  10 0.949552\n","Validation loss:  10 0.19557469\n","Validation acc:  10 0.9504275\n","epoch 10: loss = 0.20907974\n","Start of epoch 11\n","Training loss over epoch:  11 0.2009534\n","Training acc over epoch:  11 0.95013684\n","Validation loss:  11 0.1900323\n","Validation acc:  11 0.9513709\n","epoch 11: loss = 0.2009534\n","Start of epoch 12\n","Training loss over epoch:  12 0.19360271\n","Training acc over epoch:  12 0.95062953\n","Validation loss:  12 0.17989568\n","Validation acc:  12 0.9519384\n","epoch 12: loss = 0.19360268\n","Start of epoch 13\n","Training loss over epoch:  13 0.18590605\n","Training acc over epoch:  13 0.95136815\n","Validation loss:  13 0.1717749\n","Validation acc:  13 0.9529923\n","epoch 13: loss = 0.18590604\n","Start of epoch 14\n","Training loss over epoch:  14 0.17671606\n","Training acc over epoch:  14 0.9523671\n","Validation loss:  14 0.15851189\n","Validation acc:  14 0.95407575\n","epoch 14: loss = 0.17671607\n","Start of epoch 15\n","Training loss over epoch:  15 0.16761422\n","Training acc over epoch:  15 0.95343184\n","Validation loss:  15 0.14927545\n","Validation acc:  15 0.9567733\n","epoch 15: loss = 0.16761424\n","Start of epoch 16\n","Training loss over epoch:  16 0.16088617\n","Training acc over epoch:  16 0.9545502\n","Validation loss:  16 0.14341225\n","Validation acc:  16 0.9577167\n","epoch 16: loss = 0.16088617\n","Start of epoch 17\n","Training loss over epoch:  17 0.15395766\n","Training acc over epoch:  17 0.95561725\n","Validation loss:  17 0.13631447\n","Validation acc:  17 0.9603405\n","epoch 17: loss = 0.15395765\n","Start of epoch 18\n","Training loss over epoch:  18 0.14766915\n","Training acc over epoch:  18 0.957129\n","Validation loss:  18 0.1331238\n","Validation acc:  18 0.96367186\n","epoch 18: loss = 0.14766912\n","Start of epoch 19\n","Training loss over epoch:  19 0.14232674\n","Training acc over epoch:  19 0.9589115\n","Validation loss:  19 0.12299534\n","Validation acc:  19 0.965824\n","epoch 19: loss = 0.14232673\n","Start of epoch 20\n","Training loss over epoch:  20 0.13543573\n","Training acc over epoch:  20 0.9605794\n","Validation loss:  20 0.11510371\n","Validation acc:  20 0.9671064\n","epoch 20: loss = 0.13543573\n","Start of epoch 21\n","Training loss over epoch:  21 0.12942885\n","Training acc over epoch:  21 0.9623018\n","Validation loss:  21 0.11079718\n","Validation acc:  21 0.9686173\n","epoch 21: loss = 0.12942885\n","Start of epoch 22\n","Training loss over epoch:  22 0.12444413\n","Training acc over epoch:  22 0.96369815\n","Validation loss:  22 0.103997916\n","Validation acc:  22 0.97027564\n","epoch 22: loss = 0.12444411\n","Start of epoch 23\n","Training loss over epoch:  23 0.119297184\n","Training acc over epoch:  23 0.96543175\n","Validation loss:  23 0.09948581\n","Validation acc:  23 0.97238356\n","epoch 23: loss = 0.119297184\n","Start of epoch 24\n","Training loss over epoch:  24 0.11475079\n","Training acc over epoch:  24 0.96684813\n","Validation loss:  24 0.09591763\n","Validation acc:  24 0.97308373\n","epoch 24: loss = 0.114750765\n","Start of epoch 25\n","Training loss over epoch:  25 0.110493414\n","Training acc over epoch:  25 0.9680354\n","Validation loss:  25 0.092119515\n","Validation acc:  25 0.97473466\n","epoch 25: loss = 0.1104934\n","Start of epoch 26\n","Training loss over epoch:  26 0.10675896\n","Training acc over epoch:  26 0.9693813\n","Validation loss:  26 0.08808931\n","Validation acc:  26 0.9751253\n","epoch 26: loss = 0.106758945\n","Start of epoch 27\n","Training loss over epoch:  27 0.10321074\n","Training acc over epoch:  27 0.9702016\n","Validation loss:  27 0.08526486\n","Validation acc:  27 0.976253\n","epoch 27: loss = 0.10321076\n","Start of epoch 28\n","Training loss over epoch:  28 0.10054875\n","Training acc over epoch:  28 0.9710901\n","Validation loss:  28 0.084064685\n","Validation acc:  28 0.9771374\n","epoch 28: loss = 0.10054875\n","Start of epoch 29\n","Training loss over epoch:  29 0.09772065\n","Training acc over epoch:  29 0.9721275\n","Validation loss:  29 0.080433644\n","Validation acc:  29 0.9778744\n","epoch 29: loss = 0.09772065\n","Start of epoch 30\n","Training loss over epoch:  30 0.09445029\n","Training acc over epoch:  30 0.97307044\n","Validation loss:  30 0.07760244\n","Validation acc:  30 0.9784714\n","epoch 30: loss = 0.09445029\n","Start of epoch 31\n","Training loss over epoch:  31 0.09204377\n","Training acc over epoch:  31 0.973793\n","Validation loss:  31 0.075746514\n","Validation acc:  31 0.97912\n","epoch 31: loss = 0.092043765\n","Start of epoch 32\n","Training loss over epoch:  32 0.089532405\n","Training acc over epoch:  32 0.9745773\n","Validation loss:  32 0.075967796\n","Validation acc:  32 0.97977597\n","epoch 32: loss = 0.08953242\n","Start of epoch 33\n","Training loss over epoch:  33 0.08745448\n","Training acc over epoch:  33 0.9753424\n","Validation loss:  33 0.073889576\n","Validation acc:  33 0.9801002\n","epoch 33: loss = 0.0874545\n","Start of epoch 34\n","Training loss over epoch:  34 0.08519178\n","Training acc over epoch:  34 0.97610503\n","Validation loss:  34 0.06985374\n","Validation acc:  34 0.98106575\n","epoch 34: loss = 0.085191764\n","Start of epoch 35\n","Training loss over epoch:  35 0.08365221\n","Training acc over epoch:  35 0.9766346\n","Validation loss:  35 0.06864146\n","Validation acc:  35 0.9816775\n","epoch 35: loss = 0.08365222\n","Start of epoch 36\n","Training loss over epoch:  36 0.08200154\n","Training acc over epoch:  36 0.9769919\n","Validation loss:  36 0.06727233\n","Validation acc:  36 0.9821713\n","epoch 36: loss = 0.082001515\n","Start of epoch 37\n","Training loss over epoch:  37 0.08018535\n","Training acc over epoch:  37 0.97760713\n","Validation loss:  37 0.06934089\n","Validation acc:  37 0.9822745\n","epoch 37: loss = 0.08018534\n","Start of epoch 38\n","Training loss over epoch:  38 0.07874506\n","Training acc over epoch:  38 0.9780846\n","Validation loss:  38 0.064808436\n","Validation acc:  38 0.9828788\n","epoch 38: loss = 0.07874505\n","Start of epoch 39\n","Training loss over epoch:  39 0.07694419\n","Training acc over epoch:  39 0.9786206\n","Validation loss:  39 0.064649545\n","Validation acc:  39 0.98265034\n","epoch 39: loss = 0.07694419\n","Start of epoch 40\n","Training loss over epoch:  40 0.07575783\n","Training acc over epoch:  40 0.9789226\n","Validation loss:  40 0.0641601\n","Validation acc:  40 0.98341686\n","epoch 40: loss = 0.07575783\n","Start of epoch 41\n","Training loss over epoch:  41 0.0743577\n","Training acc over epoch:  41 0.97932315\n","Validation loss:  41 0.061467655\n","Validation acc:  41 0.9839549\n","epoch 41: loss = 0.074357696\n","Start of epoch 42\n","Training loss over epoch:  42 0.07308126\n","Training acc over epoch:  42 0.9798535\n","Validation loss:  42 0.06278386\n","Validation acc:  42 0.983778\n","epoch 42: loss = 0.07308126\n","Start of epoch 43\n","Training loss over epoch:  43 0.072062895\n","Training acc over epoch:  43 0.9801227\n","Validation loss:  43 0.05986986\n","Validation acc:  43 0.9844708\n","epoch 43: loss = 0.072062895\n","Start of epoch 44\n","Training loss over epoch:  44 0.07063862\n","Training acc over epoch:  44 0.98061293\n","Validation loss:  44 0.06048706\n","Validation acc:  44 0.98415387\n","epoch 44: loss = 0.07063863\n","Start of epoch 45\n","Training loss over epoch:  45 0.06920996\n","Training acc over epoch:  45 0.98088855\n","Validation loss:  45 0.057979662\n","Validation acc:  45 0.98489094\n","epoch 45: loss = 0.06920996\n","Start of epoch 46\n","Training loss over epoch:  46 0.06812592\n","Training acc over epoch:  46 0.98134595\n","Validation loss:  46 0.057687696\n","Validation acc:  46 0.98500144\n","epoch 46: loss = 0.06812594\n","Start of epoch 47\n","Training loss over epoch:  47 0.06703175\n","Training acc over epoch:  47 0.9816207\n","Validation loss:  47 0.05655117\n","Validation acc:  47 0.9852152\n","epoch 47: loss = 0.06703176\n","Start of epoch 48\n","Training loss over epoch:  48 0.06551011\n","Training acc over epoch:  48 0.9820061\n","Validation loss:  48 0.0556504\n","Validation acc:  48 0.98548055\n","epoch 48: loss = 0.06551011\n","Start of epoch 49\n","Training loss over epoch:  49 0.06418916\n","Training acc over epoch:  49 0.982373\n","Validation loss:  49 0.056208547\n","Validation acc:  49 0.98548794\n","epoch 49: loss = 0.064189166\n","Start of epoch 50\n","Training loss over epoch:  50 0.06344204\n","Training acc over epoch:  50 0.9825821\n","Validation loss:  50 0.05483072\n","Validation acc:  50 0.98582697\n","epoch 50: loss = 0.06344204\n","Start of epoch 51\n","Training loss over epoch:  51 0.06231037\n","Training acc over epoch:  51 0.9829162\n","Validation loss:  51 0.05354411\n","Validation acc:  51 0.9862618\n","epoch 51: loss = 0.062310375\n","Start of epoch 52\n","Training loss over epoch:  52 0.061051704\n","Training acc over epoch:  52 0.98331434\n","Validation loss:  52 0.052997403\n","Validation acc:  52 0.9863281\n","epoch 52: loss = 0.061051697\n","Start of epoch 53\n","Training loss over epoch:  53 0.059762962\n","Training acc over epoch:  53 0.98354506\n","Validation loss:  53 0.05271667\n","Validation acc:  53 0.9862913\n","epoch 53: loss = 0.05976296\n","Start of epoch 54\n","Training loss over epoch:  54 0.059123196\n","Training acc over epoch:  54 0.98371005\n","Validation loss:  54 0.051711135\n","Validation acc:  54 0.9865861\n","epoch 54: loss = 0.059123196\n","Start of epoch 55\n","Training loss over epoch:  55 0.05769131\n","Training acc over epoch:  55 0.98418915\n","Validation loss:  55 0.051339734\n","Validation acc:  55 0.9869841\n","epoch 55: loss = 0.05769131\n","Start of epoch 56\n","Training loss over epoch:  56 0.056934666\n","Training acc over epoch:  56 0.9844399\n","Validation loss:  56 0.05069311\n","Validation acc:  56 0.98699147\n","epoch 56: loss = 0.056934662\n","Start of epoch 57\n","Training loss over epoch:  57 0.056191172\n","Training acc over epoch:  57 0.9845969\n","Validation loss:  57 0.049796395\n","Validation acc:  57 0.987161\n","epoch 57: loss = 0.05619117\n","Start of epoch 58\n","Training loss over epoch:  58 0.0551946\n","Training acc over epoch:  58 0.9847948\n","Validation loss:  58 0.051921256\n","Validation acc:  58 0.9866082\n","epoch 58: loss = 0.055194613\n","Start of epoch 59\n","Training loss over epoch:  59 0.054253105\n","Training acc over epoch:  59 0.98497826\n","Validation loss:  59 0.04944478\n","Validation acc:  59 0.987161\n","epoch 59: loss = 0.054253105\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 834/834 [00:10<00:00, 76.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The average imputation accuracyon test data with 0.2 missing genotypes is 0.9527: \n","Sensitivity: nan\n","Specificity: 0.9576856985968495\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-149-6db1d8498207>:190: RuntimeWarning: invalid value encountered in true_divide\n","  TPR = TP/(TP+FN)\n"]},{"output_type":"stream","name":"stdout","text":["F1-score macro: 0.15162445079339731\n","F1-score micro: 0.9526769117250579\n"]}]},{"cell_type":"code","source":["to_save_array.shape, Y_train.index[test_index].shape, headers[:].shape"],"metadata":{"id":"ifLke-oWc1Ok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672347869205,"user_tz":300,"elapsed":50,"user":{"displayName":"net crowmaster","userId":"14126946344391667924"}},"outputId":"62b4a1dd-f614-4bfa-bc08-8976175a1b13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((834, 848), (834,), (848,))"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":[],"metadata":{"id":"CRjKkWClfjmM"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1lo9cmptjRRESsVlVECvlJFHbLeaygq2s","timestamp":1640128751823},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb","timestamp":1621552889682}],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}